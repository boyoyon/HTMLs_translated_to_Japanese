<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Object Recognition from Local Scale-Invariant Features</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
           .margin-abstract {
               margin-left: 60px; /* å·¦ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
               margin-right: 60px; /* å³ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
           }
        </style>
    <style>
        .styleRef { 
            text-indent: -40px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 10px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 40px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 30px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 40px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
<style>
.container {
  display: flex;
  flex-wrap: wrap;
}
.column {
  flex: 0.50%; /* å¹…ã‚’50%ã«è¨­å®šã—ã¦2åˆ—ã« */
  box-sizing: border-box;
  padding: 10px;
}
</style>

    </head>
    <body>
        <h1><center>Object Recognition from Local Scale-Invariant Features<br><span style="color:blue;">å±€æ‰€ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ç‰¹å¾´ã‹ã‚‰ã®ç‰©ä½“èªè­˜</span></center></h1>

<center>David G. Lowe</center>
<center>Computer Science Department </center>
<center>University of British Columbia </center>
<center>Vancouver, B.C., V6T 1Z4, Canada</center>
<center>lowe@cs.ubc.ca</center>

<h2><center>Abstract <span style="color:blue;">è¦æ—¨</span></center></h2>


<p>
An object recognition system has been developed that uses a 
new class of local image features. The features are invariant 
to image scaling, translation, and rotation, and partially invariant 
to illumination changes and affine or 3D projection. 
These features share similar properties with neurons in inferior 
temporal cortex that are used for object recognition 
in primate vision. Features are efficiently detected through 
a staged filtering approach that identifies stable points in 
scale space. Image keys are created that allow for local geometric 
deformations by representing blurred image gradients 
in multiple orientation planes and at multiple scales. 
The keys are used as input to a nearest-neighbor indexing 
method that identifies candidate object matches. Final verification 
of each match is achieved by finding a low-residual 
least-squares solution for the unknown model parameters. 
Experimental results show that robust object recognition 
can be achieved in cluttered partially-occluded images with 
a computation time of under 2 seconds.

<br><span style="color:blue;">
æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã®å±€æ‰€ç”»åƒç‰¹å¾´ã‚’ç”¨ã„ãŸç‰©ä½“èªè­˜ã‚·ã‚¹ãƒ†ãƒ ãŒé–‹ç™ºã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®ç‰¹å¾´ã¯ã€ç”»åƒã®æ‹¡å¤§ç¸®å°ã€ä¸¦é€²ã€å›è»¢ã«å¯¾ã—ã¦ä¸å¤‰ã§ã‚ã‚Šã€ç…§æ˜ã®å¤‰åŒ–ã‚„ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ã€3DæŠ•å½±ã«å¯¾ã—ã¦éƒ¨åˆ†çš„ã«ä¸å¤‰ã§ã™ã€‚ã“ã‚Œã‚‰ã®ç‰¹å¾´ã¯ã€éœŠé•·é¡ã®è¦–è¦šã«ãŠã‘ã‚‹ç‰©ä½“èªè­˜ã«ç”¨ã„ã‚‰ã‚Œã‚‹ä¸‹å´é ­è‘‰çš®è³ªã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¨åŒæ§˜ã®ç‰¹æ€§ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ç‰¹å¾´ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“å†…ã®å®‰å®šç‚¹ã‚’è­˜åˆ¥ã™ã‚‹æ®µéšçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã«ã‚ˆã£ã¦åŠ¹ç‡çš„ã«æ¤œå‡ºã•ã‚Œã¾ã™ã€‚è¤‡æ•°ã®æ–¹å‘å¹³é¢ã¨è¤‡æ•°ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã¼ã‚„ã‘ãŸç”»åƒå‹¾é…ã‚’è¡¨ç¾ã™ã‚‹ã“ã¨ã§ã€å±€æ‰€çš„ãªå¹¾ä½•å­¦çš„å¤‰å½¢ã‚’å¯èƒ½ã«ã™ã‚‹ç”»åƒã‚­ãƒ¼ãŒä½œæˆã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚­ãƒ¼ã¯ã€å€™è£œã¨ãªã‚‹ç‰©ä½“ã®ä¸€è‡´ã‚’è­˜åˆ¥ã™ã‚‹æœ€è¿‘å‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ³•ã¸ã®å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚å„ä¸€è‡´ã®æœ€çµ‚çš„ãªæ¤œè¨¼ã¯ã€æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ä½æ®‹å·®æœ€å°äºŒä¹—è§£ã‚’æ±‚ã‚ã‚‹ã“ã¨ã«ã‚ˆã£ã¦è¡Œã‚ã‚Œã¾ã™ã€‚å®Ÿé¨“çµæœã«ã‚ˆã‚‹ã¨ã€ä¹±é›‘ã§éƒ¨åˆ†çš„ã«é®è”½ã•ã‚ŒãŸç”»åƒã§ã‚‚ã€2ç§’æœªæº€ã®è¨ˆç®—æ™‚é–“ã§å …ç‰¢ãªç‰©ä½“èªè­˜ã‚’å®Ÿç¾ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
</span>
</p>

<h2>1. Introduction <span style="color:blue;">ã¯ã˜ã‚ã«</span></h2>
<p>
Object recognition in cluttered real-world scenes requires 
local image features that are unaffected by nearby clutter or 
partial occlusion. The features must be at least partially invariant 
to illumination, 3D projective transforms, and common 
object variations. On the other hand, the features must 
also be sufficiently distinctive to identify specific objects 
among many alternatives. The difficulty of the object recognition 
problem is due in large part to the lack of success in 
finding such image features. However, recent research on 
the use of dense local features (e.g., Schmid & Mohr [19]) 
has shown that efficient recognition can often be achieved 
by using local image descriptors sampled at a large number 
of repeatable locations. 

<br><span style="color:blue;">
é›‘ç„¶ã¨ã—ãŸç¾å®Ÿä¸–ç•Œã®ã‚·ãƒ¼ãƒ³ã«ãŠã‘ã‚‹ç‰©ä½“èªè­˜ã«ã¯ã€è¿‘å‚ã®ä¹±é›‘ã•ã‚„éƒ¨åˆ†çš„ãªé®è”½ã®å½±éŸ¿ã‚’å—ã‘ãªã„å±€æ‰€çš„ãªç”»åƒç‰¹å¾´ãŒå¿…è¦ã§ã™ã€‚ã“ã‚Œã‚‰ã®ç‰¹å¾´ã¯ã€ç…§æ˜ã€3æ¬¡å…ƒå°„å½±å¤‰æ›ã€ãŠã‚ˆã³ä¸€èˆ¬çš„ãªç‰©ä½“ã®å¤‰å‹•ã«å¯¾ã—ã¦ã€å°‘ãªãã¨ã‚‚éƒ¨åˆ†çš„ã«ä¸å¤‰ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚ä¸€æ–¹ã§ã€ã“ã‚Œã‚‰ã®ç‰¹å¾´ã¯ã€å¤šãã®é¸æŠè‚¢ã®ä¸­ã‹ã‚‰ç‰¹å®šã®ç‰©ä½“ã‚’è­˜åˆ¥ã§ãã‚‹ã»ã©ååˆ†ã«ç‰¹å¾´çš„ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚ç‰©ä½“èªè­˜å•é¡Œã®é›£ã—ã•ã¯ã€ä¸»ã«ãã®ã‚ˆã†ãªç”»åƒç‰¹å¾´ã®ç™ºè¦‹ãŒæˆåŠŸã—ãªã„ç‚¹ã«èµ·å› ã—ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€ç¨ å¯†ãªå±€æ‰€ç‰¹å¾´ã®åˆ©ç”¨ã«é–¢ã™ã‚‹æœ€è¿‘ã®ç ”ç©¶ï¼ˆä¾‹ï¼šSchmid & Mohr [19]ï¼‰ã§ã¯ã€å¤šæ•°ã®ç¹°ã‚Šè¿”ã—å¯èƒ½ãªä½ç½®ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸå±€æ‰€çš„ãªç”»åƒè¨˜è¿°å­ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€åŠ¹ç‡çš„ãªèªè­˜ãŒé”æˆã§ãã‚‹å ´åˆãŒå¤šã„ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
</span>
</p><p>

This paper presents a new method for image feature generation 
called the Scale Invariant Feature Transform (SIFT). 
This approach transforms an image into a large collection 
of local feature vectors, each of which is invariant to image 
translation, scaling, and rotation, and partially invariant to 
illumination changes and affine or 3D projection. Previous 
approaches to local feature generation lacked invariance to 
scale and were more sensitive to projective distortion and 
illumination change. The SIFT features share a number of 
properties in common with the responses of neurons in inferior 
temporal (IT) cortex in primate vision. This paper also 
describes improved approaches to indexing and model verification.

<br><span style="color:blue;">
æœ¬è«–æ–‡ã§ã¯ã€ç”»åƒç‰¹å¾´ç”Ÿæˆã®ãŸã‚ã®æ–°ã—ã„æ‰‹æ³•ã§ã‚ã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ç‰¹å¾´å¤‰æ›ï¼ˆSIFTï¼‰ã‚’ç´¹ä»‹ã™ã‚‹ã€‚
ã“ã®æ‰‹æ³•ã¯ã€ç”»åƒã‚’å¤§è¦æ¨¡ãªå±€æ‰€ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®é›†åˆã«å¤‰æ›ã™ã‚‹ã€‚å„ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯ç”»åƒã®ç§»å‹•ã€æ‹¡å¤§ç¸®å°ã€å›è»¢ã«å¯¾ã—ã¦ä¸å¤‰ã§ã‚ã‚Šã€ç…§æ˜å¤‰åŒ–ã‚„ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ã€3DæŠ•å½±ã«å¯¾ã—ã¦éƒ¨åˆ†çš„ã«ä¸å¤‰ã§ã‚ã‚‹ã€‚å¾“æ¥ã®å±€æ‰€ç‰¹å¾´ç”Ÿæˆæ‰‹æ³•ã¯ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ã§ã¯ãªãã€æŠ•å½±æ­ªã¿ã‚„ç…§æ˜å¤‰åŒ–ã«å¯¾ã—ã¦ã‚ˆã‚Šæ•æ„Ÿã§ã‚ã£ãŸã€‚SIFTç‰¹å¾´ã¯ã€éœŠé•·é¡ã®è¦–è¦šã«ãŠã‘ã‚‹ä¸‹å´é ­è‘‰ï¼ˆITï¼‰çš®è³ªã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å¿œç­”ã¨å…±é€šã™ã‚‹å¤šãã®ç‰¹æ€§ã‚’æŒã¤ã€‚æœ¬è«–æ–‡ã§ã¯ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã¨ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã«å¯¾ã™ã‚‹æ”¹è‰¯ã•ã‚ŒãŸæ‰‹æ³•ã«ã¤ã„ã¦ã‚‚èª¬æ˜ã™ã‚‹ã€‚
</span>
</p><p>

The scale-invariant features are efficiently identified by 
using a staged filtering approach. The first stage identifies 
key locations in scale space by looking for locations that 
are maxima or minima of a difference-of-Gaussian function. 
Each point is used to generate a feature vector that describes 
the local image region sampled relative to its scale-space coordinate 
frame. The features achieve partial invariance to 
local variations, such as affine or 3D projections, by blurring 
image gradient locations. This approach is based on a 
model of the behavior of complex cells in the cerebral cortex 
of mammalian vision. The resulting feature vectors are 
called SIFT keys. In the current implementation, each image 
generates on the order of 1000 SIFT keys, a process that 
requires less than 1 second of computation time. 

<br><span style="color:blue;">
ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ç‰¹å¾´ã¯ã€æ®µéšçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ç”¨ã„ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ã«è­˜åˆ¥ã•ã‚Œã¾ã™ã€‚ç¬¬1æ®µéšã§ã¯ã€ã‚¬ã‚¦ã‚¹é–¢æ•°ã®å·®ã®æœ€å¤§å€¤ã¾ãŸã¯æœ€å°å€¤ã¨ãªã‚‹ä½ç½®ã‚’æ¢ã™ã“ã¨ã§ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“å†…ã®ä¸»è¦ãªä½ç½®ã‚’ç‰¹å®šã—ã¾ã™ã€‚å„ç‚¹ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“åº§æ¨™ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åŸºæº–ã¨ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸå±€æ‰€ç”»åƒé ˜åŸŸã‚’è¨˜è¿°ã™ã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ç‰¹å¾´ã¯ã€ç”»åƒã®å‹¾é…ä½ç½®ã‚’ã¼ã‹ã™ã“ã¨ã§ã€ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ã‚„3DæŠ•å½±ãªã©ã®å±€æ‰€çš„ãªå¤‰å‹•ã«å¯¾ã—ã¦éƒ¨åˆ†çš„ãªä¸å¤‰æ€§ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ã“ã®æ‰‹æ³•ã¯ã€å“ºä¹³é¡ã®è¦–è¦šã«ãŠã‘ã‚‹å¤§è„³çš®è³ªã®è¤‡é›‘ãªç´°èƒã®æŒ™å‹•ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€SIFTã‚­ãƒ¼ã¨å‘¼ã°ã‚Œã¾ã™ã€‚ç¾åœ¨ã®å®Ÿè£…ã§ã¯ã€å„ç”»åƒã¯ç´„1000å€‹ã®SIFTã‚­ãƒ¼ã‚’ç”Ÿæˆã—ã€ã“ã®å‡¦ç†ã«å¿…è¦ãªè¨ˆç®—æ™‚é–“ã¯1ç§’æœªæº€ã§ã™ã€‚
</span>
</p><p>

The SIFT keys derived from an image are used in a 
nearest-neighbour approach to indexing to identify candidate 
object models. Collections of keys that agree on a potential 
model pose are first identified through a Hough transformhash 
table,and thenthroughaleast-squaresfit toafinal 
estimate of model parameters. When at least 3 keys agree 
on the model parameters with low residual, there is strong 
evidence for the presence of the object. Since there may be 
dozens of SIFT keys in the image of a typical object, it is 
possible to have substantial levels of occlusion in the image 
and yet retain high levels of reliability. 

<br><span style="color:blue;">
ç”»åƒã‹ã‚‰å¾—ã‚‰ã‚ŒãŸSIFTã‚­ãƒ¼ã¯ã€æœ€è¿‘å‚æ³•ã«ã‚ˆã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã«ä½¿ç”¨ã•ã‚Œã€å€™è£œã¨ãªã‚‹ç‰©ä½“ãƒ¢ãƒ‡ãƒ«ã‚’è­˜åˆ¥ã—ã¾ã™ã€‚æ½œåœ¨çš„ãªãƒ¢ãƒ‡ãƒ«ã®å§¿å‹¢ã«ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã®é›†åˆã¯ã€ã¾ãšãƒãƒ•å¤‰æ›ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”¨ã„ã¦è­˜åˆ¥ã•ã‚Œã€æ¬¡ã«æœ€å°äºŒä¹—è¿‘ä¼¼ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€çµ‚æ¨å®šå€¤ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚å°‘ãªãã¨ã‚‚3ã¤ã®ã‚­ãƒ¼ãŒãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä½ã„æ®‹å·®ã§ä¸€è‡´ã™ã‚‹å ´åˆã€ç‰©ä½“ã®å­˜åœ¨ã‚’ç¤ºã™å¼·åŠ›ãªè¨¼æ‹ ã¨ãªã‚Šã¾ã™ã€‚å…¸å‹çš„ãªç‰©ä½“ã®ç”»åƒã«ã¯æ•°åå€‹ã®SIFTã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ç”»åƒã«ã‹ãªã‚Šã®ãƒ¬ãƒ™ãƒ«ã®é®è”½ãŒã‚ã£ã¦ã‚‚ã€é«˜ã„ä¿¡é ¼æ€§ã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚
</span>
</p><p>

The current object models are represented as 2D locations 
of SIFT keys that can undergo affine projection. Sufficient 
variation in feature location is allowed to recognize 
perspective projection of planar shapes at up to a 60 degree 
rotation away from the camera or to allow up to a 20 degree 
rotation of a 3D object. 

<br><span style="color:blue;">
ç¾åœ¨ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ãŒå¯èƒ½ãªSIFTã‚­ãƒ¼ã®2æ¬¡å…ƒä½ç½®ã¨ã—ã¦è¡¨ç¾ã•ã‚Œã¾ã™ã€‚ç‰¹å¾´ç‚¹ã®ä½ç½®ã«ã¯ååˆ†ãªå¤‰åŒ–ãŒè¨±å®¹ã•ã‚Œã‚‹ãŸã‚ã€ã‚«ãƒ¡ãƒ©ã‹ã‚‰æœ€å¤§60åº¦å›è»¢ã—ãŸå¹³é¢å½¢çŠ¶ã®é€è¦–æŠ•å½±ã‚’èªè­˜ã§ãã€3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯æœ€å¤§20åº¦å›è»¢ã§ãã¾ã™ã€‚
</span>
</p>

<h2>2. Related research <span style="color:blue;">é–¢é€£ç ”ç©¶</span></h2>
<p>
Object recognition is widely used in the machine vision industry 
for the purposes of inspection, registration, and manipulation. 
However, current commercial systems for object 
recognition depend almost exclusively on correlation-based 
template matching. While very effective for certain engineered 
environments, where object pose and illumination 
aretightlycontrolled,templatematchingbecomes computationally 
infeasible when object rotation, scale, illumination, 
and 3D pose are allowed to vary, and even more so when 
dealing with partial visibility and large model databases. 

<br><span style="color:blue;">
ç‰©ä½“èªè­˜ã¯ã€ãƒã‚·ãƒ³ãƒ“ã‚¸ãƒ§ãƒ³æ¥­ç•Œã§æ¤œæŸ»ã€ä½ç½®åˆã‚ã›ã€æ“ä½œã®ç›®çš„ã§åºƒãåˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
ã—ã‹ã—ãªãŒã‚‰ã€ç¾åœ¨ã®å•†ç”¨ç‰©ä½“èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ç›¸é–¢ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒƒãƒãƒ³ã‚°ã«ã»ã¼å…¨é¢çš„ã«ä¾å­˜ã—ã¦ã„ã¾ã™ã€‚ç‰©ä½“ã®å§¿å‹¢ã¨ç…§æ˜ãŒå³å¯†ã«åˆ¶å¾¡ã•ã‚Œã¦ã„ã‚‹ç‰¹å®šã®å·¥å­¦ç’°å¢ƒã§ã¯éå¸¸ã«åŠ¹æœçš„ã§ã™ãŒã€ç‰©ä½“ã®å›è»¢ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ç…§æ˜ã€3Då§¿å‹¢ãŒå¤‰åŒ–ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒƒãƒãƒ³ã‚°ã¯è¨ˆç®—ä¸Šä¸å¯èƒ½ã«ãªã‚Šã€éƒ¨åˆ†çš„ãªå¯è¦–æ€§ã‚„å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ‰±ã†å ´åˆã¯ã•ã‚‰ã«å›°é›£ã«ãªã‚Šã¾ã™ã€‚
</span>
</p><p>

An alternative to searching all image locations for 
matches is to extract features from the image that are at 
least partially invariant to the image formation process and 
matching only to those features. Many candidate feature 
types have been proposed and explored, including line segments 
[6], groupings of edges [11, 14], and regions [2], 
among many other proposals. While these features have 
worked well for certain object classes, they are often not detected 
frequently enough or with sufficient stability to form 
a basis for reliable recognition.

<br><span style="color:blue;">
ç”»åƒã®ã™ã¹ã¦ã®ä½ç½®ã‚’æ¤œç´¢ã—ã¦ä¸€è‡´ã‚’æ¢ã™ä»£ã‚ã‚Šã«ã€ç”»åƒã‹ã‚‰ã€ç”»åƒå½¢æˆãƒ—ãƒ­ã‚»ã‚¹ã«å¯¾ã—ã¦å°‘ãªãã¨ã‚‚éƒ¨åˆ†çš„ã«ä¸å¤‰ãªç‰¹å¾´ã‚’æŠ½å‡ºã—ã€ãã‚Œã‚‰ã®ç‰¹å¾´ã®ã¿ã«ä¸€è‡´ã•ã›ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ç·šåˆ†[6]ã€ã‚¨ãƒƒã‚¸ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–[11, 14]ã€é ˜åŸŸ[2]ãªã©ã€å¤šãã®å€™è£œã¨ãªã‚‹ç‰¹å¾´ã‚¿ã‚¤ãƒ—ãŒææ¡ˆã•ã‚Œã€æ¤œè¨ã•ã‚Œã¦ãã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®ç‰¹å¾´ã¯ç‰¹å®šã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚¯ãƒ©ã‚¹ã§ã¯ã†ã¾ãæ©Ÿèƒ½ã—ã¦ã„ã¾ã™ãŒã€ä¿¡é ¼æ€§ã®é«˜ã„èªè­˜ã®åŸºç›¤ã‚’å½¢æˆã™ã‚‹ã®ã«ååˆ†ãªé »åº¦ã§ã€ã¾ãŸã¯ååˆ†ãªå®‰å®šæ€§ã§æ¤œå‡ºã•ã‚Œãªã„ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚
</span>
</p><p>

There has been recent work on developing much denser 
collections of image features. One approach has been to 
use a corner detector (more accurately, a detector of peaks 
in local image variation) to identify repeatable image locations, 
around which local image properties can be measured. 
Zhang et al. [23] used the Harris corner detector to identify 
feature locations for epipolar alignment of images taken 
from differing viewpoints. Rather than attempting to correlate 
regions from one image against all possible regions 
in a second image, large savings in computation time were 
achieved by only matching regions centered at corner points 
in each image.e.

<br><span style="color:blue;">
è¿‘å¹´ã€ç”»åƒç‰¹å¾´ã®ã‚ˆã‚Šé«˜å¯†åº¦ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹ç™ºã«é–¢ã™ã‚‹ç ”ç©¶ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ãã®1ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ã‚³ãƒ¼ãƒŠãƒ¼æ¤œå‡ºå™¨ï¼ˆã‚ˆã‚Šæ­£ç¢ºã«ã¯ã€å±€æ‰€çš„ãªç”»åƒå¤‰å‹•ã®ãƒ”ãƒ¼ã‚¯æ¤œå‡ºå™¨ï¼‰ã‚’ç”¨ã„ã¦ã€ç”»åƒä¸Šã®å†ç¾æ€§ã®ã‚ã‚‹ä½ç½®ã‚’ç‰¹å®šã—ã€ãã®å‘¨å›²ã§å±€æ‰€çš„ãªç”»åƒç‰¹æ€§ã‚’æ¸¬å®šã™ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚Zhangã‚‰[23]ã¯ã€Harrisã‚³ãƒ¼ãƒŠãƒ¼æ¤œå‡ºå™¨ã‚’ç”¨ã„ã¦ã€ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰æ’®å½±ã•ã‚ŒãŸç”»åƒã®ã‚¨ãƒ”ãƒãƒ¼ãƒ©ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®ãŸã‚ã®ç‰¹å¾´ä½ç½®ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚ã‚ã‚‹ç”»åƒã®é ˜åŸŸã‚’åˆ¥ã®ç”»åƒã®ã™ã¹ã¦ã®å¯èƒ½æ€§ã®ã‚ã‚‹é ˜åŸŸã¨ç›¸é–¢ã•ã›ã‚‹ã®ã§ã¯ãªãã€å„ç”»åƒã®ã‚³ãƒ¼ãƒŠãƒ¼ç‚¹ã‚’ä¸­å¿ƒã¨ã—ãŸé ˜åŸŸã®ã¿ã‚’ãƒãƒƒãƒãƒ³ã‚°ã•ã›ã‚‹ã“ã¨ã§ã€è¨ˆç®—æ™‚é–“ã‚’å¤§å¹…ã«ç¯€ç´„ã—ã¾ã—ãŸã€‚
</span>
</p><p>

For the object recognition problem, Schmid & Mohr 
[19] also used the Harris corner detector to identify interest 
points, and then created a local image descriptor at 
each interest point from an orientation-invariant vector of 
derivative-of-Gaussian image measurements. These image 
descriptors were used for robust object recognition by looking 
for multiple matching descriptors that satisfied object-
based orientation and location constraints. This work was 
impressive both for the speed of recognition in a large 
database and the ability to handle cluttered images. 

<br><span style="color:blue;">
ç‰©ä½“èªè­˜å•é¡Œã«ãŠã„ã¦ã€Schmid & Mohr
[19] ã¯ã€Harrisã‚³ãƒ¼ãƒŠãƒ¼æ¤œå‡ºå™¨ã‚’ç”¨ã„ã¦é–¢å¿ƒç‚¹ã‚’è­˜åˆ¥ã—ã€å„é–¢å¿ƒç‚¹ã«ãŠã„ã¦ã€ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¾®åˆ†ç”»åƒæ¸¬å®šå€¤ã®æ–¹å‘ä¸å¤‰ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰å±€æ‰€ç”»åƒè¨˜è¿°å­ã‚’ä½œæˆã—ãŸã€‚ã“ã‚Œã‚‰ã®ç”»åƒè¨˜è¿°å­ã¯ã€ç‰©ä½“ã«åŸºã¥ãæ–¹å‘ã¨ä½ç½®ã®åˆ¶ç´„ã‚’æº€ãŸã™è¤‡æ•°ã®ä¸€è‡´ã™ã‚‹è¨˜è¿°å­ã‚’æ¢ã™ã“ã¨ã§ã€å …ç‰¢ãªç‰©ä½“èªè­˜ã«ä½¿ç”¨ã•ã‚ŒãŸã€‚ã“ã®ç ”ç©¶ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãŠã‘ã‚‹èªè­˜é€Ÿåº¦ã¨ã€ä¹±é›‘ãªç”»åƒã‚’å‡¦ç†ã™ã‚‹èƒ½åŠ›ã®ä¸¡æ–¹ã«ãŠã„ã¦å°è±¡çš„ã§ã‚ã£ãŸã€‚
</span>
</p><p>

The corner detectors used in these previous approaches 
have a major failing, which is that they examine an image 
at only a single scale. As the change in scale becomes significant, 
these detectors respond to different image points. 
Also, since the detector does not provide an indication of the 
object scale, it is necessary to create image descriptors and 
attempt matching at a large number of scales. This paper describes 
an efficient method to identify stable key locations 
in scale space. This means that different scalings of an image 
will have no effect on the set of key locations selected. 
Furthermore, an explicit scale is determined for each point, 
which allows the image description vector for that point to 
be sampled at an equivalent scale in each image. A canonical 
orientation is determined at each location, so that matching 
can be performed relative to a consistent local 2D coordinate 
frame. This allows for the use of more distinctive 
image descriptors than the rotation-invariant ones used by 
Schmid and Mohr, and the descriptor is further modified to 
improve its stability to changes in affine projection and illumination.

<br><span style="color:blue;">
å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ¼ãƒŠãƒ¼æ¤œå‡ºå™¨ã«ã¯ã€ç”»åƒã‚’å˜ä¸€ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã—ã‹æ¤œæŸ»ã—ãªã„ã¨ã„ã†å¤§ããªæ¬ ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚ã‚¹ã‚±ãƒ¼ãƒ«ã®å¤‰åŒ–ãŒå¤§ãããªã‚‹ã¨ã€ã“ã‚Œã‚‰ã®æ¤œå‡ºå™¨ã¯ç•°ãªã‚‹ç”»åƒç‚¹ã«åå¿œã—ã¾ã™ã€‚ã¾ãŸã€æ¤œå‡ºå™¨ã¯ç‰©ä½“ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’ç¤ºã—ã¦ã„ãªã„ãŸã‚ã€ç”»åƒè¨˜è¿°å­ã‚’ä½œæˆã—ã€å¤šæ•°ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒãƒƒãƒãƒ³ã‚°ã‚’è©¦è¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æœ¬è«–æ–‡ã§ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“ã«ãŠã„ã¦å®‰å®šã—ãŸã‚­ãƒ¼ä½ç½®ã‚’ç‰¹å®šã™ã‚‹åŠ¹ç‡çš„ãªæ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ç”»åƒã®ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒã€é¸æŠã•ã‚ŒãŸã‚­ãƒ¼ä½ç½®ã®é›†åˆã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€å„ç‚¹ã«å¯¾ã—ã¦æ˜ç¤ºçš„ãªã‚¹ã‚±ãƒ¼ãƒ«ãŒæ±ºå®šã•ã‚Œã‚‹ãŸã‚ã€å„ç”»åƒã«ãŠã„ã¦ãã®ç‚¹ã®ç”»åƒè¨˜è¿°ãƒ™ã‚¯ãƒˆãƒ«ã‚’åŒç­‰ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å„ä½ç½®ã§æ¨™æº–çš„ãªæ–¹å‘ãŒæ±ºå®šã•ã‚Œã‚‹ãŸã‚ã€ä¸€è²«ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«2Dåº§æ¨™ç³»ã‚’åŸºæº–ã¨ã—ã¦ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Schmidã¨MohrãŒä½¿ç”¨ã—ãŸå›è»¢ä¸å¤‰ã®ç”»åƒè¨˜è¿°å­ã‚ˆã‚Šã‚‚ç‰¹å¾´çš„ãªç”»åƒè¨˜è¿°å­ã®ä½¿ç”¨ãŒå¯èƒ½ã«ãªã‚Šã€è¨˜è¿°å­ã¯ã•ã‚‰ã«ä¿®æ­£ã•ã‚Œã€ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ã¨ç…§æ˜ã®å¤‰åŒ–ã«å¯¾ã™ã‚‹å®‰å®šæ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚
</span>
</p><p>

Other approaches to appearance-based recognition include 
eigenspace matching [13], color histograms [20], and 
receptive field histograms [18]. These approaches have all 
been demonstrated successfully on isolated objects or presegmented 
images, but due to their more global features it 
has been difficult to extend them to cluttered and partially 
occluded images. Ohba & Ikeuchi [15] successfully apply 
the eigenspace approach to cluttered images by using many 
small local eigen-windows, but this then requires expensive 
search for each window in a new image, as with template 
matching.

<br><span style="color:blue;">
å¤–è¦³ãƒ™ãƒ¼ã‚¹ã®èªè­˜ã«å¯¾ã™ã‚‹ä»–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ã¯ã€å›ºæœ‰ç©ºé–“ãƒãƒƒãƒãƒ³ã‚° [13]ã€è‰²ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  [20]ã€å—å®¹é‡ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  [18] ãªã©ãŒã‚ã‚‹ã€‚ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã„ãšã‚Œã‚‚å­¤ç«‹ã—ãŸç‰©ä½“ã‚„äº‹å‰ã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã•ã‚ŒãŸç”»åƒã§ã¯æˆåŠŸãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ãŒã€ã‚ˆã‚Šå¤§åŸŸçš„ãªç‰¹å¾´ã‚’æŒã¤ãŸã‚ã€ä¹±é›‘ãªç”»åƒã‚„éƒ¨åˆ†çš„ã«é®è”½ã•ã‚ŒãŸç”»åƒã«æ‹¡å¼µã™ã‚‹ã“ã¨ã¯å›°é›£ã§ã‚ã£ãŸã€‚Ohba & Ikeuchi [15] ã¯ã€å¤šæ•°ã®å°ã•ãªå±€æ‰€çš„ãªå›ºæœ‰ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ä¹±é›‘ãªç”»åƒã«å›ºæœ‰ç©ºé–“ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é©ç”¨ã™ã‚‹ã“ã¨ã«æˆåŠŸã—ãŸãŒã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒƒãƒãƒ³ã‚°ã¨åŒæ§˜ã«ã€æ–°ã—ã„ç”»åƒã®å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«å¯¾ã—ã¦ã‚³ã‚¹ãƒˆã®ã‹ã‹ã‚‹æ¢ç´¢ãŒå¿…è¦ã¨ãªã‚‹ã€‚
</span>
</p>

<h2>3. Key localization <span style="color:blue;">ã‚­ãƒ¼ã®å®šä½</span></h2>
<p>

We wish to identify locations in image scale space that are 
invariant with respect to image translation, scaling, and rotation, 
and are minimally affected by noise and small distortions. 
Lindeberg [8] has shown that under some rather 
general assumptions on scale invariance, the Gaussian kernel 
and its derivatives are the only possible smoothing kernels 
for scale space analysis. 

<br><span style="color:blue;">
æˆ‘ã€…ã¯ã€ç”»åƒã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“ã«ãŠã„ã¦ã€ç”»åƒã®ç§»å‹•ã€æ‹¡å¤§ç¸®å°ã€å›è»¢ã«å¯¾ã—ã¦ä¸å¤‰ã§ã‚ã‚Šã€ãƒã‚¤ã‚ºã‚„å°ã•ãªæ­ªã¿ã®å½±éŸ¿ãŒæœ€å°é™ã§ã‚ã‚‹ä½ç½®ã‚’ç‰¹å®šã—ãŸã„ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚
Lindeberg [8] ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰æ€§ã«é–¢ã™ã‚‹ã„ãã¤ã‹ã®æ¯”è¼ƒçš„ä¸€èˆ¬çš„ãªä»®å®šã®ä¸‹ã§ã€ã‚¬ã‚¦ã‚¹ã‚«ãƒ¼ãƒãƒ«ã¨ãã®å°é–¢æ•°ãŒã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“è§£æã«ä½¿ç”¨å¯èƒ½ãªå”¯ä¸€ã®å¹³æ»‘åŒ–ã‚«ãƒ¼ãƒãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚
</span>
</p><p>

To achieve rotation invariance and a high level of efficiency, 
we have chosen to select key locations at maxima 
and minima of a difference of Gaussian function applied in 
scale space. This can be computed very efficiently by building 
an image pyramid with resampling between each level. 
Furthermore, it locates key points at regions and scales of 
high variation, making these locations particularly stable for 
characterizing the image. Crowley & Parker [4] and Lindeberg 
[9] have previously used the difference-of-Gaussian in 
scale space for other purposes. In the following, we describe 
a particularly efficient and stable method to detect and characterize 
the maxima and minima of this function. 

<br><span style="color:blue;">
å›è»¢ä¸å¤‰æ€§ã¨é«˜ã„åŠ¹ç‡æ€§ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“ã«é©ç”¨ã•ã‚ŒãŸã‚¬ã‚¦ã‚¹é–¢æ•°ã®å·®ã®æœ€å¤§å€¤ã¨æœ€å°å€¤ã«ãŠã‘ã‚‹é‡è¦ãªä½ç½®ã‚’é¸æŠã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€å„ãƒ¬ãƒ™ãƒ«é–“ã§å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã£ãŸç”»åƒãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§éå¸¸ã«åŠ¹ç‡çš„ã«è¨ˆç®—ã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€ã“ã®æ‰‹æ³•ã¯ã€å¤‰åŒ–ã®å¤§ãã„é ˜åŸŸã¨ã‚¹ã‚±ãƒ¼ãƒ«ã«é‡è¦ãªç‚¹ã‚’é…ç½®ã™ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã®ä½ç½®ã¯ç”»åƒã®ç‰¹æ€§è©•ä¾¡ã«ãŠã„ã¦ç‰¹ã«å®‰å®šã—ã¾ã™ã€‚Crowley & Parker [4] ã¨ Lindeberg [9] ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“ã«ãŠã‘ã‚‹ã‚¬ã‚¦ã‚¹é–¢æ•°ã®å·®ã‚’ä»–ã®ç›®çš„ã§ä½¿ç”¨ã—ã¦ã„ã¾ã—ãŸã€‚ä»¥ä¸‹ã§ã¯ã€ã“ã®é–¢æ•°ã®æœ€å¤§å€¤ã¨æœ€å°å€¤ã‚’æ¤œå‡ºã—ã€ç‰¹æ€§è©•ä¾¡ã™ã‚‹ãŸã‚ã®ç‰¹ã«åŠ¹ç‡çš„ã§å®‰å®šã—ãŸæ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚
</span>
</p><p>

As the 2D Gaussian function is separable, its convolution 
with the input image can be efficiently computed by applying 
two passes of the 1D Gaussian function in the horizontal 
and vertical directions: 

<br><span style="color:blue;">
2æ¬¡å…ƒã‚¬ã‚¦ã‚¹é–¢æ•°ã¯åˆ†é›¢å¯èƒ½ã§ã‚ã‚‹ãŸã‚ã€å…¥åŠ›ç”»åƒã¨ã®ç•³ã¿è¾¼ã¿ã¯ã€1æ¬¡å…ƒã‚¬ã‚¦ã‚¹é–¢æ•°ã‚’æ°´å¹³æ–¹å‘ã¨å‚ç›´æ–¹å‘ã«2å›é©ç”¨ã™ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ã«è¨ˆç®—ã§ãã¾ã™ã€‚
</span>

\[
g(x)=\frac{1}{\sqrt{2Ï€}Ïƒ}e^{-x^2/2Ïƒ^2}
\]

For key localization, all smoothing operations are done using \(Ïƒ=\sqrt{2}\), which can be approximated with suf ficient accuracy using a 1D kernel with 7 sample points.

<br><span style="color:blue;">
ã‚­ãƒ¼ã®ä½ç½®ç‰¹å®šã§ã¯ã€ã™ã¹ã¦ã®å¹³æ»‘åŒ–æ“ä½œã¯ \(Ïƒ=\sqrt{2}\) ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€7 ã¤ã®ã‚µãƒ³ãƒ—ãƒ« ãƒã‚¤ãƒ³ãƒˆã‚’æŒã¤ 1D ã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã—ã¦ååˆ†ãªç²¾åº¦ã§è¿‘ä¼¼ã§ãã¾ã™ã€‚
</span>
</p><p>

The input image is  first convolved with the Gaussian
function using \(Ïƒ=\sqrt{2}\) to give an image A. This is then repeated a second time with a further incremental smoothing of \(Ïƒ=\sqrt{2}\) to give a new image, B, which now has an effective smoothing of \(Ïƒ=2\). The difference of Gaussian functionis obtained by subtractingimage B from A, resulting in a ratio of \(2/\sqrt{2}=\sqrt{2}\) between the two Gaussians.

<br><span style="color:blue;">
å…¥åŠ›ç”»åƒã¯ã¾ãšã€\(Ïƒ=\sqrt{2}\) ã‚’ç”¨ã„ã¦ã‚¬ã‚¦ã‚¹é–¢æ•°ã§ç•³ã¿è¾¼ã¾ã‚Œã€ç”»åƒAãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚æ¬¡ã«ã€\(Ïƒ=\sqrt{2}\) ã®å¢—åˆ†å¹³æ»‘åŒ–ã‚’æ–½ã—ã¦2å›ç›®ã®ç•³ã¿è¾¼ã¿ã‚’è¡Œã„ã€å®ŸåŠ¹å¹³æ»‘åŒ–ãŒ \(Ïƒ=2\) ã§ã‚ã‚‹æ–°ã—ã„ç”»åƒBãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚ã‚¬ã‚¦ã‚¹é–¢æ•°ã®å·®ã¯ã€ç”»åƒAã‹ã‚‰ç”»åƒBã‚’æ¸›ç®—ã™ã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã€2ã¤ã®ã‚¬ã‚¦ã‚¹é–¢æ•°ã®æ¯”ã¯ \(2/\sqrt{2}=\sqrt{2}\) ã¨ãªã‚Šã¾ã™ã€‚
</span>
</p><p>

To generate the next pyramid level, we resample the al-ready smoothed image B using bilinear interpolationwith a pixel spacing of 1.5 in each direction. While it may seem
more natural to resample with a relative scale of \(\sqrt{2}\), the only constraint is that sampling be frequent enough to de-tect peaks. The 1.5 spacing means that each new sample will be a constant linear combination of 4 adjacent pixels. This is ef ficient to compute and minimizes aliasing artifacts that would arise from changing the resampling coef ficients.

<br><span style="color:blue;">
æ¬¡ã®ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ãƒ¬ãƒ™ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€æ—¢ã«å¹³æ»‘åŒ–ã•ã‚ŒãŸç”»åƒBã‚’ã€å„æ–¹å‘ã«1.5ãƒ”ã‚¯ã‚»ãƒ«é–“éš”ã§åŒç·šå½¢è£œé–“ã‚’ç”¨ã„ã¦å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚ç›¸å¯¾ã‚¹ã‚±ãƒ¼ãƒ«\(\sqrt{2}\)ã§å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹æ–¹ãŒè‡ªç„¶ã«æ€ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€å”¯ä¸€ã®åˆ¶ç´„ã¯ã€ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡ºã§ãã‚‹ã»ã©é »ç¹ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã™ã€‚1.5é–“éš”ã¯ã€å„æ–°ã—ã„ã‚µãƒ³ãƒ—ãƒ«ãŒéš£æ¥ã™ã‚‹4ã¤ã®ãƒ”ã‚¯ã‚»ãƒ«ã®å®šæ•°ç·šå½¢çµåˆã«ãªã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã‚Œã¯è¨ˆç®—åŠ¹ç‡ãŒé«˜ãã€å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¿‚æ•°ã®å¤‰æ›´ã«ã‚ˆã£ã¦ç”Ÿã˜ã‚‹ã‚¨ã‚¤ãƒªã‚¢ã‚·ãƒ³ã‚°ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’æœ€å°é™ã«æŠ‘ãˆã¾ã™ã€‚
</span>
</p><p>

Maxima and minima of this scale-space function are de-termined by comparing each pixel in the pyramid to its neighbours.  first, a pixel is compared to its 8 neighboursat the same level of the pyramid. If it is a maxima or minima at this level, then the closest pixel location is calculated at the next lowest level of the pyramid, taking account of the 1.5 times resampling. If the pixel remains higher (or lower) than this closest pixel and its 8 neighbours, then the test is repeated forthelevel above. Since most pixelswill beelim-inatedwithina fewcomparisons, thecost ofthisdetectionis small and much lower than that of buildingthe pyramid.

<br><span style="color:blue;">
ã“ã®ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“é–¢æ•°ã®æœ€å¤§å€¤ã¨æœ€å°å€¤ã¯ã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰å†…ã®å„ãƒ”ã‚¯ã‚»ãƒ«ã‚’ãã®è¿‘å‚ãƒ”ã‚¯ã‚»ãƒ«ã¨æ¯”è¼ƒã™ã‚‹ã“ã¨ã§æ±ºå®šã•ã‚Œã¾ã™ã€‚ã¾ãšã€ã‚ã‚‹ãƒ”ã‚¯ã‚»ãƒ«ã‚’ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã®åŒã˜ãƒ¬ãƒ™ãƒ«ã«ã‚ã‚‹8ã¤ã®è¿‘å‚ãƒ”ã‚¯ã‚»ãƒ«ã¨æ¯”è¼ƒã—ã¾ã™ã€‚ã“ã®ãƒ¬ãƒ™ãƒ«ã§æœ€å¤§å€¤ã¾ãŸã¯æœ€å°å€¤ã¨ãªã‚‹å ´åˆã€1.5å€ã®å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è€ƒæ…®ã—ã¦ã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã®æ¬¡ã«ä½ã„ãƒ¬ãƒ™ãƒ«ã§æœ€ã‚‚è¿‘ã„ãƒ”ã‚¯ã‚»ãƒ«ã®ä½ç½®ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ã“ã®æœ€ã‚‚è¿‘ã„ãƒ”ã‚¯ã‚»ãƒ«ã¨ãã®8ã¤ã®è¿‘å‚ãƒ”ã‚¯ã‚»ãƒ«ã‚ˆã‚Šã‚‚é«˜ã„ï¼ˆã¾ãŸã¯ä½ã„ï¼‰ã¾ã¾ã§ã‚ã‚‹å ´åˆã€ä¸Šã®ãƒ¬ãƒ™ãƒ«ã«å¯¾ã—ã¦åŒã˜ãƒ†ã‚¹ãƒˆã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚ã»ã¨ã‚“ã©ã®ãƒ”ã‚¯ã‚»ãƒ«ã¯æ•°å›ã®æ¯”è¼ƒã§é™¤å»ã•ã‚Œã‚‹ãŸã‚ã€ã“ã®æ¤œå‡ºã«ã‹ã‹ã‚‹ã‚³ã‚¹ãƒˆã¯å°ã•ãã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã‚’æ§‹ç¯‰ã™ã‚‹ã‚³ã‚¹ãƒˆã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ä½ããªã‚Šã¾ã™ã€‚
</span>
</p><p>

If the first level of the pyramid is sampled at the same rate 
as the input image, the highest spatial frequencies will be ignored. 
This is due to the initial smoothing, which is needed 
to provide separation of peaks for robust detection. Therefore, 
we expand the input image by a factor of 2, using bilinear 
interpolation, prior to building the pyramid. This gives 
on the order of 1000 key points for a typical 512Ã—512 pixel 
image, compared to only a quarter as many without the initial 
expansion. 

<br><span style="color:blue;">
ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã®æœ€åˆã®ãƒ¬ãƒ™ãƒ«ãŒå…¥åŠ›ç”»åƒã¨åŒã˜ãƒ¬ãƒ¼ãƒˆã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã¨ã€æœ€ã‚‚é«˜ã„ç©ºé–“å‘¨æ³¢æ•°ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚
ã“ã‚Œã¯ã€ãƒ­ãƒã‚¹ãƒˆãªæ¤œå‡ºã®ãŸã‚ã«ãƒ”ãƒ¼ã‚¯ã‚’åˆ†é›¢ã™ã‚‹ãŸã‚ã«å¿…è¦ãªåˆæœŸå¹³æ»‘åŒ–ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚ãã®ãŸã‚ã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã‚’æ§‹ç¯‰ã™ã‚‹å‰ã«ã€åŒç·šå½¢è£œé–“ã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ç”»åƒã‚’2å€ã«æ‹¡å¤§ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å…¸å‹çš„ãª512Ã—512ãƒ”ã‚¯ã‚»ãƒ«ã®ç”»åƒã§ã¯ã€ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æ•°ã¯ç´„1000å€‹ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€åˆæœŸæ‹¡å¤§ã‚’è¡Œã‚ãªã„å ´åˆã®4åˆ†ã®1ã«ç›¸å½“ã—ã¾ã™ã€‚
</span>
</p>

<h3>3.1. SIFT key stability <span style="color:blue;">SIFTã‚­ãƒ¼ã®å®‰å®šæ€§</span></h3>
<p>
To characterize the image at each key location, the smoothed 
image A at each level of the pyramid is processed to extract 
image gradients and orientations. At each pixel,\(A_{ij}\), theimage 
gradientmagnitude,\(M_{ij}\) , and orientation,\(R_{ij}\), are computed 
using pixel differences:

<br><span style="color:blue;">
å„ã‚­ãƒ¼ä½ç½®ã«ãŠã‘ã‚‹ç”»åƒã‚’ç‰¹å¾´ä»˜ã‘ã‚‹ãŸã‚ã«ã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã®å„ãƒ¬ãƒ™ãƒ«ã§å¹³æ»‘åŒ–ã•ã‚ŒãŸç”»åƒAã‚’å‡¦ç†ã—ã€ç”»åƒã®å‹¾é…ã¨æ–¹å‘ã‚’æŠ½å‡ºã—ã¾ã™ã€‚å„ãƒ”ã‚¯ã‚»ãƒ«\(A_{ij}\)ã«ãŠã„ã¦ã€ç”»åƒã®å‹¾é…ã®å¤§ãã•\(M_{ij}\)ã¨æ–¹å‘\(R_{ij}\)ãŒãƒ”ã‚¯ã‚»ãƒ«å·®ã‚’ç”¨ã„ã¦è¨ˆç®—ã•ã‚Œã¾ã™ã€‚
</span>

\[
\begin{align}
M_{ij} &= \sqrt{(A_{ij}-A_{i+1,j})^2+(A_{i,j}-A_{i,j+1})^2} \\
\\
R_{ij} &= \text{atan2}(A_{ij}-A_{i+1,j}, A_{i,j+1}-A_{ij})
\end{align}
\]

The pixel differences are efficient to compute and provide 
sufficient accuracy due to the substantial level of previous 
smoothing. The effective half-pixel shift in position is compensated 
for when determining key location. 

<br><span style="color:blue;">
ãƒ”ã‚¯ã‚»ãƒ«å·®ã®è¨ˆç®—ã¯åŠ¹ç‡çš„ã«è¡Œã‚ã‚Œã€ååˆ†ãªç²¾åº¦ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚
äº‹å‰ã®å¹³æ»‘åŒ–ãŒååˆ†ã«è¡Œã‚ã‚Œã¦ã„ã‚‹ãŸã‚ã€‚
ã‚­ãƒ¼ã®ä½ç½®ã‚’æ±ºå®šã™ã‚‹éš›ã«ã€å®ŸåŠ¹çš„ãªåŠãƒ”ã‚¯ã‚»ãƒ«ã®ä½ç½®ãšã‚ŒãŒè£œæ­£ã•ã‚Œã¾ã™ã€‚
</span>
</p><p>

Robustness to illumination change is enhanced by thresholding 
the gradient magnitudes at a value of 0.1 times the maximum possible gradient value. This reduces the effect 
of a change in illumination direction for a surface with 3D 
relief, as an illumination change may result in large changes 
to gradient magnitude but is likely to have less influence on 
gradient orientation. 

<br><span style="color:blue;">
ç…§æ˜å¤‰åŒ–ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ã¯ã€å‹¾é…ã®å¼·ã•ã‚’æœ€å¤§å‹¾é…å€¤ã®0.1å€ã®å€¤ã§é–¾å€¤è¨­å®šã™ã‚‹ã“ã¨ã§å¼·åŒ–ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€3Dãƒ¬ãƒªãƒ¼ãƒ•ã‚’æŒã¤ã‚µãƒ¼ãƒ•ã‚§ã‚¹ã«ãŠã‘ã‚‹ç…§æ˜æ–¹å‘ã®å¤‰åŒ–ã®å½±éŸ¿ãŒè»½æ¸›ã•ã‚Œã¾ã™ã€‚ç…§æ˜å¤‰åŒ–ã¯å‹¾é…ã®å¼·ã•ã«å¤§ããªå¤‰åŒ–ã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€å‹¾é…ã®å‘ãã¸ã®å½±éŸ¿ã¯å°ã•ã„ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚
</span>
</p><p>

Each key location is assigned a canonical orientation so 
that the image descriptors are invariant to rotation. In order 
to make this as stable as possible against lighting or contrast 
changes, the orientation is determined by the peak in a 
histogram of local image gradient orientations. The orientation 
histogram is created using a Gaussian-weighted window 
with \(Ïƒ\) of 3 times that of the current smoothing scale. 
These weights are multiplied by the thresholded gradient 
values and accumulated in the histogram at locations corresponding 
to the orientation,\(R_{ij}\). The histogramhas 36 bins 
covering the 360 degree range of rotations, and is smoothed 
prior to peak selection. 

<br><span style="color:blue;">
å„ã‚­ãƒ¼ä½ç½®ã«ã¯æ¨™æº–çš„ãªæ–¹å‘ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€ç”»åƒè¨˜è¿°å­ã¯å›è»¢ã«å¯¾ã—ã¦ä¸å¤‰ã§ã™ã€‚ç…§æ˜ã‚„ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã®å¤‰åŒ–ã«å¯¾ã—ã¦ã“ã‚Œã‚’å¯èƒ½ãªé™ã‚Šå®‰å®šã•ã›ã‚‹ãŸã‚ã€æ–¹å‘ã¯å±€æ‰€çš„ãªç”»åƒå‹¾é…æ–¹å‘ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®ãƒ”ãƒ¼ã‚¯ã«ã‚ˆã£ã¦æ±ºå®šã•ã‚Œã¾ã™ã€‚æ–¹å‘ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¯ã€\(Ïƒ\) ãŒç¾åœ¨ã®å¹³æ»‘åŒ–ã‚¹ã‚±ãƒ¼ãƒ«ã®3å€ã§ã‚ã‚‹ã‚¬ã‚¦ã‚¹é‡ã¿ä»˜ã‘ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½¿ç”¨ã—ã¦ä½œæˆã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã®é‡ã¿ã¯é–¾å€¤å‹¾é…å€¤ã«ä¹—ç®—ã•ã‚Œã€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å†…ã®æ–¹å‘ \(R_{ij}\) ã«å¯¾å¿œã™ã‚‹ä½ç½®ã«ç´¯ç©ã•ã‚Œã¾ã™ã€‚ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã¯360åº¦ã®å›è»¢ç¯„å›²ã‚’ã‚«ãƒãƒ¼ã™ã‚‹36ã®ãƒ“ãƒ³ã‚’æŒã¡ã€ãƒ”ãƒ¼ã‚¯é¸æŠã®å‰ã«å¹³æ»‘åŒ–ã•ã‚Œã¾ã™ã€‚
</span>
</p><p>

The stability of the resulting keys can be tested by subjecting 
natural images to affine projection, contrast and 
brightness changes, and addition of noise. The location of 
each key detected in the first image can be predicted in the 
transformed image from knowledge of the transform parameters. 
This framework was used to select the various sampling 
and smoothing parameters given above, so that maximum efficiency could be obtained while retaining stability 
to changes. 

<br><span style="color:blue;">
å¾—ã‚‰ã‚ŒãŸã‚­ãƒ¼ã®å®‰å®šæ€§ã¯ã€è‡ªç„¶ç”»åƒã«ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ã€ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã¨æ˜ã‚‹ã•ã®å¤‰åŒ–ã€ãƒã‚¤ã‚ºã®ä»˜åŠ ã‚’æ–½ã™ã“ã¨ã§ãƒ†ã‚¹ãƒˆã§ãã¾ã™ã€‚æœ€åˆã®ç”»åƒã§æ¤œå‡ºã•ã‚ŒãŸå„ã‚­ãƒ¼ã®ä½ç½®ã¯ã€å¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çŸ¥è­˜ã‹ã‚‰å¤‰æ›å¾Œã®ç”»åƒã§äºˆæ¸¬ã§ãã¾ã™ã€‚
ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€ä¸Šè¨˜ã®æ§˜ã€…ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŠã‚ˆã³ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é¸æŠã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã€å¤‰åŒ–ã«å¯¾ã™ã‚‹å®‰å®šæ€§ã‚’ç¶­æŒã—ãªãŒã‚‰æœ€å¤§ã®åŠ¹ç‡ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚
</span>
</p><p>

Figure 1 shows a relatively small number of keys detected 
over a 2 octave range of only the larger scales (to 
avoid excessive clutter). Each key is shown as a square, with 
a line from the center to one side of the square indicating orientation. 
In the second half of this figure, the image is rotated 
by 15 degrees, scaled by a factor of 0.9, and stretched 
by a factor of 1.1 in the horizontal direction. The pixel intensities, 
in the range of 0 to 1, have 0.1 subtracted from their 
brightness values and the contrast reduced by multiplication 
by 0.9. Random pixel noise is then added to give less than 
5 bits/pixel of signal. In spite of these transformations, 78% 
of the keys in the first image had closely matching keys in 
the second image at the predicted locations, scales, and orientations 

<br><span style="color:blue;">
å›³1ã¯ã€2ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ã®ç¯„å›²ã«ã‚ãŸã£ã¦ã€å¤§ããªã‚¹ã‚±ãƒ¼ãƒ«ã®ã¿ã§æ¤œå‡ºã•ã‚ŒãŸæ¯”è¼ƒçš„å°‘æ•°ã®ã‚­ãƒ¼ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼ˆéåº¦ã®æ··ä¹±ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰ã€‚å„ã‚­ãƒ¼ã¯æ­£æ–¹å½¢ã§ç¤ºã•ã‚Œã€ä¸­å¿ƒã‹ã‚‰æ­£æ–¹å½¢ã®ä¸€è¾ºã«å‘ã‹ã†ç·šã¯æ–¹å‘ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚å›³ã®å¾ŒåŠã§ã¯ã€ç”»åƒãŒ15åº¦å›è»¢ã•ã‚Œã€0.9å€ã«æ‹¡å¤§ã•ã‚Œã€æ°´å¹³æ–¹å‘ã«1.1å€ã«å¼•ãä¼¸ã°ã•ã‚Œã¦ã„ã¾ã™ã€‚0ã‹ã‚‰1ã®ç¯„å›²ã®ãƒ”ã‚¯ã‚»ãƒ«å¼·åº¦ã¯ã€è¼åº¦å€¤ã‹ã‚‰0.1ãŒå·®ã—å¼•ã‹ã‚Œã€ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã¯0.9å€ã«ä¸‹ã’ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ãã®å¾Œã€ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ”ã‚¯ã‚»ãƒ«ãƒã‚¤ã‚ºãŒè¿½åŠ ã•ã‚Œã€ä¿¡å·ã¯5ãƒ“ãƒƒãƒˆ/ãƒ”ã‚¯ã‚»ãƒ«æœªæº€ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®å¤‰æ›ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€æœ€åˆã®ç”»åƒã®ã‚­ãƒ¼ã®78%ã¯ã€äºˆæ¸¬ã•ã‚ŒãŸä½ç½®ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ãŠã‚ˆã³æ–¹å‘ã«ãŠã„ã¦ã€2ç•ªç›®ã®ç”»åƒã«ã»ã¼ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã‚’æŒã£ã¦ã„ã¾ã—ãŸã€‚
</span>
</p>

<center><img src="images/fig1.png"></center>

<p>

Figure 1: The second image was generated from the first by 
rotation, scaling, stretching, change of brightness and contrast, 
and addition of pixel noise. In spite of these changes, 
78% of the keys from the first image have a closely matching 
key in the second image. These examples show only a 
subset of the keys to reduce clutter. 

<br><span style="color:blue;">
å›³1ï¼š2æšç›®ã®ç”»åƒã¯ã€1æšç›®ã®ç”»åƒã‹ã‚‰ã€å›è»¢ã€æ‹¡å¤§ç¸®å°ã€å¼•ãä¼¸ã°ã—ã€æ˜ã‚‹ã•ã¨ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã®å¤‰æ›´ã€ãƒ”ã‚¯ã‚»ãƒ«ãƒã‚¤ã‚ºã®è¿½åŠ ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®å¤‰æ›´ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€1æšç›®ã®ç”»åƒã®ã‚­ãƒ¼ã®78%ã¯ã€2æšç›®ã®ç”»åƒã«ã‚‚ã»ã¼ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ä¾‹ã§ã¯ã€ç…©é›‘ã•ã‚’è»½æ¸›ã™ã‚‹ãŸã‚ã«ã€ã‚­ãƒ¼ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã®ã¿ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
</span>
</p><p>

The overall stability of the keys to image transformations 
can be judged from Table 2. Each entry in this table is generated 
from combining the results of 20 diverse test images 
and summarizes the matching of about 15,000 keys. Each 
line of the table shows a particular image transformation. 
The first figure gives the percent of keys that have a matching 
key in the transformed image within  in location (relative 
to scale for that key) and a factor of 1.5 in scale. The 
second column gives the percent that match these criteria as 
well as having an orientation within 20 degrees of the prediction. 

<br><span style="color:blue;">
ç”»åƒå¤‰æ›ã«å¯¾ã™ã‚‹ã‚­ãƒ¼ã®å…¨ä½“çš„ãªå®‰å®šæ€§ã¯ã€è¡¨2ã‹ã‚‰åˆ¤æ–­ã§ãã¾ã™ã€‚ã“ã®è¡¨ã®å„é …ç›®ã¯ã€20ç¨®é¡ã®ç•°ãªã‚‹ãƒ†ã‚¹ãƒˆç”»åƒã®çµæœã‚’çµ„ã¿åˆã‚ã›ã¦ç”Ÿæˆã•ã‚Œã€ç´„15,000å€‹ã®ã‚­ãƒ¼ã®ãƒãƒƒãƒãƒ³ã‚°ã‚’è¦ç´„ã—ã¦ã„ã¾ã™ã€‚è¡¨ã®å„è¡Œã¯ã€ç‰¹å®šã®ç”»åƒå¤‰æ›ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚æœ€åˆã®å›³ã¯ã€å¤‰æ›ã•ã‚ŒãŸç”»åƒã«ãŠã„ã¦ã€ä½ç½®ï¼ˆãã®ã‚­ãƒ¼ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’åŸºæº–ã¨ã—ã¦ï¼‰ã¨ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°1.5ã®ç¯„å›²å†…ã«ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã‚’æŒã¤ã‚­ãƒ¼ã®å‰²åˆã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚2ç•ªç›®ã®åˆ—ã¯ã€ã“ã‚Œã‚‰ã®åŸºæº–ã«ä¸€è‡´ã—ã€ã‹ã¤äºˆæ¸¬ã‹ã‚‰20åº¦ä»¥å†…ã®æ–¹å‘ã‚’æŒã¤ã‚­ãƒ¼ã®å‰²åˆã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
</span>

\[
\begin{array}{|l|c|c|}
\hline
\textbf{ç”»åƒå¤‰æ›} & \textbf{Match %} & \textbf{Ori %} \\
\hline
\text{A. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã‚’1.2å€} & 89.0 & 86.6 \\ 
\hline
\text{B. è¼åº¦ã‚’0.2ä¸‹ã’ã‚‹} &  88.5 & 85.9 \\ 
\hline
\text{C. 20Â°å›è»¢} &  85.4 & 81.0 \\ 
\hline
\text{D. 0.7å€ã«ç¸®å°} &  85.1 & 80.3 \\ 
\hline
\text{E. 1.2å€æ‹¡å¤§} &  83.5 & 76.1 \\ 
\hline
\text{F. 1.5å€æ‹¡å¤§} &  77.7 & 65.0 \\ 
\hline
\text{G. 10%ãƒ”ã‚¯ã‚»ãƒ«ãƒã‚¤ã‚ºä»˜åŠ } &  90.3 & 88.4 \\ 
\hline
\text{H. Aï½Gã®ã™ã¹ã¦} &  78.6 & 71.8 \\
\hline
\end{array}
\]

Figure 2: For various image transformations applied to a 
sample of 20 images, this table gives the percent of keys that 
are found at matching locations and scales (Match %) and 
that also match in orientation (Ori %).

<br><span style="color:blue;">
å›³2ï¼š20æšã®ç”»åƒã‚µãƒ³ãƒ—ãƒ«ã«æ§˜ã€…ãªç”»åƒå¤‰æ›ã‚’é©ç”¨ã—ãŸå ´åˆã€ã“ã®è¡¨ã¯ã€ä¸€è‡´ã™ã‚‹ä½ç½®ã¨ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆä¸€è‡´ç‡ï¼‰ã§è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ã¨ã€æ–¹å‘ï¼ˆæ–¹å‘ç‡ï¼‰ã‚‚ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã®å‰²åˆã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
</span>
</p>

<h2>4. Local image description <span style="color:blue;">å±€æ‰€ç”»åƒè¨˜è¿°</span></h2>
<p>
Given a stable location, scale, and orientation for each key, it 
is now possible to describe the local image region in a mannerinvarianttothesetransformations. 
Inaddition,itisdesirable 
to make this representation robust against small shifts 
in local geometry, such as arise from affine or 3D projection. 
One approach to this is suggested by the response properties 
of complex neurons in the visual cortex, in which a feature 
position is allowed to vary over a small region while orientation 
and spatial frequency specificity are maintained. Edelman, 
Intrator & Poggio [5] have performed experiments that 
simulated the responses of complex neurons to different 3D 
views of computer graphic models, and found that the complex 
cell outputs provided much better discrimination than 
simple correlation-based matching. This can be seen, for example, 
if an affine projection stretches an image in one direction 
relative to another, which changes the relative locations 
of gradient features while having a smaller effect on 
their orientations and spatial frequencies. 

<br><span style="color:blue;">
å„ã‚­ãƒ¼ã®ä½ç½®ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ãŠã‚ˆã³æ–¹å‘ãŒå®‰å®šã—ã¦ã„ã‚Œã°ã€
ã“ã‚Œã‚‰ã®å¤‰æ›ã«å¯¾ã—ã¦ä¸å¤‰ãªæ–¹æ³•ã§å±€æ‰€çš„ãªç”»åƒé ˜åŸŸã‚’è¨˜è¿°ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
ã•ã‚‰ã«ã€ã“ã®è¡¨ç¾ã‚’ã€ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ã‚„3DæŠ•å½±ãªã©ã«ã‚ˆã£ã¦ç”Ÿã˜ã‚‹å±€æ‰€çš„ãªå¹¾ä½•å­¦çš„å¤‰åŒ–ã«å¯¾ã—ã¦å …ç‰¢ã«ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„ã§ã™ã€‚
ã“ã®ãŸã‚ã®1ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€è¦–è¦šçš®è³ªã®è¤‡é›‘ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å¿œç­”ç‰¹æ€§ã«ã‚ˆã£ã¦ç¤ºå”†ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ç‰¹æ€§ã§ã¯ã€ç‰¹å¾´ã®ä½ç½®ã¯å°ã•ãªé ˜åŸŸå†…ã§å¤‰åŒ–ã—ã¾ã™ãŒã€æ–¹å‘ã¨ç©ºé–“å‘¨æ³¢æ•°ã®ç‰¹ç•°æ€§ã¯ç¶­æŒã•ã‚Œã¾ã™ã€‚Edelmanã€Intratorã€Poggio [5]ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ç•°ãªã‚‹3Dãƒ“ãƒ¥ãƒ¼ã«å¯¾ã™ã‚‹è¤‡é›‘ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å¿œç­”ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹å®Ÿé¨“ã‚’è¡Œã„ã€è¤‡é›‘ãªç´°èƒå‡ºåŠ›ã¯å˜ç´”ãªç›¸é–¢ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒãƒãƒ³ã‚°ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«å„ªã‚ŒãŸè­˜åˆ¥ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€ä¾‹ãˆã°ã€ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ãŒç”»åƒã‚’ã‚ã‚‹æ–¹å‘ã«å¼•ãä¼¸ã°ã—ã€åˆ¥ã®æ–¹å‘ã«å¼•ãä¼¸ã°ã™å ´åˆã«è¦‹ã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å‹¾é…ç‰¹å¾´ã®ç›¸å¯¾çš„ãªä½ç½®ã¯å¤‰åŒ–ã—ã¾ã™ãŒã€å‹¾é…ç‰¹å¾´ã®æ–¹å‘ã¨ç©ºé–“å‘¨æ³¢æ•°ã¸ã®å½±éŸ¿ã¯å°ã•ããªã‚Šã¾ã™ã€‚
</span>
</p><p>

This robustness to local geometric distortion can be obtained 
by representing the local image region with multiple 
images representing each of a number of orientations (referred 
to as orientation planes). Each orientation plane contains 
only the gradients corresponding to that orientation, 
with linear interpolation used for intermediate orientations. 
Each orientation plane is blurred and resampled to allow for 
larger shifts in positions of the gradients. 

<br><span style="color:blue;">
å±€æ‰€çš„ãªå¹¾ä½•å­¦çš„æ­ªã¿ã«å¯¾ã™ã‚‹ã“ã®å …ç‰¢æ€§ã¯ã€å±€æ‰€çš„ãªç”»åƒé ˜åŸŸã‚’è¤‡æ•°ã®æ–¹å‘ï¼ˆæ–¹å‘å¹³é¢ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã®ãã‚Œãã‚Œã‚’è¡¨ã™è¤‡æ•°ã®ç”»åƒã§è¡¨ç¾ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å®Ÿç¾ã•ã‚Œã¾ã™ã€‚å„æ–¹å‘å¹³é¢ã«ã¯ã€ãã®æ–¹å‘ã«å¯¾å¿œã™ã‚‹å‹¾é…ã®ã¿ãŒå«ã¾ã‚Œã€ä¸­é–“ã®æ–¹å‘ã«ã¤ã„ã¦ã¯ç·šå½¢è£œé–“ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚å„æ–¹å‘å¹³é¢ã¯ã€å‹¾é…ã®ä½ç½®ã®ã‚ˆã‚Šå¤§ããªã‚·ãƒ•ãƒˆã‚’è¨±å®¹ã™ã‚‹ãŸã‚ã«ã€ã¼ã‹ã—ã¨å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒè¡Œã‚ã‚Œã¾ã™ã€‚
</span>
</p><p>

This approach can be efficiently implemented by using 
the same precomputed gradients and orientations for each 
level of the pyramid that were used for orientation selection. 
For each keypoint, we use the pixel sampling from the pyramid 
level at which the key was detected. The pixels that fall 
in a circle of radius 8 pixels around the key location are inserted 
into the orientation planes. The orientation is measured 
relative to that of the key by subtracting the keyâ€™s orientation. 
For our experiments we used 8 orientation planes, 
each sampled over a 4 4grid of locations, with a sample 
spacing 4 times that of the pixel spacing used for gradient 
detection. The blurring is achieved by allocating the gradient 
of each pixel among its 8 closest neighbors in the sample 
grid, using linear interpolation in orientation and the two 
spatial dimensions. This implementation is much more efficient 
than performing explicit blurring and resampling, yet 
gives almost equivalent results. 

<br><span style="color:blue;">
ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã®å„ãƒ¬ãƒ™ãƒ«ã«å¯¾ã—ã¦ã€æ–¹å‘é¸æŠã«ä½¿ç”¨ã—ãŸã®ã¨åŒã˜äº‹å‰è¨ˆç®—æ¸ˆã¿ã®å‹¾é…ã¨æ–¹å‘ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ã«å®Ÿè£…ã§ãã¾ã™ã€‚
å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã«ã¤ã„ã¦ã€ã‚­ãƒ¼ãŒæ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ©ãƒŸãƒƒãƒ‰ãƒ¬ãƒ™ãƒ«ã‹ã‚‰ã®ãƒ”ã‚¯ã‚»ãƒ«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚­ãƒ¼ã®ä½ç½®ã‹ã‚‰åŠå¾„8ãƒ”ã‚¯ã‚»ãƒ«ã®å††å†…ã«ã‚ã‚‹ãƒ”ã‚¯ã‚»ãƒ«ãŒã€æ–¹å‘å¹³é¢ã«æŒ¿å…¥ã•ã‚Œã¾ã™ã€‚æ–¹å‘ã¯ã€ã‚­ãƒ¼ã®æ–¹å‘ã‚’å·®ã—å¼•ãã“ã¨ã§ã€ã‚­ãƒ¼ã®æ–¹å‘ã‚’åŸºæº–ã¨ã—ã¦æ¸¬å®šã•ã‚Œã¾ã™ã€‚
å®Ÿé¨“ã§ã¯ã€8ã¤ã®æ–¹å‘å¹³é¢ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚å„å¹³é¢ã¯4Ã—4ã‚°ãƒªãƒƒãƒ‰ã®ä½ç½®ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã€ã‚µãƒ³ãƒ—ãƒ«é–“éš”ã¯å‹¾é…æ¤œå‡ºã«ä½¿ç”¨ã—ãŸãƒ”ã‚¯ã‚»ãƒ«é–“éš”ã®4å€ã§ã™ã€‚ã¼ã‹ã—ã¯ã€æ–¹å‘ã¨2ã¤ã®ç©ºé–“æ¬¡å…ƒã®ç·šå½¢è£œé–“ã‚’ä½¿ç”¨ã—ã¦ã€ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒªãƒƒãƒ‰å†…ã®8ã¤ã®æœ€ã‚‚è¿‘ã„è¿‘å‚ãƒ”ã‚¯ã‚»ãƒ«ã«å„ãƒ”ã‚¯ã‚»ãƒ«ã®å‹¾é…ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã¾ã™ã€‚ã“ã®å®Ÿè£…ã¯ã€æ˜ç¤ºçš„ãªã¼ã‹ã—ã¨ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«åŠ¹ç‡çš„ã§ã‚ã‚ŠãªãŒã‚‰ã€
ã»ã¼åŒç­‰ã®çµæœã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚
</span>
</p><p>

In order to sample the image at a larger scale, the same 
process is repeated for a second level of the pyramid one octave 
higher. However, this time a 2Ã—2rather than a 4Ã—4 
sample region is used. This means that approximately the 
same image region will be examined at both scales, so that 
any nearby occlusions will not affect one scale more than the 
other. Therefore, the total number of samples in the SIFT 
key vector, from both scales, is 8Ã—4Ã—4+8Ã—2Ã—2 or 160 
elements, giving enough measurements for high specificity. 

<br><span style="color:blue;">
ã‚ˆã‚Šå¤§ããªã‚¹ã‚±ãƒ¼ãƒ«ã§ç”»åƒã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ã€ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã®1ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ä¸Šã®2ç•ªç›®ã®ãƒ¬ãƒ™ãƒ«ã«å¯¾ã—ã¦åŒã˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚ãŸã ã—ã€ä»Šå›ã¯4Ã—4ã§ã¯ãªã2Ã—2ã®ã‚µãƒ³ãƒ—ãƒ«é ˜åŸŸã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ä¸¡æ–¹ã®ã‚¹ã‚±ãƒ¼ãƒ«ã§ã»ã¼åŒã˜ç”»åƒé ˜åŸŸãŒæ¤œæŸ»ã•ã‚Œã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ãã®ãŸã‚ã€è¿‘ãã®é®è”½ç‰©ãŒä¸€æ–¹ã®ã‚¹ã‚±ãƒ¼ãƒ«ã«ä»–æ–¹ã‚ˆã‚Šã‚‚å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã—ãŸãŒã£ã¦ã€SIFTã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã¯ã€ä¸¡æ–¹ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‹ã‚‰åˆè¨ˆ8Ã—4Ã—4+8Ã—2Ã—2ã€ã¤ã¾ã‚Š160è¦ç´ ã¨ãªã‚Šã€é«˜ã„ç‰¹ç•°åº¦ã‚’å¾—ã‚‹ã®ã«ååˆ†ãªæ¸¬å®šå€¤ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚
</span>
</p>

<h2>5. Indexing and matching <span style="color:blue;">ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æŒ¯ã‚Šã¨ãƒãƒƒãƒãƒ³ã‚°</span></h2>
<p>
For indexing, we need to store the SIFT keys for sample images 
and then identify matching keys from new images. The 
problem of identifyingthe most similar keys for high dimensional vectors is known to have high complexity if an exact 
solution is required. However, a modification of the k-d 
tree algorithm called the best-bin-first search method (Beis 
& Lowe [3]) can identify the nearest neighbors with high 
probability using only a limited amount of computation. To 
furtherimprovetheefficiency ofthebest-bin-firstalgorithm, 
the SIFT key samples generated at the larger scale are given 
twice the weight of those at the smaller scale. This means 
that the larger scale is in effect able to filter the most likely 
neighbours for checking at the smaller scale. This also improves 
recognition performance by giving more weight to 
the least-noisy scale. In our experiments, it is possible to 
have a cut-off for examining at most 200 neighbors in a 
probabilisticbest-bin-firstsearch of30,000key vectors with 
almost no loss of performance compared to finding an exact 
solution. 

<br><span style="color:blue;">
ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã®ãŸã‚ã«ã¯ã€ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã®SIFTã‚­ãƒ¼ã‚’ä¿å­˜ã—ã€æ–°ã—ã„ç”»åƒã‹ã‚‰ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã‚’è­˜åˆ¥ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã®æœ€ã‚‚é¡ä¼¼ã—ãŸã‚­ãƒ¼ã‚’è­˜åˆ¥ã™ã‚‹å•é¡Œã¯ã€æ­£ç¢ºãªè§£ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆã€éå¸¸ã«è¤‡é›‘ã«ãªã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€k-dãƒ„ãƒªãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¿®æ­£ç‰ˆã§ã‚ã‚‹best-bin-firstæ¢ç´¢æ³•ï¼ˆBeis & Lowe [3]ï¼‰ã¯ã€é™ã‚‰ã‚ŒãŸè¨ˆç®—é‡ã§ã€é«˜ã„ç¢ºç‡ã§æœ€è¿‘å‚ç‚¹ã‚’è­˜åˆ¥ã§ãã¾ã™ã€‚best-bin-firstã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®åŠ¹ç‡ã‚’ã•ã‚‰ã«å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ã‚ˆã‚Šå¤§ããªã‚¹ã‚±ãƒ¼ãƒ«ã§ç”Ÿæˆã•ã‚ŒãŸSIFTã‚­ãƒ¼ã‚µãƒ³ãƒ—ãƒ«ã«ã¯ã€ã‚ˆã‚Šå°ã•ãªã‚¹ã‚±ãƒ¼ãƒ«ã®2å€ã®é‡ã¿ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚ˆã‚Šå¤§ããªã‚¹ã‚±ãƒ¼ãƒ«ãŒã€ã‚ˆã‚Šå°ã•ãªã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ã«æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„è¿‘å‚ç‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒã‚¤ã‚ºã®æœ€ã‚‚å°‘ãªã„ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤šãã®é‡ã¿ãŒä¸ãˆã‚‰ã‚Œã‚‹ãŸã‚ã€èªè­˜æ€§èƒ½ã‚‚å‘ä¸Šã—ã¾ã™ã€‚ç§ãŸã¡ã®å®Ÿé¨“ã§ã¯ã€30,000å€‹ã®ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã®ç¢ºç‡çš„ãƒ™ã‚¹ãƒˆãƒ“ãƒ³ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚µãƒ¼ãƒã«ãŠã„ã¦ã€æœ€å¤§200å€‹ã®è¿‘å‚ã‚’èª¿ã¹ã‚‹ã¨ã„ã†ã‚«ãƒƒãƒˆã‚ªãƒ•ã‚’è¨­ã‘ã¦ã‚‚ã€æ­£ç¢ºãªè§£ã‚’æ±‚ã‚ã‚‹å ´åˆã¨æ¯”ã¹ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ã¯ã»ã¨ã‚“ã©è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚
</span>
</p><p>

An efficient way to cluster reliable model hypotheses 
is to use the Hough transform [1] to search for keys that 
agree upon a particular model pose. Each model key in the 
database contains a record of the keyâ€™s parameters relative 
to the model coordinate system. Therefore, we can create 
an entry in a hash table predicting the model location, orientation, 
and scale from the match hypothesis. We use a 
bin size of 30 degrees for orientation, a factor of 2 for scale, 
and 0.25 times the maximum model dimension for location. 
These rather broad bin sizes allow for clustering even in the 
presence of substantial geometric distortion, such as due to a 
change in 3D viewpoint. To avoid the problem of boundary 
effects in hashing, each hypothesis is hashed into the 2 closest 
bins in each dimension, giving a total of 16 hash table 
entries for each hypothesis. 

<br><span style="color:blue;">
ä¿¡é ¼æ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ä»®èª¬ã‚’åŠ¹ç‡çš„ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã¯ã€ãƒãƒ•å¤‰æ› [1] ã‚’ä½¿ç”¨ã—ã¦ã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚ºã«ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®å„ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«åº§æ¨™ç³»ã‚’åŸºæº–ã¨ã—ãŸã‚­ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨˜éŒ²ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€ä¸€è‡´ã™ã‚‹ä»®èª¬ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®ä½ç½®ã€æ–¹å‘ã€ã‚¹ã‚±ãƒ¼ãƒ«ã‚’äºˆæ¸¬ã™ã‚‹ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆã§ãã¾ã™ã€‚æ–¹å‘ã«ã¯30åº¦ã€ã‚¹ã‚±ãƒ¼ãƒ«ã«ã¯ä¿‚æ•°2ã€ä½ç½®ã«ã¯æœ€å¤§ãƒ¢ãƒ‡ãƒ«æ¬¡å…ƒã®0.25å€ã®ãƒ“ãƒ³ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ¯”è¼ƒçš„åºƒã„ãƒ“ãƒ³ã‚µã‚¤ã‚ºã«ã‚ˆã‚Šã€3Dè¦–ç‚¹ã®å¤‰åŒ–ãªã©ã«ã‚ˆã‚‹å¤§ããªå¹¾ä½•å­¦çš„æ­ªã¿ãŒã‚ã‚‹å ´åˆã§ã‚‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ãƒãƒƒã‚·ãƒ¥ã«ãŠã‘ã‚‹å¢ƒç•ŒåŠ¹æœã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€å„ä»®èª¬ã¯å„æ¬¡å…ƒã§æœ€ã‚‚è¿‘ã„2ã¤ã®ãƒ“ãƒ³ã«ãƒãƒƒã‚·ãƒ¥ã•ã‚Œã€å„ä»®èª¬ã«å¯¾ã—ã¦åˆè¨ˆ16å€‹ã®ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ãƒ³ãƒˆãƒªãŒä½œæˆã•ã‚Œã¾ã™ã€‚
</span>
</p>

<h2>6. Solution for affine parameters <span style="color:blue;">ã‚¢ãƒ•ã‚£ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£</span></h2>
<p>
The hash table is searched to identify all clusters of at least 
3 entries in a bin, and the bins are sorted into decreasing order 
of size. Each such cluster is then subject to a verification 
procedure in which a least-squares solution is performed for 
the affine projection parameters relating the model to the image. 

<br><span style="color:blue;">
ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œç´¢ã—ã€ãƒ“ãƒ³ã«3ã¤ä»¥ä¸Šã®ã‚¨ãƒ³ãƒˆãƒªã‚’æŒã¤ã™ã¹ã¦ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ç‰¹å®šã—ã€ãƒ“ãƒ³ã¯ã‚µã‚¤ã‚ºã®é™é †ã§ã‚½ãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚æ¬¡ã«ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å¯¾ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã¨ç”»åƒã‚’é–¢é€£ä»˜ã‘ã‚‹ã‚¢ãƒ•ã‚£ãƒ³æŠ•å½±ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¤ã„ã¦æœ€å°äºŒä¹—æ³•ã‚’å®Ÿè¡Œã™ã‚‹æ¤œè¨¼æ‰‹é †ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
</span>
</p><p>

The affine transformation of a model point \([x y]^\top\) to an 
image point \([u v]^\top\) can be written as 

<br><span style="color:blue;">
ãƒ¢ãƒ‡ãƒ«ç‚¹\([x y]^\top\)ã‹ã‚‰ç”»åƒç‚¹\([u v]^\top\)ã¸ã®ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹ã€‚
</span>

\[
\begin{bmatrix}
u \\
v
\end{bmatrix}
=
\begin{bmatrix}
m_1 & m_2 \\
m_3 & m_4
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
+
\begin{bmatrix}
t_x \\
t_y
\end{bmatrix}
\]

where the model translation is \([t_x t_y]^\top\) and the affine rotation, 
scale, and stretch are represented by the \(m_i\) parameters. 

<br><span style="color:blue;">
ã“ã“ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ç§»å‹•ã¯\([t_x t_y]^\top\)ã§ã‚ã‚Šã€ã‚¢ãƒ•ã‚£ãƒ³å›è»¢ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ä¼¸ç¸®ã¯\(m_i\)ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦è¡¨ã•ã‚Œã¾ã™ã€‚
</span>
</p><p>

We wish to solve for the transformation parameters, so the equation above can be rewritten as 

<br><span style="color:blue;">
å¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è§£ããŸã„ã®ã§ã€ä¸Šã®å¼ã¯æ¬¡ã®ã‚ˆã†ã«æ›¸ãç›´ã™ã“ã¨ãŒã§ãã‚‹ã€‚
</span>

\[
\begin{bmatrix}
x & y & 0 & 0 & 1 & 0 \\
0 & 0 & x & y & 0 & 1 \\
  &   & \cdots & & & \\
  &   & \cdots & & & \\
\end{bmatrix}
\begin{bmatrix}
m_1 \\
m_2 \\
m_3 \\
m_4 \\
t_x \\
t_y 
\end{bmatrix}
=
\begin{bmatrix}
u \\
v \\
\vdots
\end{bmatrix}
\]

This equation shows a single match, but any number of further 
matches can be added, with each match contributing 
two more rows to the first and last matrix. At least 3 matches 
are needed to provide a solution. 

<br><span style="color:blue;">
ã“ã®å¼ã¯1ã¤ã®ä¸€è‡´ã‚’ç¤ºã—ã¦ã„ã¾ã™ãŒã€ã•ã‚‰ã«ä»»æ„ã®æ•°ã®ä¸€è‡´ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãŒã§ãã€å„ä¸€è‡´ã¯æœ€åˆã®è¡Œåˆ—ã¨æœ€å¾Œã®è¡Œåˆ—ã«2è¡Œãšã¤è¿½åŠ ã•ã‚Œã¾ã™ã€‚è§£ã‚’å¾—ã‚‹ã«ã¯å°‘ãªãã¨ã‚‚3ã¤ã®ä¸€è‡´ãŒå¿…è¦ã§ã™ã€‚
</span>
</p><p>

We can write this linear system as 

<br><span style="color:blue;">
ã“ã®ç·šå½¢ã‚·ã‚¹ãƒ†ãƒ ã¯æ¬¡ã®ã‚ˆã†ã«æ›¸ã‘ã‚‹ã€‚
</span>

\[
\mathbf{Ax} = \mathbf{b} 
\]

The least-squares solution for the parameters \(\mathbf{x}\) can be determined by solving the corresponding normal equations, 

<br><span style="color:blue;">
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\(\mathbf{x}\)ã®æœ€å°äºŒä¹—è§£ã¯ã€å¯¾å¿œã™ã‚‹æ­£è¦æ–¹ç¨‹å¼ã‚’è§£ãã“ã¨ã«ã‚ˆã£ã¦æ±ºå®šã§ãã‚‹ã€‚
</span>

\[
\mathbf{x}=[\mathbf{A}^\top\mathbf{A}]^{-1}\mathbf{A}^\top\mathbf{b}
\] 

which minimizes the sum of the squares of the distances 
from the projected model locations to the corresponding image 
locations. This least-squares approach could readily be 
extended to solving for 3D pose and internal parameters of 
articulated and flexible objects [12]. 

<br><span style="color:blue;">
ã“ã‚Œã¯ã€æŠ•å½±ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ä½ç½®ã‹ã‚‰å¯¾å¿œã™ã‚‹ç”»åƒã®ä½ç½®ã¾ã§ã®è·é›¢ã®äºŒä¹—å’Œã‚’æœ€å°åŒ–ã™ã‚‹ã‚‚ã®ã§ã™ã€‚ã“ã®æœ€å°äºŒä¹—æ³•ã¯ã€é–¢ç¯€å‹ãŠã‚ˆã³æŸ”è»Ÿä½“ã®3æ¬¡å…ƒå§¿å‹¢ã¨å†…éƒ¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£ã‚’æ±‚ã‚ã‚‹ã“ã¨ã«å®¹æ˜“ã«æ‹¡å¼µã§ãã¾ã™[12]ã€‚
</span>
</p><p>

Outliers can now be removed by checking for agreement 
between each image feature and the model, given the parameter 
solution. Each match must agree within 15 degrees orientation, \(\sqrt{2}\) change in scale, and 0.2 times maximum model size in terms of location. If fewer than 3 points remain after 
discarding outliers, then the match is rejected. If any outliers 
are discarded, the least-squares solution is re-solved with the 
remaining points. 

<br><span style="color:blue;">
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€å„ç”»åƒç‰¹å¾´ã¨ãƒ¢ãƒ‡ãƒ«é–“ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã“ã¨ã§ã€å¤–ã‚Œå€¤ã‚’é™¤å»ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å„ä¸€è‡´ã¯ã€æ–¹å‘15åº¦ä»¥å†…ã€ã‚¹ã‚±ãƒ¼ãƒ«å¤‰åŒ–\(\sqrt{2}\)ä»¥å†…ã€ä½ç½®ã«é–¢ã—ã¦ã¯ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§ã‚µã‚¤ã‚ºã®0.2å€ä»¥å†…ã§ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å¤–ã‚Œå€¤ã‚’é™¤å¤–ã—ãŸå¾Œã«æ®‹ã‚‹ç‚¹ãŒ3ç‚¹æœªæº€ã®å ´åˆã€ä¸€è‡´ã¯æ‹’å¦ã•ã‚Œã¾ã™ã€‚å¤–ã‚Œå€¤ãŒé™¤å¤–ã•ã‚ŒãŸå ´åˆã¯ã€æ®‹ã‚Šã®ç‚¹ã‚’ç”¨ã„ã¦æœ€å°äºŒä¹—è§£ãŒå†åº¦æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚
</span>
</p>

<h2>7. Experiments <span style="color:blue;">å®Ÿé¨“</span></h2>
<p>
The affine solution provides a good approximation to perspective 
projection of planar objects, so planar models provide 
a good initial test of the approach. The top row of Figure 
3 shows three model images of rectangular planar faces 
of objects. The figure also shows a cluttered image containing 
the planar objects, and the same image is shown overlayed 
with the models following recognition. The model 
keys that are displayed are the ones used for recognition and 
final least-squares solution. Since only 3 keys are needed 
for robust recognition, it can be seen that the solutions are 
highly redundant and would survive substantial occlusion. 
Also shown are the rectangular borders of the model images, 
projected using the affine transform from the least-square 
solution. These closely agree with the true borders of the 
planar regions in the image, except for small errors introduced 
by the perspective projection. Similar experiments 
have been performed for many images of planar objects, and 
the recognition has proven to be robust to at least a 60 degree 
rotation of the object in any direction away from the camera. 

<br><span style="color:blue;">
ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã¯å¹³é¢ç‰©ä½“ã®é€è¦–æŠ•å½±ã®è‰¯ã„è¿‘ä¼¼å€¤ã‚’æä¾›ã™ã‚‹ãŸã‚ã€å¹³é¢ãƒ¢ãƒ‡ãƒ«ã¯
ã“ã®æ‰‹æ³•ã®åˆæœŸãƒ†ã‚¹ãƒˆã«é©ã—ã¦ã„ã¾ã™ã€‚å›³3ã®ä¸€ç•ªä¸Šã®è¡Œã¯ã€ç‰©ä½“ã®é•·æ–¹å½¢å¹³é¢ã®ãƒ¢ãƒ‡ãƒ«ç”»åƒ3æšã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚å›³ã«ã¯ã€å¹³é¢ç‰©ä½“ã‚’å«ã‚€é›‘ç„¶ã¨ã—ãŸç”»åƒã‚‚ç¤ºã•ã‚Œã¦ãŠã‚Šã€åŒã˜ç”»åƒãŒèªè­˜å¾Œã®ãƒ¢ãƒ‡ãƒ«ã¨é‡ã­ã¦è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ã¯ã€èªè­˜ã¨æœ€çµ‚çš„ãªæœ€å°äºŒä¹—è§£ã«ä½¿ç”¨ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚å …ç‰¢ãªèªè­˜ã«ã¯3ã¤ã®ã‚­ãƒ¼ã—ã‹å¿…è¦ãªã„ãŸã‚ã€è§£ã¯éå¸¸ã«å†—é•·ã§ã‚ã‚Šã€å¤§ããªé®è”½ãŒã‚ã£ã¦ã‚‚è€ãˆã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã¾ãŸã€æœ€å°äºŒä¹—è§£ã‹ã‚‰ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã‚’ä½¿ç”¨ã—ã¦æŠ•å½±ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ç”»åƒã®é•·æ–¹å½¢ã®å¢ƒç•Œã‚‚ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ã€é€è¦–æŠ•å½±ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸå°ã•ãªèª¤å·®ã‚’é™¤ã‘ã°ã€ç”»åƒå†…ã®å¹³é¢é ˜åŸŸã®å®Ÿéš›ã®å¢ƒç•Œã¨ã»ã¼ä¸€è‡´ã—ã¦ã„ã¾ã™ã€‚åŒæ§˜ã®å®Ÿé¨“ãŒå¤šãã®å¹³é¢ç‰©ä½“ã®ç”»åƒã«å¯¾ã—ã¦è¡Œã‚ã‚Œã€ã‚«ãƒ¡ãƒ©ã‹ã‚‰é›¢ã‚ŒãŸä»»æ„ã®æ–¹å‘ã«ç‰©ä½“ã‚’å°‘ãªãã¨ã‚‚60åº¦å›è»¢ã•ã›ã¦ã‚‚èªè­˜ãŒå …ç‰¢ã§ã‚ã‚‹ã“ã¨ãŒè¨¼æ˜ã•ã‚Œã¾ã—ãŸã€‚
</span>
</p>

<center><img src="images/fig3.png"></center>

<p>

Figure 3: Model images of planar objects are shown in the 
toprow. Recognitionresultsbelowshowmodeloutlinesand 
image keys used for matching. 

<br><span style="color:blue;">
å›³3ï¼šå¹³é¢ç‰©ä½“ã®ãƒ¢ãƒ‡ãƒ«ç”»åƒã‚’ä¸Šæ®µã«ç¤ºã—ã¾ã™ã€‚
ä¸‹ã®èªè­˜çµæœã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ã¨ã€
ãƒãƒƒãƒãƒ³ã‚°ã«ä½¿ç”¨ã—ãŸç”»åƒã‚­ãƒ¼ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
</span>
</p><p>

Although the model images and affine parameters do not 
account for rotation in depth of 3D objects, they are still 
sufficient to perform robust recognition of 3D objects over 
about a 20 degree range of rotation in depth away from each 
model view. An example of three model images is shown in the top row of Figure 4. The models were photographed on a 
black background, and object outlines extracted by segmenting out the background region. An example of recognition is shown in the same figure, again showing the SIFT keys used for recognition. The object outlines are projected using the 
affine parameter solution, but this time the agreement is not 
as close because the solution does not account for rotation 
in depth. Figure 5 shows more examples in which there is 
significant partial occlusion. 

<br><span style="color:blue;">
ãƒ¢ãƒ‡ãƒ«ç”»åƒã¨ã‚¢ãƒ•ã‚£ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å¥¥è¡Œãæ–¹å‘ã®å›è»¢ã‚’è€ƒæ…®ã—ã¦ã„ã¾ã›ã‚“ãŒã€å„ãƒ¢ãƒ‡ãƒ«ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ç´„20åº¦ã®ç¯„å›²ã§å¥¥è¡Œãæ–¹å‘ã«å›è»¢ã™ã‚‹3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å …ç‰¢ãªèªè­˜ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ååˆ†ã§ã™ã€‚å›³4ã®ä¸€ç•ªä¸Šã®è¡Œã«ã€3ã¤ã®ãƒ¢ãƒ‡ãƒ«ç”»åƒã®ä¾‹ã‚’ç¤ºã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯é»’ã„èƒŒæ™¯ã§æ’®å½±ã•ã‚Œã€èƒŒæ™¯é ˜åŸŸã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã™ã‚‹ã“ã¨ã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¼ªéƒ­ãŒæŠ½å‡ºã•ã‚Œã¾ã—ãŸã€‚åŒã˜å›³ã«èªè­˜ä¾‹ã‚’ç¤ºã—ã€èªè­˜ã«ä½¿ç”¨ã—ãŸSIFTã‚­ãƒ¼ã‚’å†ã³ç¤ºã—ã¦ã„ã¾ã™ã€‚ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¼ªéƒ­ã¯ã‚¢ãƒ•ã‚£ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£ã‚’ä½¿ç”¨ã—ã¦æŠ•å½±ã•ã‚Œã¦ã„ã¾ã™ãŒã€ä»Šå›ã¯è§£ãŒå¥¥è¡Œãæ–¹å‘ã®å›è»¢ã‚’è€ƒæ…®ã—ã¦ã„ãªã„ãŸã‚ã€ä¸€è‡´ã¯ãã‚Œã»ã©é«˜ãã‚ã‚Šã¾ã›ã‚“ã€‚å›³5ã¯ã€éƒ¨åˆ†çš„ãªé®è”½ãŒé¡•è‘—ãªä¾‹ã‚’ã•ã‚‰ã«ç¤ºã—ã¦ã„ã¾ã™ã€‚
</span>
</p>

<center><img src="images/fig4.png"></center>

<p>

Figure 4: Top row shows model images for 3D objects with 
outlines found by background segmentation. Bottom image 
shows recognition results for 3D objects with model outlines 
and image keys used for matching. 

<br><span style="color:blue;">
å›³4ï¼šä¸Šæ®µã¯ã€èƒŒæ™¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦æ¤œå‡ºã•ã‚ŒãŸè¼ªéƒ­ç·šã‚’å«ã‚€3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ãƒ‡ãƒ«ç”»åƒã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
ä¸‹æ®µã¯ã€ãƒ¢ãƒ‡ãƒ«ã®è¼ªéƒ­ç·šã¨ãƒãƒƒãƒãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚ŒãŸç”»åƒã‚­ãƒ¼ã‚’å«ã‚€3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®èªè­˜çµæœã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
</span>
</p>

<center><img src="images/fig5.png"></center>

<p>

Figure 5: Examples of 3D object recognition with occlusion. 

<br><span style="color:blue;">
å›³ 5: é®è”½ã®ã‚ã‚‹ 3D ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜ã®ä¾‹ã€‚
</span>
</p><p>

The images in these examples are of size 384Ã—512 pixels. 
The computation times for recognition of all objects in 
each image are about 1.5 seconds on a Sun Sparc 10 processor, 
with about 0.9 seconds required to build the scale-
space pyramid and identify the SIFT keys, and about 0.6 
seconds to perform indexing and least-squares verification. 
This does not include time to pre-process each model image, 
which would be about 1 second per image, but would only 
need to be done once for initial entry into a model database. 

<br><span style="color:blue;">
ã“ã‚Œã‚‰ã®ä¾‹ã®ç”»åƒã®ã‚µã‚¤ã‚ºã¯384Ã—512ãƒ”ã‚¯ã‚»ãƒ«ã§ã™ã€‚
å„ç”»åƒå†…ã®ã™ã¹ã¦ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®èªè­˜ã«ã‹ã‹ã‚‹è¨ˆç®—æ™‚é–“ã¯ã€Sun Sparc 10ãƒ—ãƒ­ã‚»ãƒƒã‚µä¸Šã§ç´„1.5ç§’ã§ã™ã€‚
ã‚¹ã‚±ãƒ¼ãƒ«ã‚¹ãƒšãƒ¼ã‚¹ãƒ”ãƒ©ãƒŸãƒƒãƒ‰ã®æ§‹ç¯‰ã¨SIFTã‚­ãƒ¼ã®è­˜åˆ¥ã«ã¯ç´„0.9ç§’ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã¨æœ€å°äºŒä¹—æ³•ã«ã‚ˆã‚‹æ¤œè¨¼ã«ã¯ç´„0.6ç§’ã‹ã‹ã‚Šã¾ã™ã€‚
ã“ã‚Œã«ã¯ã€å„ãƒ¢ãƒ‡ãƒ«ç”»åƒã®å‰å‡¦ç†ã«ã‹ã‹ã‚‹æ™‚é–“ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
å‰å‡¦ç†ã¯ç”»åƒã”ã¨ã«ç´„1ç§’ã‹ã‹ã‚Šã¾ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æœ€åˆã®å…¥åŠ›æ™‚ã«1å›ã ã‘å®Ÿè¡Œã™ã‚Œã°æ¸ˆã¿ã¾ã™ã€‚
</span>
</p><p>

The illumination invariance of the SIFT keys is demonstrated 
in Figure 6. The two images are of the same scene 
from the same viewpoint, except that the first image is illuminated 
from the upper left and the second from the center 
right. The full recognition system is run to identify the 
second image using the first image as the model, and the 
second image is correctly recognized as matching the first. 
Only SIFT keys that were part of the recognition are shown. 
There were 273 keys that were verified as part of the final 
match, which means that in each case not only was the same 
key detected at the same location, but it also was the closest match to the correct corresponding key in the second image. 
Any 3 of these keys would be sufficient for recognition. 
While matching keys are not found in some regions where 
highlights or shadows change (for example on the shiny top 
of the camera) in general the keys show good invariance to 
illumination change. 

<br><span style="color:blue;">
SIFTã‚­ãƒ¼ã®ç…§æ˜ä¸å¤‰æ€§ã¯å›³6ã«ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚2æšã®ç”»åƒã¯åŒã˜ã‚·ãƒ¼ãƒ³ã‚’åŒã˜è¦–ç‚¹ã‹ã‚‰æ’®å½±ã—ãŸã‚‚ã®ã§ã™ãŒã€æœ€åˆã®ç”»åƒã¯å·¦ä¸Šã‹ã‚‰ã€2æšç›®ã¯ä¸­å¤®å³ã‹ã‚‰ç…§æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚æœ€åˆã®ç”»åƒã‚’ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦2æšç›®ã®ç”»åƒã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã«ã€å®Œå…¨ãªèªè­˜ã‚·ã‚¹ãƒ†ãƒ ãŒå®Ÿè¡Œã•ã‚Œã€2æšç›®ã®ç”»åƒã¯æœ€åˆã®ç”»åƒã¨ä¸€è‡´ã™ã‚‹ã‚‚ã®ã¨ã—ã¦æ­£ã—ãèªè­˜ã•ã‚Œã¾ã—ãŸã€‚
èªè­˜ã«ä½¿ç”¨ã•ã‚ŒãŸSIFTã‚­ãƒ¼ã®ã¿ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚
æœ€çµ‚çš„ãªä¸€è‡´ã®ä¸€éƒ¨ã¨ã—ã¦æ¤œè¨¼ã•ã‚ŒãŸã‚­ãƒ¼ã¯273å€‹ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯ã€ã„ãšã‚Œã®å ´åˆã‚‚ã€åŒã˜å ´æ‰€ã§åŒã˜ã‚­ãƒ¼ãŒæ¤œå‡ºã•ã‚ŒãŸã ã‘ã§ãªãã€2æšç›®ã®ç”»åƒå†…ã®æ­£ã—ã„å¯¾å¿œã™ã‚‹ã‚­ãƒ¼ã«æœ€ã‚‚è¿‘ã„ä¸€è‡´ã§ã‚ã£ãŸã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
ã“ã‚Œã‚‰ã®ã‚­ãƒ¼ã®ã†ã¡3ã¤ãŒã‚ã‚Œã°èªè­˜ã«ã¯ååˆ†ã§ã™ã€‚
ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚„ã‚·ãƒ£ãƒ‰ã‚¦ãŒå¤‰åŒ–ã™ã‚‹é ˜åŸŸï¼ˆä¾‹ãˆã°ã€ã‚«ãƒ¡ãƒ©ã®å…‰æ²¢ã®ã‚ã‚‹ä¸Šéƒ¨ï¼‰ã§ã¯ä¸€è‡´ã™ã‚‹ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ãŒã€ä¸€èˆ¬çš„ã«ã‚­ãƒ¼ã¯ç…§æ˜ã®å¤‰åŒ–ã«å¯¾ã—ã¦è‰¯å¥½ãªä¸å¤‰æ€§ã‚’ç¤ºã—ã¾ã™ã€‚
</span>
</p>

<center><img src="images/fig6.png"></center>

<p>

Figure 6: Stability of image keys is tested under differing 
illumination. The first image is illuminated from upper left 
and the second from center right. Keys shown in the bottom 
image were those used to match second image to first.

<br><span style="color:blue;">
å›³6ï¼šç•°ãªã‚‹ç…§æ˜ä¸‹ã§ã®ç”»åƒã‚­ãƒ¼ã®å®‰å®šæ€§ã‚’ãƒ†ã‚¹ãƒˆã€‚
æœ€åˆã®ç”»åƒã¯å·¦ä¸Šã‹ã‚‰ã€
2ç•ªç›®ã®ç”»åƒã¯å³ä¸­å¤®ã‹ã‚‰ç…§æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚
ä¸‹ã®ç”»åƒã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ã¯ã€2ç•ªç›®ã®ç”»åƒã‚’1ç•ªç›®ã®ç”»åƒã¨ä¸€è‡´ã•ã›ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚
</span>
</p>

<h2>8. Connections to biological vision <span style="color:blue;">ç”Ÿç‰©å­¦çš„è¦–è¦šã¨ã®ã¤ãªãŒã‚Š</span></h2>
<p>
The performance of human vision is obviously far superior 
to that of current computer vision systems, so there is potentially 
much to be gained by emulating biological processes. 
Fortunately, there have been dramatic improvements within 
the past few years in understanding how object recognition 
is accomplished in animals and humans. 

<br><span style="color:blue;">
äººé–“ã®è¦–è¦šæ€§èƒ½ã¯æ˜ã‚‰ã‹ã«ç¾åœ¨ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ãƒ“ã‚¸ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’ã¯ã‚‹ã‹ã«ä¸Šå›ã£ã¦ã„ã‚‹ãŸã‚ã€ç”Ÿç‰©å­¦çš„ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¨¡å€£ã™ã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ã¯å¤§ããã‚ã‚Šã¾ã™ã€‚å¹¸ã„ãªã“ã¨ã«ã€ã“ã“æ•°å¹´ã§ã€å‹•ç‰©ã‚„äººé–“ã«ãŠã‘ã‚‹ç‰©ä½“èªè­˜ã®ä»•çµ„ã¿ã«é–¢ã™ã‚‹ç†è§£ã¯åŠ‡çš„ã«é€²æ­©ã—ã¾ã—ãŸã€‚
</span>
</p><p>

Recent research in neuroscience has shown that object 
recognition in primates makes use of features of intermediate 
complexity that are largely invariant to changes in scale, 
location, and illumination (Tanaka [21], Perrett & Oram 
[16]). Some examples of such intermediate features found 
in inferior temporal cortex (IT) are neurons that respond to 
a dark five-sided star shape, a circle with a thin protruding 
element, or a horizontal textured region within a triangular 
boundary. These neurons maintain highly specific responses 
to shape features that appear anywhere within a large portion 
of the visual field and over a several octave range of 
scales (Ito et. al [7]). The complexity of many of these features 
appears to be roughly the same as for the current SIFT 
features, although there are also some neurons that respond 
to more complex shapes, such as faces. Many of the neurons 
respond to color and texture properties in addition to 
shape. The feature responses have been shown to depend 
on previous visual learning from exposure to specific objects 
containing the features (Logothetis, Pauls & Poggio [10]). 
These features appear to be derived in the brain by a highly 
computation-intensive parallel process, which is quite different 
from the staged filtering approach given in this paper. 
However, the results are much the same: an image is transformed 
into a large set of local features that each match a 
small fraction of potential objects yet are largely invariant 
to common viewing transformations. 

<br><span style="color:blue;">
ç¥çµŒç§‘å­¦ã«ãŠã‘ã‚‹æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€éœŠé•·é¡ã®ç‰©ä½“èªè­˜ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ä½ç½®ã€ç…§æ˜ã®å¤‰åŒ–ã«å¯¾ã—ã¦ã»ã¼ä¸å¤‰ãªä¸­ç¨‹åº¦ã®è¤‡é›‘ã•ã®ç‰¹å¾´ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼ˆTanaka [21]ã€Perrett & Oram [16]ï¼‰ã€‚ä¸‹å´é ­è‘‰çš®è³ªï¼ˆITï¼‰ã«è¦‹ã‚‰ã‚Œã‚‹ã“ã®ã‚ˆã†ãªä¸­ç¨‹åº¦ã®è¤‡é›‘ã•ã®ç‰¹å¾´ã®ä¾‹ã¨ã—ã¦ã€æš—ã„äº”è§’å½¢ã®æ˜Ÿå½¢ã€ç´°ã„çªèµ·ã®ã‚ã‚‹å††ã€ä¸‰è§’å½¢ã®å¢ƒç•Œå†…ã«ã‚ã‚‹æ°´å¹³ã®ãƒ†ã‚¯ã‚¹ãƒãƒ£é ˜åŸŸã«åå¿œã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€è¦–é‡ã®å¤§éƒ¨åˆ†ã®ã©ã“ã«ã§ã‚‚ç¾ã‚Œã‚‹å½¢çŠ¶ç‰¹å¾´ã«å¯¾ã—ã¦ã€æ•°ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ã®ç¯„å›²ã«ã‚ãŸã£ã¦éå¸¸ã«ç‰¹ç•°çš„ãªåå¿œã‚’ç¶­æŒã—ã¾ã™ï¼ˆIto et al [7]ï¼‰ã€‚ã“ã‚Œã‚‰ã®ç‰¹å¾´ã®å¤šãã®è¤‡é›‘ã•ã¯ã€ç¾åœ¨ã®SIFTç‰¹å¾´ã¨ã»ã¼åŒã˜ã§ã‚ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ãŒã€é¡”ãªã©ã®ã‚ˆã‚Šè¤‡é›‘ãªå½¢çŠ¶ã«åå¿œã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚‚ã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚å¤šãã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€å½¢çŠ¶ã«åŠ ãˆã¦ã€è‰²ã‚„è³ªæ„Ÿã®ç‰¹æ€§ã«ã‚‚åå¿œã—ã¾ã™ã€‚ç‰¹å¾´ã«å¯¾ã™ã‚‹åå¿œã¯ã€ãã®ç‰¹å¾´ã‚’å«ã‚€ç‰¹å®šã®ç‰©ä½“ã¸ã®æ›éœ²ã«ã‚ˆã‚‹éå»ã®è¦–è¦šå­¦ç¿’ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ï¼ˆLogothetis, Pauls & Poggio [10]ï¼‰ã€‚
ã“ã‚Œã‚‰ã®ç‰¹å¾´ã¯ã€è„³å†…ã§éå¸¸ã«è¨ˆç®—é›†ç´„çš„ãªä¸¦åˆ—ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹ã‚ˆã†ã§ã€ã“ã‚Œã¯æœ¬è«–æ–‡ã§ç¤ºã—ãŸæ®µéšçš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã¨ã¯å…¨ãç•°ãªã‚Šã¾ã™ã€‚
ã—ã‹ã—ã€çµæœã¯ã»ã¼åŒã˜ã§ã™ã€‚ã¤ã¾ã‚Šã€ç”»åƒã¯ã€ãã‚Œãã‚ŒãŒæ½œåœ¨çš„ãªç‰©ä½“ã®ã”ãä¸€éƒ¨ã«ä¸€è‡´ã™ã‚‹ã€å¤§è¦æ¨¡ãªå±€æ‰€ç‰¹å¾´ã‚»ãƒƒãƒˆã«å¤‰æ›ã•ã‚Œã¾ã™ãŒã€ä¸€èˆ¬çš„ãªè¦–è¦šå¤‰æ›ã«å¯¾ã—ã¦ã¯ã»ã¼ä¸å¤‰ã§ã™ã€‚
</span>
</p><p>

It is also known that object recognition in the brain depends 
on a serial process of attention to bind features to object 
interpretations, determine pose, and segment an object 
from a cluttered background [22]. This process is presumably 
playing the same role in verification as the parameter 
solving and outlier detection used in this paper, since the 
accuracy of interpretations can often depend on enforcing a 
single viewpoint constraint [11]. 

<br><span style="color:blue;">
è„³ã«ãŠã‘ã‚‹ç‰©ä½“èªè­˜ã¯ã€ç‰¹å¾´ã¨ç‰©ä½“ã®è§£é‡ˆã‚’çµã³ä»˜ã‘ã€å§¿å‹¢ã‚’æ±ºå®šã—ã€é›‘ç„¶ã¨ã—ãŸèƒŒæ™¯ã‹ã‚‰ç‰©ä½“ã‚’åˆ†å‰²ã™ã‚‹ã¨ã„ã†ä¸€é€£ã®æ³¨æ„ãƒ—ãƒ­ã‚»ã‚¹ã«ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã‚‚çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™[22]ã€‚è§£é‡ˆã®ç²¾åº¦ã¯ã€å¤šãã®å ´åˆã€å˜ä¸€ã®è¦–ç‚¹åˆ¶ç´„ã®é©ç”¨ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€æœ¬è«–æ–‡ã§ä½¿ç”¨ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£æ±ºã‚„å¤–ã‚Œå€¤æ¤œå‡ºã¨åŒæ§˜ã«ã€æ¤œè¨¼ã«ãŠã„ã¦å½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™[11]ã€‚
</span>
</p>

<h2>9. Conclusions and comments <span style="color:blue;">çµè«–ã¨ã‚³ãƒ¡ãƒ³ãƒˆ</span></h2>
<p>
The SIFT features improveon previous approaches by being 
largely invariant to changes in scale, illumination, and local affine distortions. The large number of features in a typical 
image allow for robust recognition under partial occlusion in 
cluttered images. A final stage that solves for affine model 
parameters allows for more accurate verification and pose 
determination than in approaches that rely only on indexing. 

<br><span style="color:blue;">
SIFTç‰¹å¾´é‡ã¯ã€ã‚¹ã‚±ãƒ¼ãƒ«ã€ç…§æ˜ã€å±€æ‰€çš„ãªã‚¢ãƒ•ã‚£ãƒ³æ­ªã¿ã®å¤‰åŒ–ã«å¯¾ã—ã¦ã»ã¼ä¸å¤‰ã§ã‚ã‚‹ãŸã‚ã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã¾ã™ã€‚å…¸å‹çš„ãªç”»åƒã«ã¯å¤šæ•°ã®ç‰¹å¾´é‡ãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€ä¹±é›‘ãªç”»åƒã«ãŠã‘ã‚‹éƒ¨åˆ†çš„ãªé®è”½ä¸‹ã§ã‚‚å …ç‰¢ãªèªè­˜ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã‚¢ãƒ•ã‚£ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ±‚ã‚ã‚‹æœ€çµ‚æ®µéšã§ã¯ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã¿ã«ä¾å­˜ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚æ­£ç¢ºãªæ¤œè¨¼ã¨å§¿å‹¢åˆ¤å®šãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
</span>
</p><p>

An important area for further research is to build models 
from multiple views that represent the 3D structure of objects. 
This would have the further advantage that keys from 
multiple viewing conditions could be combined into a single 
model, thereby increasing the probability of finding matches 
in new views. The models could be true 3D representations 
based on structure-from-motion solutions, or could represent 
the space of appearance in terms of automated clustering 
and interpolation (Pope & Lowe [17]). An advantage of 
the latter approach is that it could also model non-rigid deformations. 

<br><span style="color:blue;">
ä»Šå¾Œã®é‡è¦ãªç ”ç©¶åˆ†é‡ã¯ã€ç‰©ä½“ã®3Dæ§‹é€ ã‚’è¡¨ç¾ã™ã‚‹è¤‡æ•°ã®ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã™ã€‚
ã“ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ã€è¤‡æ•°ã®è¦³å¯Ÿæ¡ä»¶ã‹ã‚‰ã®ã‚­ãƒ¼ã‚’å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã§ãã‚‹ã¨ã„ã†ã•ã‚‰ãªã‚‹åˆ©ç‚¹ãŒã‚ã‚Šã€æ–°ã—ã„ãƒ“ãƒ¥ãƒ¼ã§ä¸€è‡´ã™ã‚‹ã‚‚ã®ãŒè¦‹ã¤ã‹ã‚‹ç¢ºç‡ãŒé«˜ã¾ã‚Šã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã€å‹•ãã‹ã‚‰æ§‹é€ ã‚’æ¨å®šã™ã‚‹ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã«åŸºã¥ãçœŸã®3Dè¡¨ç¾ã¨ãªã‚‹å ´åˆã‚‚ã‚ã‚Œã°ã€è‡ªå‹•ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨è£œé–“ã«ã‚ˆã£ã¦å¤–è¦³ç©ºé–“ã‚’è¡¨ç¾ã™ã‚‹å ´åˆã‚‚ã‚ã‚Šã¾ã™ï¼ˆPope & Lowe [17]ï¼‰ã€‚å¾Œè€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®åˆ©ç‚¹ã¯ã€éå‰›ä½“å¤‰å½¢ã‚‚ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã‚‹ã“ã¨ã§ã™ã€‚
</span>
</p><p>

The recognition performance could be further improved 
by adding new SIFT feature types to incorporate color, texture, 
and edge groupings, as well as varying feature sizes 
and offsets. Scale-invariant edge groupings that make local 
figure-ground discriminations would be particularly useful 
at object boundaries where background clutter can interfere 
with other features. The indexing and verification framework 
allows for all types of scale and rotation invariant features 
to be incorporated into a single model representation. 
Maximum robustness would be achieved by detecting many 
different feature types and relying on the indexing and clustering 
to select those that are most useful in a particular image. 

<br><span style="color:blue;">
èªè­˜æ€§èƒ½ã¯ã€è‰²ã€ãƒ†ã‚¯ã‚¹ãƒãƒ£ã€ã‚¨ãƒƒã‚¸ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã€ãã—ã¦ã•ã¾ã–ã¾ãªç‰¹å¾´ã®ã‚µã‚¤ã‚ºã¨ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’çµ„ã¿è¾¼ã‚“ã æ–°ã—ã„SIFTç‰¹å¾´ã‚¿ã‚¤ãƒ—ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã•ã‚‰ã«å‘ä¸Šã—ã¾ã™ã€‚ã‚¹ã‚±ãƒ¼ãƒ«ä¸å¤‰ã®ã‚¨ãƒƒã‚¸ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¯ã€å±€æ‰€çš„ãªå›³åœ°è­˜åˆ¥ã‚’å¯èƒ½ã«ã—ã€èƒŒæ™¯ã®ä¹±é›‘ã•ãŒä»–ã®ç‰¹å¾´ã«å¹²æ¸‰ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ç‰©ä½“å¢ƒç•Œã«ãŠã„ã¦ç‰¹ã«æœ‰ç”¨ã§ã™ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã¨æ¤œè¨¼ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚Šã€ã‚ã‚‰ã‚†ã‚‹ç¨®é¡ã®ã‚¹ã‚±ãƒ¼ãƒ«ã¨å›è»¢ä¸å¤‰ã®ç‰¹å¾´ã‚’å˜ä¸€ã®ãƒ¢ãƒ‡ãƒ«è¡¨ç¾ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚æœ€å¤§é™ã®å …ç‰¢æ€§ã¯ã€å¤šãã®ç•°ãªã‚‹ç‰¹å¾´ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡ºã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’åˆ©ç”¨ã—ã¦ç‰¹å®šã®ç”»åƒã§æœ€ã‚‚æœ‰ç”¨ãªç‰¹å¾´ã‚’é¸æŠã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šå®Ÿç¾ã•ã‚Œã¾ã™ã€‚
</span>
</p><p>

<h2>References <span style="color:blue;">å‚è€ƒæ–‡çŒ®</span></h2>
<p>
<div class="styleRef">
<ul><li>
[1]  Ballard, D.H., â€œGeneralizing the Hough transform to detect arbitrary patterns,â€ Pattern Recognition, 13, 2 (1981), pp. 111-122.

<br><span style="color:blue;">
ã€Houghå¤‰æ›ã®ä¸€èˆ¬åŒ–ã«ã‚ˆã‚‹ä»»æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºã€
</span>
</li><br><li>
[2]  Basri, Ronen, and David. W. Jacobs,â€œRecognition using re-gion correspondences,â€ International Journal of Computer Vision, 25, 2 (1996), pp. 141â€“162.

<br><span style="color:blue;">
ã€é ˜åŸŸå¯¾å¿œã‚’ç”¨ã„ãŸèªè­˜ã€
</span>
</li><br><li>
[3]  Beis,  Jeff,  and David G. Lowe,  â€œShape indexing using approximate nearest-neighbour search in high-dimensional spaces,â€ConferenceonComputerVisionandPatternRecog-nition, Puerto Rico (1997), pp. 1000â€“1006.

<br><span style="color:blue;">
ã€é«˜æ¬¡å…ƒç©ºé–“ã«ãŠã‘ã‚‹è¿‘ä¼¼æœ€è¿‘å‚æ¢ç´¢ã‚’ç”¨ã„ãŸå½¢çŠ¶ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã€
</span>
</li><br><li>
[4]  Crowley, James L., and Alice C. Parker, â€œA representation forshapebasedon peaksandridgesin the difference oflow-pass transform,â€ IEEE Trans. on Pattern Analysis and Ma-chine Intelligence, 6, 2 (1984), pp. 156â€“170.

<br><span style="color:blue;">
ã€ãƒ­ãƒ¼ãƒ‘ã‚¹å¤‰æ›ã®å·®ã«ãŠã‘ã‚‹ãƒ”ãƒ¼ã‚¯ã¨ãƒªãƒƒã‚¸ã«åŸºã¥ãå½¢çŠ¶ã®è¡¨ç¾ã€
</span>
</li><br><li>
[5]  Edelman, Shimon, Nathan Intrator,  and Tomaso Poggio, â€œComplex  cells  and  object  recognition,â€   Unpublished Manuscript,  preprint  at  http://www.ai.mit.edu/
edelman/mirror/nips97.ps.Z

<br><span style="color:blue;">
ã€è¤‡é›‘ç´°èƒã¨ç‰©ä½“èªè­˜ã€(æœªç™ºè¡¨åŸç¨¿)<br>
 (Google Scholarã§æ¤œç´¢ã™ã‚‹ã¨ pdf ã‚’å…¥æ‰‹ã§ãã‚‹ã€‚ã‚°ãƒ©ãƒ•, å›³ãŒæ–°ã—ãã†ã ã‘ã©ï½¥ï½¥ï½¥)
</span>
</li><br><li>
[6]  Grimson,  Eric,  and  ThomaÂ´s  Lozano-PeÂ´rez,  â€œLocalizing overlappingpartsbysearchingthe interpretationtree,â€ IEEE Trans. on  Pattern  Analysis and  Machine Intelligence,  9 (1987), pp. 469â€“482.

<br><span style="color:blue;">
ã€è§£é‡ˆæœ¨æ¢ç´¢ã«ã‚ˆã‚‹é‡ãªã£ãŸéƒ¨å“ã®å®šä½ã€
</span>
</li><br><li>
[7]  Ito,  Minami,  Hiroshi  Tamura,  Ichiro  Fujita,  and  Keiji Tanaka,â€œSize andposition invarianceof neuronalresponses in monkeyinferotemporalcortex,â€ JournalofNeurophysiol-ogy, 73, 1 (1995), pp. 218â€“226.

<br><span style="color:blue;">
ã€ã‚µãƒ«ä¸‹å´é ­è‘‰çš®è³ªã«ãŠã‘ã‚‹ç¥çµŒå¿œç­”ã®å¤§ãã•ã¨ä½ç½®ã®ä¸å¤‰æ€§ã€
</span>
</li><br><li>
[8]  Lindeberg,  Tony, â€œScale-space theory:   A basic tool for analysing structures at different scalesâ€, Journal of Applied Statistics, 21, 2 (1994), pp. 224â€“270.

<br><span style="color:blue;">
ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“ç†è«–ï¼šç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®æ§‹é€ ã‚’åˆ†æã™ã‚‹ãŸã‚ã®åŸºæœ¬ãƒ„ãƒ¼ãƒ«ã€
</span>
</li><br><li>
[9]  Lindeberg, Tony, â€œDetecting salient blob-like image struc-tures  and their scales with  a scale-space primal sketch: a method for focus-of-attention,â€ International Journal of Computer Vision, 11, 3 (1993), pp. 283â€“318.

<br><span style="color:blue;">
ã€ã‚¹ã‚±ãƒ¼ãƒ«ç©ºé–“ãƒ—ãƒ©ã‚¤ãƒãƒ«ã‚¹ã‚±ãƒƒãƒã‚’ç”¨ã„ãŸé¡•è‘—ãªå¡ŠçŠ¶ç”»åƒæ§‹é€ ã¨ãã®ã‚¹ã‚±ãƒ¼ãƒ«ã®æ¤œå‡ºï¼šæ³¨ç›®ã®ç„¦ç‚¹ã®ãŸã‚ã®æ‰‹æ³•ã€
</span>
</li><br><li>
[10]  Logothetis,NikosK.,JonPauls,andTomasoPoggio,â€œShape representation in the inferior temporal cortex of monkeys,â€ Current Biology, 5, 5 (1995), pp. 552â€“563.

<br><span style="color:blue;">
ã€ã‚µãƒ«ã®ä¸‹å´é ­è‘‰çš®è³ªã«ãŠã‘ã‚‹å½¢çŠ¶è¡¨ç¾ã€
</span>
</li><br><li>
[11]  Lowe,  David  G.,  â€œThree-dimensional object recognition from  single  two-dimensional  images,â€  Arti ficial  Intelli-gence, 31, 3 (1987), pp. 355â€“395.

<br><span style="color:blue;">
ã€å˜ä¸€2æ¬¡å…ƒç”»åƒã‹ã‚‰ã®3æ¬¡å…ƒç‰©ä½“èªè­˜ã€
</span>
</li><br><li>
[12]  Lowe, David G., â€œ fitting parameterized three-dimensional modelstoimages,â€IEEETrans.onPatternAnalysisandMa-chine Intelligence, 13, 5 (1991), pp. 441â€“450.

<br><span style="color:blue;">
ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸ3æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”»åƒã¸ã®å¤‰æ›ã€
</span>
</li><br><li>
[13]  Murase, Hiroshi, and Shree K. Nayar, â€œVisual learning and recognition of 3-D objects from appearance,â€ International Journal of ComputerVision, 14, 1 (1995), pp. 5â€“24.

<br><span style="color:blue;">
ã€å¤–è¦³ã«åŸºã¥ã3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¦–è¦šå­¦ç¿’ã¨èªè­˜ã€
</span>
</li><br><li>
[14]  Nelson, Randal C., and Andrea Selinger, â€œLarge-scale tests of a keyed, appearance-based 3-D object recognition sys-tem,â€ Vision Research,38, 15 (1998), pp. 2469â€“88.

<br><span style="color:blue;">
ã€ã‚­ãƒ¼ä»˜ãå¤–è¦³ãƒ™ãƒ¼ã‚¹3Dã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜ã‚·ã‚¹ãƒ†ãƒ ã®å¤§è¦æ¨¡ãƒ†ã‚¹ãƒˆã€
</span>
</li><br><li>
[15]  Ohba,   Kohtaro,   and  Katsushi  Ikeuchi,   â€œDetectability, uniqueness,  and  reliability  of  eigen  windows  for  stable veri fication of partially occluded objects,â€ IEEE Trans. on Pattern Analysis and Machine Intelligence, 19, 9 (1997), pp. 1043â€“48.

<br><span style="color:blue;">
ã€éƒ¨åˆ†çš„ã«é®è”½ã•ã‚ŒãŸç‰©ä½“ã®å®‰å®šæ¤œè¨¼ã«ãŠã‘ã‚‹å›ºæœ‰ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æ¤œå‡ºå¯èƒ½æ€§ã€ä¸€æ„æ€§ã€ä¿¡é ¼æ€§ã€
</span>
</li><br><li>
[16]  Perrett, David I., and Mike W. Oram, â€œVisual recognition based on temporal cortex cells: viewer-centered processing of pattern con figuration,â€ Zeitschrift fuÂ¨r Naturforschung C, 53c (1998), pp. 518â€“541.

<br><span style="color:blue;">
ã€å´é ­è‘‰çš®è³ªç´°èƒã«åŸºã¥ãè¦–è¦šèªè­˜ï¼šè¦³å¯Ÿè€…ä¸­å¿ƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ§‹æˆå‡¦ç†ã€
</span>
</li><br><li>
[17]  Pope, Arthur R. and David G. Lowe, â€œLearning probabilis-tic appearance models for object recognition,â€ in Early Vi-sualLearning,eds.ShreeNayarandTomasoPoggio(Oxford University Press, 1996), pp. 67â€“97.

<br><span style="color:blue;">
ã€ç‰©ä½“èªè­˜ã®ãŸã‚ã®ç¢ºç‡çš„å¤–è¦³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€
</span>
</li><br><li>
[18]  Schiele, Bernt, and James L. Crowley, â€œObject recognition using multidimensional receptive  field histograms,â€ Fourth EuropeanConference on ComputerVision, Cambridge, UK (1996), pp. 610â€“619.

<br><span style="color:blue;">
ã€å¤šæ¬¡å…ƒå—å®¹é‡ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ç”¨ã„ãŸç‰©ä½“èªè­˜ã€
</span>
</li><br><li>
[19]  Schmid, C., and R. Mohr, â€œLocal grayvalue invariants for image retrieval,â€ IEEE PAMI, 19, 5 (1997), pp. 530â€“534.

<br><span style="color:blue;">
ã€Localç”»åƒæ¤œç´¢ã«ãŠã‘ã‚‹ã‚°ãƒ¬ãƒ¼å€¤ä¸å¤‰é‡ã€
</span>
</li><br><li>
[20]  Swain, M., and D. Ballard, â€œColor indexing,â€ International Journal of ComputerVision, 7, 1 (1991), pp. 11â€“32.

<br><span style="color:blue;">
ã€ã‚«ãƒ©ãƒ¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€
</span>
</li><br><li>
[21]  Tanaka, Keiji, â€œMechanisms of visual object recognition: monkeyandhumanstudies,â€Current Opinion in Neurobiol-ogy, 7 (1997), pp. 523â€“529.

<br><span style="color:blue;">
ã€è¦–è¦šçš„ç‰©ä½“èªè­˜ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼šã‚µãƒ«ã¨äººé–“ã®ç ”ç©¶ã€
</span>
</li><br><li>
[22]  Treisman, Anne M., and Nancy G. Kanwisher, â€œPerceiv-ing visually presented objects: recognition, awareness, and modularity,â€CurrentOpinionin Neurobiology,8(1998), pp. 218â€“226.

<br><span style="color:blue;">
ã€è¦–è¦šçš„ã«æç¤ºã•ã‚ŒãŸç‰©ä½“ã®çŸ¥è¦šï¼šèªè­˜ã€æ„è­˜ã€ãã—ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ€§ã€
</span>
</li><br><li>
[23]  Zhang, Z., R. Deriche, O. Faugeras, Q.T. Luong, â€œA robust techniquefor matching two uncalibratedimagesthrough the recovery of the unknown epipolar geometry,â€ Arti ficial In-telligence, 78, (1995), pp. 87-119.

<br><span style="color:blue;">
ã€æœªçŸ¥ã®ã‚¨ãƒ”ãƒãƒ¼ãƒ©å¹¾ä½•å­¦ã®å›å¾©ã‚’é€šã˜ã¦2ã¤ã®æœªè¼ƒæ­£ç”»åƒã‚’ãƒãƒƒãƒãƒ³ã‚°ã™ã‚‹å …ç‰¢ãªæ‰‹æ³•ã€
</span>
</li></ul></div>
</p>
    </body>
</html>