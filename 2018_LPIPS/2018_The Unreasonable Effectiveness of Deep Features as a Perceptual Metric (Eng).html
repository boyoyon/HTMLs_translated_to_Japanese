<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>LPIPS</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -30px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 50px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1><center>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</center></h1>
<div class="two-columns">
<div class="column">
<center>Richard Zhang Phillip Isola Alexei A. Efros</center>
<center>University of California, Berkeley</center>
<center>frich.zhang, isola, efrosg@eecs.berkeley.edu</center>
 </div>
<div class="column">
<center>Eli Shechtman Oliver Wang</center>
<center>Adobe Research</center>
<center>felishe,owangg@adobe.com</center>
</div>
</div>
<center><img src="images/fig1.png"></center>
<p>
Figure 1: Which patch (left or right) is “closer” to the middle patch in these examples? In each case, the traditional
metrics (L2/PSNR, SSIM, FSIM) disagree with human judgments. But deep networks, even across architectures
(Squeezenet [18], AlexNet [25], VGG [48]) and supervision type (supervised [44], self-supervised [12, 37, 40, 59], and
even unsupervised [24]), provide an emergent embedding which agrees surprisingly well with humans. We further calibrate
existing deep embeddings on a large-scale database of perceptual judgments; models and data can be found at
https://www.github.com/richzhang/PerceptualSimilarity.
</p>
<h2><center>Abstract</center></h2>
<p>
While it is nearly effortless for humans to quickly assess
the perceptual similarity between two images, the underlying
processes are thought to be quite complex. Despite
this, the most widely used perceptual metrics today, such as
PSNR and SSIM, are simple, shallow functions, and fail to
account for many nuances of human perception. Recently,
the deep learning community has found that features of the
VGG network trained on the ImageNet classification task
has been remarkably useful as a training loss for image
synthesis. But how perceptual are these so-called “perceptual
losses”? What elements are critical for their success?
To answer these questions, we introduce a new Full Reference
Image Quality Assessment (FR-IQA) dataset of perceptual
human judgments, orders of magnitude larger than
previous datasets. We systematically evaluate deep features
across different architectures and tasks and compare them
with classic metrics. We find that deep features outperform
all previous metrics by huge margins. More surprisingly,
this result is not restricted to ImageNet-trained VGG features,
but holds across different deep architectures and levels
of supervision (supervised, self-supervised, or even unsupervised).
Our results suggest that perceptual similarity
is an emergent property shared across deep visual representations.
</p>
<h2>1. Motivation</h2>
<p>
The ability to compare data items is perhaps the most
fundamental operation underlying all of computing. In
many areas of computer science it does not pose much difficulty:
one can use Hamming distance to compare binary
patterns, edit distance to compare text files, Euclidean distance
to compare vectors, etc. The unique challenge of computer
vision is that even this seemingly simple task of comparing
visual patterns remains a wide-open problem. Not
only are visual patterns very high-dimensional and highly
correlated, but, the very notion of visual similarity is often
subjective, aiming to mimic human visual perception. For
instance, in image compression, the goal is for the compressed
image to be indistinguishable from the original by
a human observer, irrespective of the fact that their pixel
representations might be very different.
</p>
<p>
Classic per-pixel measures, such as the \(l^2\) Euclidean distance
metric, commonly used for regression problems, or
the related Peak Signal-to-Noise Ratio (PSNR), are insufficient
for assessing structured outputs such as images, since
they assume each output pixel is conditionally independent
of all others, given the input. A well-known example is
that blurring an image causes large perceptual but small Euclidean
change. 
</p>
<p>
What we would really like is a “perceptual distance”,
which measures how similar are two images in a way
that coincides with human judgment. This problem, often called full-reference image quality assessment (FR-IQA),
has been a longstanding goal in the community, and there
have been numerous perceptually motivated distance metrics
proposed, such as SSIM [53], MSSIM [55], FSIM [57],
and HDR-VDP [32].
</p>
<p>
However, constructing a perceptual metric is challenging,
because human judgments of similarity (1) depend on
high-order image structure [53], (2) are context-dependent
[17, 34, 33], and (3) may not actually constitute a distance
metric [51]. The crux of (2) is that there are many different
“senses of similarity” that we can simultaneously hold
in mind: is a red circle more similar to a red square or to a
blue circle? Directly fitting a function to human judgments
may be intractable due the the context-dependent and pairwise
nature of the judgments (which compare the similarity
between two images). Indeed, we show in this paper a
negative result where this approach fails to generalize, even
when trained on datasets several orders of magnitude larger
than any previously available.
</p>
<p>
Instead, might there be a way to learn a notion of perceptual
similarity without directly training for it? The computer
vision community has discovered that internal activations
of deep convolutional networks, though trained on a
high-level image classification task, are often surprisingly
useful as a representational space for a much wider variety
of tasks. For example, features from the VGG architecture
[48] have been used on tasks such as neural style
transfer [16], image superresolution [21], and conditional
image synthesis [13, 8]. These methods measure distance
in VGG feature space as a “perceptual” loss for image regression
problems [21, 13].
</p>
<p>
But how well do these “perceptual losses” actually correspond
to human visual perception? How do they compare
to traditional perceptual image evaluation metrics? Does the
network architecture matter? Does it have to be trained on
the ImageNet classification task, or would other tasks work
just as well? Do the networks need to be trained at all?
</p>
<p>
In this paper, we evaluate these questions on a new
large-scale database of human judgments, and arrive at several
surprising conclusions. We find that internal activations of networks trained for high-level classification tasks,
even with no further calibration and across network architectures
[18, 26, 48], do indeed correspond to human perceptual
judgments, far better, indeed, than the commonly
used metrics like SSIM and FSIM [53, 57]. Furthermore,
the best performing self-supervised networks, including Bi-
GANs [12], cross-channel prediction [59], and puzzle solving
[37] perform just as well at this task, even without the
benefit of human-labeled training data. Even a simple unsupervised
network initialization with stacked k-means [24]
beats the classic metrics by a large margin! This illustrates
an emergent property shared across networks, even across
architectures and training signals. Importantly, however,
having some training signal appears crucial – a randomly
initialized network achieves much lower performance.
</p>
<p>
Our study is based on a newly collected FR-IQA dataset
containing 484k human judgments. Unlike prior datasets,
our judgments are collected on a large set of distortions and
real algorithm outputs, and contain both traditional distortions,
such as contrast and saturation adjustments, noise patterns,
filtering, and spatial warping operations, as well as
CNN-based algorithm outputs, such as autoencoding, denoising,
and colorization, produced by a variety of architectures
and losses. Our dataset is orders of magnitude richer
and more varied than previous datasets of this kind [42]. We
also collect judgments on outputs from real algorithms for
the tasks of superresolution, frame interpolation, and image
deblurring, which is especially important as these are
the real-world use cases for a perceptual metric. We show
that our data can be used to “calibrate” existing networks,
by learning a simple linear scaling of layer activations, to
better match low-level human judgments.
</p>
<p>
Our results are consistent with the hypothesis that perceptual
similarity is not a special function all of its own, but
rather a consequence of visual representations tuned to be
predictive about important structure in the world. Representations
that are effective at semantic prediction tasks are
also representations in which Euclidean distance is highly
predictive of perceptual similarity judgments.
</p>
<p>
Our contributions are as follows:
<div class="styleBullet">
<ul>
<li>We introduce a large-scale, highly varied, FR-IQA
dataset, containing 484k human judgments. Our
dataset not only includes parameterized distortions, but
also real algorithm outputs. We also collect judgments
on a different perceptual test, just noticeable differences
(JND).</li><br>
<li>We show that deep features, trained on supervised,
self-supervised, and unsupervised objectives alike,
model low-level perceptual similarity surprisingly
well, outperforming previous, widely-used metrics.</li><br>
<li>We demonstrate that network architecture alone does
not account for the performance: untrained nets
achieve much lower performance.</li><br>
<li>With our data, we can improve performance by “calibrating”
feature responses from a pre-trained network.</li>
</ul>
</div>
</p>
<center><img src="images/table1.png"></center>
<p>
Table 1: Dataset comparison. A primary differentiator between our proposed Berkeley-Adobe Perceptual Patch Similarity
(BAPPS) dataset and previous work is in scale, both in number of images and number of distortions. We provide human
perceptual judgments on distortion set using images from uncompressed images [7, 9]. Previous datasets have used a small
number of distortions at discrete levels, we use a large number of distortions (created by sequentially composing atomic
distortions together), and sample continuously. For each input patch, we corrupt it using two distortions, and ask for a few
human judgments (2 for train, 5 for test set) per pair. This enables us to obtain judgments on a large number of patches.
Previous databases summarize their judgments into a mean opinion score (MOS); we simply report our pairwise judgments
(two alternative force choice). In addition, we also provide judgments on outputs from real algorithms, as well as a same/not
same Just Noticeable Difference (JND) perceptual test.
</p>
<p>
Prior work on FR-IQA datasets In order to evaluate
existing similarity measures, a number of FR-IQA datasets
have been proposed. Some of the most popular are the
LIVE [47], TID2008 [43], CSIQ [27], and TID2013 [42]
datasets. These have served as the de-facto baselines for
development and evaluation of FR-IQA similarity metrics.
We collect a new dataset that is complementary to these:
it contains a substantially larger number of distortions, including
some from newer, deep neural network based outputs.
Additionally, it is collected on patches as opposed to
full images, in the wild, and with a different experimental
design (more details in Sec 2).
</p>
<p>
Prior work on deep networks for IQA Recently, advances
in DNNs have motivated investigation of applications
in the context of image quality assessment. Kim
and Lee [23] use a CNN to predict visual similarity by training on low-level differences. Concurrent work by
Talebi and Milanfar [50] train a deep network in the context
of no-reference IQA for image aesthetics. Gao et
al. [15] and Amirshahi et al. [3] propose techniques involving
leveraging internal activations of deep networks (VGG
and AlexNet, respectively) along with additional multiscale
post-processing. In this work, we conduct a more in-depth
study across different architectures, training signals, on a
new, large scale, highly-varied dataset.
</p>
<p>
Recently, Berardino et al. [6] train networks on FRIQA,
and importantly, assess the ability of deep networks to
make predictions on a separate task – predicting most and
least perceptually-noticeable directions of distortion. In our
work, we not only assess FR-IQA on paramterized distortions,
but also test generalization to real algorithms, as well
as generalization to a separate perceptual task – just noticeable
differences.
</p>
<h2>2. Berkeley-Adobe Perceptual Patch Similarity
(BAPPS) Dataset</h2>
<p>
To evaluate the performance of different perceptual metrics,
we collect a large-scale highly diverse dataset of perceptual
judgments using two approaches. Our main data
collection employs a two alternative forced choice (2AFC)
test, that asks which of two distortions is more similar to a
reference. This is validated by a second experiment where
we perform a just noticeable difference (JND) test, which
asks whether two consecutive images are the same or different.
These judgments are collected over a wide space of
distortions and real algorithm outputs.
</p>
<h3>2.1. Distortions</h3>
<p>
We first define our traditional and CNN-based distortions,
used for our 2AFC dataset.
</p>

<p>
Traditional distortions We create a set of “traditional”
distortions consisting of common operations performed on
the input patches, listed in Table 2 (left). In general, we
use photometric distortions, random noise, blurring, spatial
shifts and corruptions, and compression artifacts. We show
qualitative examples of our traditional distortions in Figure
2. The severity of each perturbation is parameterized -
for example, for Gaussian blur, the kernel width determines
the amount of corruption applied to the input image. We
also compose pairs of distortions sequentially to increase
the overall space of possible distortions. In total, we have
20 distortions and 308 sequentially composed distortions.
</p>
<center><img src="images/table2.png"></center>
<p>
Table 2: Our distortions. Our traditional distortions (left) are performed by basic low-level image editing operations. We
also sequentially compose them to better explore the space. Our CNN-based distortions (right) are formed by randomly
varying parameters such as task, network architecture, and learning parameters. The goal of the distortions is to mimic
plausible distortions seen in real algorithm outputs.
</p>
<center><img src="images/fig2.png"></center>
<p>
Figure 2: Example distortions. We show example distortions using our (a) traditional and (b) CNN-based methods.
</p>
<p>
CNN-based distortions To more closely simulate the
space of artifacts that can arise from deep-learning based
methods, we create a set of higher-level distortions created
by neural networks. We simulate possible algorithm outputs
by exploring a variety of tasks, architectures, and losses, as
shown in Table 2 (right). Such tasks include autoencoding,
denoising, colorization, and superresolution. All of these
tasks can be achieved by applying the appropriate corruption
to the input. In total, we generated 96 “autoencoders”
and use these as CNN-based distortion functions. We train
each of these networks on the 1.3M ImageNet dataset [44]
for 1 epoch. The goal of each network is not to solve
the task per se, but rather to explore common artifacts that
plague the outputs of deep learning based methods.
</p>
<p>
Distorted image patches from real algorithms The true
test of an image assessment algorithm is on real problems
and real algorithms. We gather perceptual judgments using
such outputs. Data on real algorithms is more limited,
as each application will have their own unique properties.
For example, different colorization methods will not show
much structural variation, but will be prone to effects such
as color bleeding and color variation. On the other hand,
superresolution will not have color ambiguity, but may see
larger structural changes from algorithm to algorithm.
</p>
<p>
Superresolution We evaluate results from the 2017 NTIRE workshop [2]. We use 3 tracks from the workshop
– ×2, ×3, ×4 upsampling rates using “unknown” downsampling
to create the input images. Each track had approximately
20 algorithm submissions. We also evaluate
several additional methods, including bicubic upsampling,
and four of the top performing deep superresolution methods
[22, 54, 29, 45]. A common qualitative way of presenting
superresolution results is zooming into specific patches
and comparing differences. As such, we sample random
64×64 triplets from random locations of images in the
Div2K [2] dataset – the ground truth high-resolution image,
along with two algorithm outputs.
</p>
<p>
Frame interpolation We sample patches from different
frame interpolation algorithms, including three variants
of flow-based interpolation [31], CNN-based interpolation
[36], and phase-based interpolation [35] on the Davis
Middleburry dataset [46]. Because artifacts arising from
frame interpolation may occur at different scales, we randomly
rescale the image before sampling a patch triplet.
</p>
<p>
Video deblurring We sample from the video deblurring
dataset [49], along with deblurring outputs from Photoshop
Shake Reduction, Weighted Fourier Aggregation [10], and
three variants of a deep video deblurring method [49].
</p>
<p>
Colorization We sample patches using random scales
on the colorization task, on images from the ImageNet
dataset [44]. The algorithms are from pix2pix [20], Larsson
et al. [28], and variants from Zhang et al. [58].
</p>
<h3>2.2. Psychophysical Similarity Measurements</h3>
<p>
2AFC similarity judgments We randomly select an image
patch x and apply two distortions to produce distorted
patches x0; x1. We then ask a human which patch is closer
to the original image patch x, and record their response
h. On average, people spent approximately 3 seconds per
judgment. Let T denote our IQA dataset of patch triplets
(x; x0; x1; h).
</p>
<p>
A comparison between our dataset and previous datasets is shown in Table 1. Previous datasets have focused on collecting
large numbers of human judgments for a few images
and distortion types. For example, the largest dataset,
TID2013 [42], has 500k judgments on 3000 distortions
(from 25 input images with 24 distortions types, each sampled
at 5 levels). We provide a complementary dataset that
focuses instead on a large number of distortions types and
a large number of images. In addition, we choose to collect
our judgments on small 64×64 patches as opposed to full
images. There are three reasons for this. First, the space of
full images is extremely large, which makes it much harder
to cover a reasonable portion of the domain with judgments
(even 64×64 color patches represent an intractable 12kdimensional
space). Second, by choosing a smaller patch
size, we focus on lower-level aspects of similarity, to mitigate
the effect of differing “respects of similarity” that may
be influenced by high-level semantics [34]. Finally, modern
methods for image synthesis train deep networks with
patch-based losses (implemented as convolutions) [8, 19].
Our dataset consists of over 161k patches, derived from the
MIT-Adobe 5k dataset [7] (5000 uncompressed images) for
training, and the RAISE1k dataset [9] for validation.
</p>
<p>
To enable large-scale collection, our data is collected
“in-the-wild” on Amazon Mechanical Turk as opposed to
a controlled lab setting. This also allows us to collect data
across a real world distribution of display devices and viewing
conditions, which is important for real world use cases
of perceptual metrics. We ask for 2 judgments per example
in our “train” set and 5 judgments in our “val” sets. Asking
for fewer judgments enables us to explore a larger set of image
patches and distortions. We add sentinels which consist
of pairs of patches with obvious deformations, e.g., a large
amount of Gaussian noise vs a small amount of Gaussian
noise. Approximately 90% of Turkers were able to correctly
pass at least 93% of the sentinels (14 of 15), indicating
that they understood the task and were paying attention.
We choose to use a larger number of distortions than prior
datasets, which allows us to more widely sample the space
of possible use cases for FR-IQA metrics
</p>
<p>
Just noticeable differences (JND) A potential shortcoming
of the 2AFC task is that it is “cognitively penetrable”,
in the sense that participants can consciously choose
which respects of similarity they will choose to focus on
in completing the task [34], which introduces subjectivity
into the judgments. To validate that the judgments actually
reflected something objective and meaningful, we also
collected user judgments of “just noticeable differences”
(JNDs). We show a reference image, followed by a randomly
distorted image, and ask a human if the images are
the same or different. The two image patches are shown for
1 second each, with a 250 ms gap in between. Two images
which look similar may be easily confused, and a good perceptual
metric will be able to order pairs from most to least confusable. JND tests like this may be considered less subjective,
since there is a single correct answer for each judgment,
and participants are presumed to be aware of what
correct behavior entails. We gather 3 JND observations for
each of the 4.8k patches in our traditional and CNN-based
validation sets. Each subject is shown 160 pairs, along with
40 sentinels (32 identical and 8 with large Gaussian noise
distortion applied). We also provide a short training period
of 10 images which contain 4 same pair, 1 obviously different
pair, and 5 different pairs generated by our distortions.
We chose to do this in order to prime the users towards expecting
approximately 40% of the patch pairs to be identical.
Indeed, 36:4% of the pairs were marked “same” (70:4%
of sentinels and 27:9% of test pairs).
</p>
<center><img src="images/table3.png"></center>
<p>
Table 3: Our dataset breakdown. We split our 2AFC
dataset in to three main portions (1,2) training and test
sets with our distortions. Our training and test sets contain
patches sampled from the MIT5k [7] and RAISE1k [9]
datasets, respectively (3) a test set containing real algorithm
outputs, containing patches from a variety of applications.
Our JND data is on traditional and CNN-based distortions.
</p>
<h2>3. Deep Feature Spaces</h2>
<p>
We evaluate feature distances in different networks. For
a given convolutional layer, we compute cosine distance (in
the channel dimension) and average across spatial dimensions
and layers of the network. We also discuss how to
tune an existing network on our data.
</p>
<p>
Network architectures We evaluate the SqueezeNet [18],
AlexNet [26], and VGG [48] architectures. The VGG
network has become the de-facto standard for image generation
tasks [16, 13, 8]. We use the 5 conv layers –conv1 2, conv2 2, conv3 3, conv4 3, conv5 3 –
commonly used for neural style transfer [16]. We also compare
against the shallower AlexNet network, which may
more closely match the architecture of the human visual
cortex [56]. We use the conv1-conv5 layers from [25].
Finally, the SqueezeNet architecture was designed to be extremely
lightweight (2:8 MB) in size, with similar performance
in classification to AlexNet. We use the first conv
layer and subsequent “fire” modules.
</p>
<p>
We additionally evaluate a number of self-supervised
methods, including puzzle solving [37], cross-channel prediction
[59], colorization [58], learning from video [40],
and generative modeling [12]. We use publicly available
networks from these methods and other self-supervised
methods, which use variants of AlexNet [26].
</p>
<p>
Computing a distance from network activations Figure
3 (left) illustrates how we obtain the “distance” between
a reference and distorted patches \((x, x_0)\), using network
\(\mathcal F\). We extract a feature stack from \(L\) layers and
normalize in the channel dimension, which we designate
as \(\hat{\mathcal F}(x)^l, \hat{\mathcal  F}(x_0)^l\in \mathbb  R^{H_l×W_l×C_l}\) for layer \(l\). For each
layer, we scale the activations in each channel by vector
\(w^l\in \mathbb R^{C_l}\) , compute the \(l_2\) distance. Finally, we average
across the spatial and sum across the channel dimensions.
Note that if we use wl = 1 across all layers, this is equivalent
to computing the cosine distance.
\[
d(x,x_0)=\sum_l \frac{1}{H_lW_l}\sum_{h,w}||w_l^T\odot(\hat{\mathcal F}(x)_{hw}^l-\hat{\mathcal F}(x_0)_{hw}^l)||_2^2
\tag{1}
\]
where \(\odot\) represents multiplication across channels
</p>
<center><img src="images/fig3.png"></center>
<p>
Figure 3: Computing distance from a network (Left) To compute a distance d0 between two patches, x, x0, given a network
F, we first compute deep embeddings, normalize the activations in the channel dimension, scale each channel by vector w,
and take the `2 distance. We then average across spatial dimension and across all layers. (Right) A small network G is trained
to predict perceptual judgment h from distance pair (d0; d1).
</p>
<center><img src="images/fig4.png"></center>
<p>
Figure 4: Qualitative comparisons on distortions. We show qualitative comparison on (a) traditional and (b) CNN-based
distortions, using the SSIM [53] metric and BiGAN network [12]. We show examples where both agree the patches are closer
or far, and examples where the metrics disagree. A primary difference is that deep embeddings appear to be more sensitive
to blur. Please see the appendix for additional examples.
</p>
<p>
Training on our data We consider a few different ways to train using our perceptual judgments; these variants are
referred to as linear, tune, and scratch. For the linear configuration,
we keep pre-trained network weights \(mathcal F\) fixed,
and learn linear weights \(w\) on top. This constitutes a “calibration”
of a few parameters in an existing feature space to
better match human perception. For example, for the VGG
network, this consists of learning 1472 parameters. For the
tune configuration, we initialize from a pre-trained classification
model, and allow all the weights for network F to
be fine-tuned. Finally, for scratch model, we initialize the
network from random Gaussian weights and train it entirely
on our judgments. Overall, we refer to these as variants of
our proposed Learned Perceptual Image Patch Similarity
(LPIPS) metric.
</p>
<p>
We illustrate the loss function for training the network in
Figure 3 (right). Given two distances, \((d_0, d_1)\), we train a
small network \(\mathcal G\) on top to map to a score \(\hat{h}\in (0,1)\). The architecture
uses two 32-channel FC-ReLU layers, followed
by a 1-channel FC layer and a sigmoid. Our final loss function
is shown in Equation 2.
\[
\mathcal L(x,x_0,x_1,h)=-h\log\mathcal G(d(x,x_0),d(x,x_1))-(1-h)\log(1-\mathcal G(d(x,x_0),d(x,x_1)))
\tag{2}
\]
</p>
<p>
In preliminary experiments, we also tried a ranking loss,
which attempts to force a constant margin between patch
pairs \(d(x,x_0)\) and \(d(x, x_1)\). We found that using a learned
network, rather than enforcing the same margin in all cases,
worked better.
</p>
<h2>4. Experiments</h2>
<p>
Results on our validation sets are shown in Figure 5. We
first evaluate how well our metrics and networks work. All
validation sets contain 5 pairwise judgments for each triplet.
Because this is an inherently noisy process, we compute
agreement of an algorithm with all of the judgments. For
example, if there are 4 preferences for \(x_0\) and 1 for \(x_1\), an
algorithm which predicts the more popular choice x0 would
receive 80% credit. If a given example is scored with fraction
p humans in one direction and fraction \(1-p\) in the other,
the theoretical maximum for an oracle is \(\max(p, 1-p)\).
However, human performance is lower; if p is perfectly representative
of the empirical distribution of all humans, a human
would achieve score \(p^2 + (1-p)^2\) on expectation.
</p>
<center><img src="images/fig5.png"></center>
<p>
Figure 5: Quantitative comparison. We show a quantitative comparison across metrics on our test sets. (Left) Results
averaged across our traditional and CNN-based distortions. (Right) Results averaged across our 4 real algorithm sets.
</p>
<center><img src="images/fig6.png"></center>
<p>
Figure 6: Correlating Perceptual Tests. We show performance
across a variety of methods, including unsupervised
[24], self-supervised [1, 41, 11, 52, 58, 38, 40, 37, 12,
59], supervised [25, 48, 18], and our proposed perceptuallylearned
metrics (LPIPS). The scores are on our perceptual
tests: 2AFC and JND, averaged across our traditional and
CNN-based distortions.
</p>

<h3>4.1. Evaluations</h3>
<p>
How well do low-level metrics and classification networks
perform? Figure 5 shows the performance of
various low-level metrics, shown in red, and deep networks,
along with human ceiling, shown in black. The scores are
averaged across the 2 distortion test sets (traditional+CNNbased)
in Figure 5(a), and 4 real algorithm benchmarks (superresolution,
frame interpolation, video deblurring, colorization)
in Figure 5(b). Table 5 in the appendix shows the
scores within each test set. Averaged across all 6 test sets,
humans are 73:9% consistent. Interestingly, the supervised
networks perform at about the same level to each other, at
68:6%, 68:9%, and 67:0%, even across variation in model
sizes – SqueezeNet (2:8 MB), AlexNet (9:1 MB), and VGG
(58:9 MB) (only convolutional layers are counted). In Figure
7, we compute scores on the TID2013 [42] dataset. We
note that even averaging across all scales and layers, with
no further calibration, the AlexNet [25] architecture gives
scores near the highest metric, FSIMc [57]. On our tra-ditional perturbations, the FSIMc metric achieves 61:4%,
close to \(l_2\) at 59:9%, while the deep classification networks
we tested achieved 73:3%, 70:6%, and 70:1%, respectively.
</p>
<center><img src="images/fig7.png"></center>
<p>
Figure 7: TID Dataset We show the Spearman correlation
coefficient of various methods on the TID2013 Dataset [42].
Note that deep networks trained for classification perform
well out of the box (blue).
</p>
<center><img src="images/table4.png"></center>
<p>
Table 4: Task correlation. We correlate scores between
our low-level perceptual tests along with high-level semantic
tests across methods. Perceptual scores are averaged between
traditional and CNN-based distortion sets. Correlation
scores are computed for AlexNet-like architectures.
</p>
<p>
Does the perceptual network have to be trained on classification?
In Figure 5, we show model performance
across a variety of unsupervised and self-supervised tasks,
shown in green – generative modeling with BiGANs [12],
solving puzzles [37], cross-channel prediction [59], and
segmenting foreground objects from video [40]. These
self-supervised tasks perform on par with classification networks.
This indicates that tasks across a large spectrum
can induce representations which transfer well to perceptual
distances. Also, the performance of the stacked k-means
method [24], shown in yellow, outperforms low-level metrics.
Random networks, shown in orange, with weights
drawn from a Gaussian, do not yield much improvement.
This indicates that the combination of network structure,
along with orienting filters in directions where data is more
dense can better correlate to perceptual judgments.
</p>
<p>
In Table 6, we explore how well our perceptual task correlates
to semantic tasks on the PASCAL dataset [14], using
results summarized in [59], including additional selfsupervised
methods [1, 41, 11, 52, 58, 38]. For each task
(perceptual or semantic), we treat the performance scores
across different methods as a vector, and compute the correlation
coefficient. The correlation from our 2AFC distortion
preference task to classification and detection is .640 and
.363, respectively. Interestingly, this is similar to the correlation
between the classification and detection tasks (.429),
even though both would be considered “high-level” semantic
tasks, and our perceptual task is “low-level.”
</p>
<p>
Do metrics correlate across different perceptual tasks?
We test if training for the 2AFC distortion preference test
corresponds with another perceptual task, the JND test. We
order patch pairs by ascending order by a given metric, and
compute precision-recall on our CNN-based distortions –
for a good metric, patches which are close together are more
likely to be confused for being the same. We compute area
under the curve, known as mAP [14]. The 2AFC distortion
preference test has high correlation to \(JND –\rho= .928\) when
averaging the results across distortion types. Figure 6 shows
how different methods perform under each perceptual test. This indicates that our test generalizes across another type
of perceptual test, and is giving us signal regarding human
judgments.
</p>
<p>
Can we train a metric on traditional and CNN-based
distortions? In Figure 5, we show performance using
our lin, scratch, and tune configurations, shown in the purple,
pink, and brown, respectively. When validating on the
traditional and CNN-based distortions (Figure 5(a)), we see
improvements. Allowing the network to tune all the way
through (brown) achieves higher performance than simply
learning linear weights (purple) or training from scratch
(pink). The higher capacity network VGG also performs
the lower capacity SqueezeNet and AlexNet. These results
are unsurprising, as the training and test distribution are the
same. However, they verify that networks can indeed learn
from perceptual judgments.
</p>
<p>
Does training on traditional and CNN-based distortions
transfer to real-world scenarios? We are more interested
in how performance generalizes to real-world algorithms,
shown in Figure 5(b). The Squeezenet, AlexNet,
and VGG architectures start at 64:0%, 65:0%, and 62:6%,
respectively. Learning a linear classifier (purple) improves
performance for all networks. Across the 3 networks and
4 real-algorithm tasks, 11 of the 12 scores improved, indicating
that “calibrating” activations on a pre-existing representation
using our data is a safe way to achieve a small
boost in performance (1:1%, 0:3%, and 1:5%, respectively).
Training a network from scratch (pink) yields slightly lower
performance for AlexNet, and slightly higher performance
for VGG than linear calibration. However, these still outperform
low-level metrics. This indicates that the distortions
we have expressed do project onto our test-time tasks
of judging real algorithms.
</p>
<p>
Interestingly, starting with a pre-trained network and tuning
throughout lowers transfer performance. This is an interesting
negative result, as training for a low-level perceptual task directly does not necessarily perform as well as
transferring a representation trained for the high-level task.
</p>
<p>
Where do deep metrics and low-level metrics disagree?
In Figure 4, we show a qualitative comparison across our
traditional and CNN-based distortions for a deep method,
BiGANs [12], and a representation traditional perceptual
method, SSIM [53]. The patch pairs which BiGAN perceives
to be far but SSIM to be close generally contain
some blur. For the opposite case, the BiGAN tends to perceive
correlated noise patterns to be a smaller distortion
than SSIM.
</p>
<h2>5. Conclusions</h2>
<p>
Our results indicate that networks trained to solve challenging
visual prediction and modeling tasks end up learning
a representation of the world that correlates well with
perceptual judgments. A similar story has recently emerged
in the representation learning literature: networks trained on
self-supervised and unsupervised objectives end up learning
a representation that is also effective at semantic tasks [11].
Interestingly, recent findings in neuroscience make much
the same point: representations trained on computer vision
tasks also end up being effective models of neural activity
in macaque visual cortex [56]. Moreover (and roughly
speaking), the stronger the representation is at the computer
vision task, the stronger it is as a model of cortical activity.
Our paper makes a similar finding: the stronger a feature
set is at classification and detection, the stronger it is as a
model of perceptual similarity judgments, as suggested in
Table 4. Together, these results suggest that a good feature
is a good feature. Features that are good at semantic tasks
are also good at self-supervised and unsupervised tasks, and
also provide good models of both human perceptual behavior
and macaque neural activity. This last point aligns with
the “rational analysis” explanation of visual cognition [4],
suggesting that the idiosyncrasies of biological perception
arise as a consequence of a rational agent attempting to
solve natural tasks. Further refining the degree to which
this is true is an important question for future research.
</p>
<h2>Acknowledgements</h2>
<p>
This research was supported, in part, by grants from
Berkeley Deep Drive, NSF IIS-1633310, and hardware
donations by NVIDIA Corp. We thank members of the
Berkeley AI Research Lab and Adobe Creative Intelligence
Lab for helpful discussions. We also thank Radu Timofte,
ZhaowenWang, MichaelWaechter, Simon Niklaus, and
Sergio Guadarrama for help preparing data. RZ is partially
supported by an Adobe Research Fellowship, and much of
this work was done while RZ was an intern at Adobe Research.
</p>
<h2>Appendix</h2>
<p>
We show full quantitative details in Appendix A.We also
discuss training details in Appendix B.
</p>
<h3>A. Quantitative Results</h3>
<p>
In Table 5, we show full quantitative results across all
validation sets and considered metrics, including low-level
metrics, along with random, unsupervised, self-supervised,
supervised, and perceptually-learned networks.
</p>
<p>
In Figures 8, 9, 10, we plot performance in individual
validation sets. Figure 8 shows our traditional and CNNbased
distortions, and Figures 9, 10 show results on real
algorithm applications individually.
</p>
<center><img src="images/fig8.png"></center>
<p>
Figure 8: Individual results (left) traditional distortions (right) CNN-based distortions
</p>
<center><img src="images/fig9.png"></center>
<p>
Figure 9: Individual results (left) superresolution (right) frame interpolation
</p>
<center><img src="images/fig10.png"></center>
<p>
Figure 10: Individual results (left) video deblurring (right) colorization
</p>
<p>
Linearly calibrating networks Learning linear weights
on top of the Alex model achieves state-of-the-art results
on the real algorithms test set. The linear models have a
learned linear layer on top of each channel, whereas the outof-
the-box versions weight each channel equally. In Figure
11a, we show the learned weights for the Alex –frozen
model. The conv1-5 layers contain 64, 192, 384, 256,
and 256 channels, respectively, for a total of 1152 weights.
For each layer, conv1-5, 79:7%, 71:4%, 56:8%, 46:5%,
27:7%, respectively, of the weights are zero. This means
that a majority of the conv1 and conv2 units are ignored,
and almost all of the conv5 units are used. Overall, about
half of the units are ignored. Taking the cosine distance is
equivalent to setting all weights to 1 (Figure 11b).
</p>
<center><img src="images/fig11.png"></center>
<p>
Figure 11: Learned linear weights by layer. (top)We show the learned weights from each layer of our Alex – frozen model.
This is the w term in Figure 3. Each subplot shows the channel weights from each layer, sorted in descending order. The
x-axis shows the channel number, and y-axis shows the weight. Weights are restricted to be non-negative, as image patches
should not have negative distance. (bottom) Unlearned weights correspond to using weighting 1 for each channel in each
layer, which results in computing cosine distance.
</p>


<p>
Data quantity for training models on distortions The
performance of the validation set on our distortions (80:6%
and 81:4% for Alex – tune and VGG – tune, respectively),
is almost equal to human performance of 82:6%. This indicates
that our training set size of 150k patch pairs and 300k
judgments is nearly large enough to fully explore the traditional
and CNN-based distortions which we defined. However,
there is a small gap between the tune and scratch models
(0:4% and 0:6% for Alex and VGG, respectively).
</p>
<h3>B. Model Training Details</h3>
<p>
Here, we provide some additional details on model training
for our networks trained on distortions. We train with
5 epochs at initial learning rate 10􀀀4, 5 epochs with linear
decay, and batch size 50. Each training patch pair is judged
2 times, and the judgments are grouped together. If, for example,
the two judges are split, then the classification target
(h in Figure 3) will be set at 0.5. We enforce non-negative
weightings on the linear layer w, since larger distances in
a certain feature should not result in two patches becoming
closer in the distance metric. This is done by projecting the
weights into the constraint set at every iteration. In other
words, we check for any negative weights, and force them
to be 0. The project was implemented using PyTorch [39].
</p>
<center><img src="images/table5.png"></center>
<p>
Table 5: Results. We show 2AFC scores (higher is better) across a spectrum of methods and test sets. The
bolded & underlined values are the highest performing. The bolded & italicized values are within 0.5% of highest. *LPIPS
metrics are trained on the same traditional and CNN-based distortions, and as such have an advantage relative to other methods
when testing on those same distortion types, even on unseen test images. These values are indicated by gray values. The
best gray value per column is also bolded.
</p>

<h2>References</h2>
<p>
<div class="styleRef">
<ul>
</li><li>[1] P. Agrawal, J. Carreira, and J. Malik. Learning to see by
 moving. In ICCV, pages 37–45, 2015. 8
 </li><li>[2] E. Agustsson and R. Timofte. Ntire 2017 challenge on single
 image super-resolution: Dataset and study. In CVPR Workshops,
 July 2017. 4
 </li><li>[3] S. Ali Amirshahi, M. Pedersen, and S. X. Yu. Image quality
 assessment by comparing cnn features between images.
 Electronic Imaging, 2017(12):42–51, 2017. 3
 </li><li>[4] J. R. Anderson. The adaptive character of thought. Psychology
 Press, 1990. 9
 </li><li>[5] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. J. Black, and
 R. Szeliski. A database and evaluation methodology for optical
 flow. IJCV, 2011. 5
 </li><li>[6] A. Berardino, V. Laparra, J. Ball´e, and E. Simoncelli. Eigendistortions
 of hierarchical representations. In NIPS, 2017.
 3
 </li><li>[7] V. Bychkovsky, S. Paris, E. Chan, and F. Durand. Learning
 photographic global tonal adjustment with a database of
 input / output image pairs. In CVPR, 2011. 3, 5
 </li><li>[8] Q. Chen and V. Koltun. Photographic image synthesis with
 cascaded refinement networks. ICCV, 2017. 2, 5
 </li><li>[9] D.-T. Dang-Nguyen, C. Pasquini, V. Conotter, and G. Boato.
 Raise: a raw images dataset for digital image forensics. In
 Proceedings of the 6th ACM Multimedia Systems Conference,
 pages 219–224. ACM, 2015. 3, 5
 </li><li>[10] M. Delbracio and G. Sapiro. Hand-held video deblurring via
 efficient fourier aggregation. IEEE Transactions on Computational
 Imaging, 1(4):270–283, 2015. 4
 </li><li>[11] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual
 representation learning by context prediction. In ICCV,
 2015. 8, 9
 </li><li>[12] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature
 learning. ICLR, 2017. 1, 2, 6, 7, 8, 9, 11
 </li><li>[13] A. Dosovitskiy and T. Brox. Generating images with perceptual
 similarity metrics based on deep networks. In HIPS,
 pages 658–666, 2016. 2, 5
 </li><li>[14] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
 and A. Zisserman. The PASCAL Visual Object Classes
 Challenge 2007 (VOC2007) Results. http://www.pascalnetwork.
 org/challenges/VOC/voc2007/workshop/index.html.
 8
 </li><li>[15] F. Gao, Y. Wang, P. Li, M. Tan, J. Yu, and Y. Zhu. Deepsim:
 Deep similarity for image quality assessment. Neurocomputing,
 2017. 3
 </li><li>[16] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
 using convolutional neural networks. In CVPR, pages 2414–
 2423, 2016. 2, 5, 7
 </li><li>[17] N. Goodman. Seven strictures on similarity. Problems and
 Projects, 1972. 2
 </li><li>[18] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
 Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
 with 50x fewer parameters and < 0.5 mb model size. CVPR,
 2017. 1, 2, 5, 8, 11
 </li><li>[19] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
 translation
 </li><li>[20] P. Isola, D. Zoran, D. Krishnan, and E. H. Adelson. Learning
 visual groups from co-occurrences in space and time. ICCV
 Workshop, 2016. 4
 </li><li>[21] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
 real-time style transfer and super-resolution. ECCV, 2016. 2
 </li><li>[22] J. Kim, J. Kwon Lee, and K. Mu Lee. Accurate image superresolution
 using very deep convolutional networks. In CVPR,
 pages 1646–1654, 2016. 4
 </li><li>[23] J. Kim and S. Lee. Deep learning of human visual sensitivity
 in image quality assessment framework. In CVPR, 2017. 3
 </li><li>[24] P. Kr¨ahenb¨uhl, C. Doersch, J. Donahue, and T. Darrell.
 Data-dependent initializations of convolutional neural networks.
 International Conference on Learning Representations, 2016. 1, 2, 8, 11
 </li><li>[25] A. Krizhevsky. One weird trick for parallelizing convolutional
 neural networks. arXiv preprint arXiv:1404.5997,
 2014. 1, 7, 8, 11
 </li><li>[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
 classification with deep convolutional neural networks. In
 NIPS, 2012. 2, 5, 7
 </li><li>[27] E. C. Larson and D. M. Chandler. Most apparent distortion:
 full-reference image quality assessment and the role of strategy.
 Journal of Electronic Imaging, 2010. 3
 </li><li>[28] G. Larsson, M. Maire, and G. Shakhnarovich. Learning representations
 for automatic colorization. ECCV, 2016. 4
 </li><li>[29] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,
 A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
 Photo-realistic single image super-resolution using a generative
 adversarial network. CVPR, 2016. 4
 </li><li>[30] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee. Enhanced
 deep residual networks for single image super-resolution. In
 CVPR Workshops, 2017. 5
 </li><li>[31] C. Liu et al. Beyond pixels: exploring new representations
 and applications for motion analysis. PhD thesis, Massachusetts
 Institute of Technology, 2009. 4
 </li><li>[32] R. Mantiuk, K. J. Kim, A. G. Rempel, andW. Heidrich. Hdrvdp-
 2: A calibrated visual metric for visibility and quality
 predictions in all luminance conditions. In ACM Transactions
 on Graphics (TOG), 2011. 2, 11
 </li><li>[33] A. B. Markman and D. Gentner. Nonintentional similarity
 processing. The new unconscious, pages 107–137, 2005. 2
 </li><li>[34] D. L. Medin, R. L. Goldstone, and D. Gentner. Respects for
 similarity. Psychological review, 100(2):254, 1993. 2, 5
 </li><li>[35] S. Meyer, O. Wang, H. Zimmer, M. Grosse, and A. Sorkine-
 Hornung. Phase-based frame interpolation for video. In CVPR, pages 1410–1418, 2015. 4
 </li><li>[36] S. Niklaus, L. Mai, and F. Liu. Video frame interpolation via
 adaptive separable convolution. In ICCV, 2017. 4
 </li><li>[37] M. Noroozi and P. Favaro. Unsupervised learning of visual
 representations by solving jigsaw puzzles. ECCV, 2016. 1,
 2, 7, 8, 11
 </li><li>[38] A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adelson,
 and W. T. Freeman. Visually indicated sounds. CVPR,
 2016. 8
 </li><li>[39] A. Paszke, S. Chintala, R. Collobert, K. Kavukcuoglu,
 C. Farabet, S. Bengio, I. Melvin, J. Weston, and J. Mariethoz.
 Pytorch: Tensors and dynamic neural networks in
 python with strong gpu acceleration, may 2017. 10
 </li><li>[40] D. Pathak, R. Girshick, P. Doll´ar, T. Darrell, and B. Hariharan.
 Learning features by watching objects move. CVPR,
 2017. 1, 7, 8, 11
 </li><li>[41] D. Pathak, P. Kr¨ahenb¨uhl, J. Donahue, T. Darrell, and
 A. Efros. Context encoders: Feature learning by inpainting.
 In CVPR, 2016. 8
 </li><li>[42] N. Ponomarenko, L. Jin, O. Ieremeiev, V. Lukin, K. Egiazarian,
 J. Astola, B. Vozel, K. Chehdi, M. Carli, F. Battisti, et al.
 Image database tid2013: Peculiarities, results and perspectives.
 Signal Processing: Image Communication, 2015. 2, 3,
 5, 8, 9
 </li><li>[43] N. Ponomarenko, V. Lukin, A. Zelensky, K. Egiazarian,
 M. Carli, and F. Battisti. Tid2008-a database for evaluation
 of full-reference visual quality assessment metrics. Advances
 of Modern Radioelectronics, 2009. 3
 </li><li>[44] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
 S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
 et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 1, 4, 5
 </li><li>[45] M. S. Sajjadi, B. Sch¨olkopf, and M. Hirsch. Enhancenet:
 Single image super-resolution through automated texture
 synthesis. ICCV, 2017. 4
 </li><li>[46] D. Scharstein and R. Szeliski. A taxonomy and evaluation of
 dense two-frame stereo correspondence algorithms. IJCV,
 2002. 4, 5
 </li><li>[47] H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A statistical
 evaluation of recent full reference image quality assessment
 algorithms. TIP, 2006. 3
 </li><li>[48] K. Simonyan and A. Zisserman. Very deep convolutional
 networks for large-scale image recognition. arXiv, 2014. 1, 2, 5, 8, 11
 </li><li>[49] S. Su, M. Delbracio, J. Wang, G. Sapiro, W. Heidrich, and
 O. Wang. Deep video deblurring for hand-held cameras. In
 CVPR, 2017. 4
 </li><li>[50] H. Talebi and P. Milanfar. Learned perceptual image enhancement.
 ICCV, 2017. 3
 </li><li>[51] A. Tversky. Features of similarity. Psychological review,
 1977. 2
 </li><li>[52] X.Wang and A. Gupta. Unsupervised learning of visual representations
 using videos. In ICCV, 2015. 8
 </li><li>[53] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.
 Image quality assessment: from error visibility to structural
 similarity. TIP, 2004. 2, 6, 9, 11
 </li><li>[54] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang. Deep
 networks for image super-resolution with sparse prior. In
 ICCV, 2015. 4
 </li><li>[55] Z.Wang, E. P. Simoncelli, and A. C. Bovik. Multiscale structural
 similarity for image quality assessment. In Signals, Systems
 and Computers. IEEE, 2004. 2
 </li><li>[56] D. L. Yamins and J. J. DiCarlo. Using goal-driven deep
 learning models to understand sensory cortex. Nature neuroscience,
 2016. 7, 9
 </li><li>[57] L. Zhang, L. Zhang, X. Mou, and D. Zhang. Fsim: A feature
 similarity index for image quality assessment. TIP, 2011. 2,
 8, 11
 </li><li>[58] R. Zhang, P. Isola, and A. A. Efros. Colorful image colorization.
 ECCV, 2016. 4, 7, 8
 </li><li>[59] R. Zhang, P. Isola, and A. A. Efros. Split-brain autoencoders:
 Unsupervised learning by cross-channel prediction.
 In CVPR, 2017. 1, 2, 7, 8, 11 </li>
</ul>
</div>
</p>
    </body>
</html>