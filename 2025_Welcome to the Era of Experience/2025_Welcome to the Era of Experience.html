<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Welcome to the Era of Experience</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1><center>Welcome to the Era of Experience</center></h1>
<center>体験の時代へようこそ</center><br>
<br>
<center>David Silver, Richard S. Sutton</center>

<h2><center>要旨</center></h2>

<p class="margin-large"> 
私たちは、前例のないレベルの能力の達成を約束する人工知能の新たな時代の入り口に立っています。新世代のエージェントは、主に経験から学習することで、超人的な能力を獲得するでしょう。本稿では、この来たる時代を特徴づける重要な特徴を探ります。
<!--
 We stand on the threshold of a new era in artificial intelligence that promises to achieve an unprecedented  level of ability. A new generation of agents will acquire superhuman capabilities by learning predominantly  from experience. This note explores the key characteristics that will define this upcoming era.  --> </p>

  <h2>ヒューマンデータの時代</h2>

  <p>  
人工知能（AI）は、人間が生成した膨大なデータで学習し、専門家の事例や好みに基づいて微調整することで、近年目覚ましい進歩を遂げてきました。このアプローチは、広範囲にわたる汎用性を達成した大規模言語モデル（LLM）に例証されています。現在では、単一のLLMで詩作や物理学の問題の解決から、医療問題の診断や法律文書の要約まで、幅広いタスクを実行できます。
<!--
Artificial intelligence (AI) has made remarkable strides over recent years by training on massive amounts of  human-generated data and fine-tuning with expert human examples and preferences. This approach is exemplified  by large language models (LLMs) that have achieved a sweeping level of generality. A single LLM  can now perform tasks spanning from writing poetry and solving physics problems to diagnosing medical  issues and summarising legal documents.  
-->
</p>
  <p>  
しかし、人間を模倣することは、多くの人間の能力を十分なレベルまで再現するのに十分ですが、このアプローチだけでは、多くの重要なトピックやタスクにおいて超人的な知能を達成できておらず、おそらく達成できないでしょう。数学、コーディング、科学といった主要分野において、人間のデータから抽出される知識は急速に限界に近づいています。高品質なデータソースの大部分、つまり強力なエージェントのパフォーマンスを実際に向上させることができるものは、すでに消費されているか、まもなく消費されるでしょう。人間のデータのみを用いた教師あり学習によって推進される進歩のペースは明らかに鈍化しており、新たなアプローチの必要性を示唆しています。さらに、新しい定理、技術、科学的ブレークスルーといった価値ある新たな洞察は、現在の人間の理解の限界を超えており、既存の人間のデータでは捉えることができません。
<!--
However, while imitating humans is enough to reproduce many human capabilities to a competent level,  this approach in isolation has not and likely cannot achieve superhuman intelligence across many important  topics and tasks. In key domains such as mathematics, coding, and science, the knowledge extracted from  human data is rapidly approaching a limit. The majority of high-quality data sources - those that can actually  improve a strong agent’s performance - have either already been, or soon will be consumed. The pace of  progress driven solely by supervised learning from human data is demonstrably slowing, signalling the need  for a new approach. Furthermore, valuable new insights, such as new theorems, technologies or scientific  breakthroughs, lie beyond the current boundaries of human understanding and cannot be captured by existing  human data.  -->
</p>

  <h2>経験の時代</h2>

  <p> 
さらなる飛躍的な進歩を遂げるには、新たなデータ源が必要です。このデータは、エージェントが強くなるにつれて継続的に改善される方法で生成されなければなりません。静的なデータ合成手順は、すぐに追い越されてしまいます。これは、エージェントが自身の経験、つまり環境との相互作用によって生成されるデータから継続的に学習できるようにすることで実現できます。AIは、経験が改善の主要な手段となり、最終的には今日のシステムで使用されている人間のデータの規模をはるかに超える、新たな時代の瀬戸際にいます。
<!--
 To progress significantly further, a new source of data is required. This data must be generated in a way that  continually improves as the agent becomes stronger; any static procedure for synthetically generating data  will quickly become outstripped. This can be achieved by allowing agents to learn continually from their  own experience, i.e., data that is generated by the agent interacting with its environment. AI is at the cusp of  a new period in which experience will become the dominant medium of improvement and ultimately dwarf  the scale of human data used in today’s systems.  
-->
</p>
  <p>  
この移行は、人間中心のAIの典型である大規模言語モデルにおいても、すでに始まっているかもしれません。その一例は数学の能力です。AlphaProof [20]は最近、国際数学オリンピックでメダルを獲得した最初のプログラムとなり、人間中心のアプローチ[27, 19]の性能を凌駕しました。AlphaProofの強化学習（RL）アルゴリズム<sup>1</sup>は、当初、人間の数学者によって長年かけて作成された約10万通りの形式的証明に曝され、その後、形式的証明システムとの継続的なインタラクションを通じてさらに1億通りの証明を生成しました。インタラクティブな体験に重点を置くことで、AlphaProofは既存の形式的証明の枠を超えた数学的可能性を探求し、斬新で困難な問題の解を発見することができました。非公式数学もまた、専門家が生成したデータを自己生成データに置き換えることで成功を収めています。例えば、DeepSeekの最近の研究は、強化学習の力と美しさを強調しています。モデルに問題の解決方法を明示的に教えるのではなく、適切なインセンティブを与えるだけで、モデルは高度な問題解決戦略を自律的に開発するのです。」[10]
<!--
This transition may have already started, even for the large language models that epitomise human-centric  AI. One example is in the capability of mathematics. AlphaProof [20] recently became the first program to  achieve a medal in the International Mathematical Olympiad, eclipsing the performance of human-centric  approaches [27, 19]. Initially exposed to around a hundred thousand formal proofs, created over many years by human mathematicians, AlphaProof’s reinforcement learning (RL) algorithm <sup>1</sup>
 subsequently generated a  hundred million more through continual interaction with a formal proving system. This focus on interactive  experience allowed AlphaProof to explore mathematical possibilities beyond the confines of pre-existing  formal proofs, so as to discover solutions to novel and challenging problems. Informal mathematics has  also achieved success by replacing expert generated data with self-generated data; for example, recent work  from DeepSeek “underscores the power and beauty of reinforcement learning: rather than explicitly teaching  the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously  develops advanced problem-solving strategies.” [10]  
-->
</p>
  <p class="margin-large">  <sup>1</sup>
強化学習アルゴリズムとは、試行錯誤によって目標を達成することを学習するアルゴリズムです。つまり、環境との相互作用の経験に基づいて行動を適応させるのです。適応は、ニューラルネットワークの重みを更新したり、環境からのフィードバックに基づいて状況に応じて適応したりするなど、様々な方法で行われます。
<!--
 An RL algorithm is one that learns to achieve a goal by trial and error, i.e., adapting its behaviour from its experience of interacting  with its environment. Adaptation may happen by any means, for example updating the weights of a neural network, or adapting incontext  based on feedback from the environment.  
-->
</p>
  <p>  
経験学習の潜在能力が最大限に活用されれば、驚くべき新たな能力が生まれるというのが私たちの主張です。この経験の時代は、膨大な経験データから学習するだけでなく、人間中心のAIシステムの限界をさらにいくつかの側面で打ち破るエージェントと環境によって特徴づけられるでしょう。<div class="styleBullet"> <ul> <li>• エージェントは、短いインタラクションの断片ではなく、経験の流れの中を生きていきます。</li>
<br><li>• エージェントの行動と観察は、人間との対話のみでインタラクションするのではなく、環境に深く根ざしたものになります。</li>
<br><li>• エージェントの報酬は、人間の先入観ではなく、環境に関する経験に基づいて決まります。</li>
<br><li>• エージェントは、人間の言葉だけで推論するのではなく、経験に基づいて計画を立てたり、推論したりします。</li>
<!--
Our contention is that incredible new capabilities will arise once the full potential of experiential learning  is harnessed. This era of experience will likely be characterised by agents and environments that, in addition  to learning from vast quantities of experiential data, will break through the limitations of human-centric AI  systems in several further dimensions:  <div class="styleBullet">  <ul>  <li>• Agents will inhabit streams of experience, rather than short snippets of interaction.  </li>
<br><li>• Their actions and observations will be richly grounded in the environment, rather than interacting via  human dialogue alone.  </li>
<br><li>• Their rewards will be grounded in their experience of the environment, rather than coming from human  prejudgement.  </li>
<br><li>• They will plan and/or>
 reason about experience, rather than reasoning solely in human terms</li>
-->
  </ul>
  </div>
  </p>
  <p>  
適切に選択されたアルゴリズムを備えた今日のテクノロジーは、これらのブレークスルーを達成するための十分に強力な基盤を既に提供していると私たちは信じています。さらに、AIコミュニティがこの課題に取り組むことで、これらの方向への新たなイノベーションが促進され、AIは真に超人的なエージェントへと急速に進化していくでしょう。
<!--
We believe that today’s technology, with appropriately chosen algorithms, already provides a sufficiently  powerful foundation to achieve these breakthroughs. Furthermore, the pursuit of this agenda by the AI  community will spur new innovations in these directions that rapidly progress AI towards truly superhuman  agents.  
-->
</p>

  <h2>ストリーム</h2>

  <p>
経験的エージェントは生涯を通じて学習を続けることができます。人間のデータの時代において、言語ベースのAIは主に短いインタラクションエピソードに焦点を当ててきました。例えば、ユーザーが質問し、（おそらく数回の思考ステップやツール使用アクションの後に）エージェントが応答するといったものです。通常、あるエピソードから次のエピソードに引き継がれる情報はほとんど、あるいは全くないため、時間の経過とともに適応することはできません。さらに、エージェントはユーザーの質問に直接答えるなど、現在のエピソード内での結果のみを目指します。対照的に、人間（および他の動物）は、長年にわたる継続的な行動と観察のストリームの中に存在します。情報はストリーム全体にわたって伝達され、彼らの行動は過去の経験から適応し、自己修正と改善を行います。さらに、目標は、ストリームの遥か未来にまで及ぶ行動と観察という観点から指定される場合があります。例えば、人間は健康の改善、言語の学習、科学的なブレークスルーの達成といった長期的な目標を達成するために行動を選択する場合があります。
<!--
  An experiential agent can continue to learn throughout a lifetime. In the era of human data, language-based AI  has largely focused on short interaction episodes: e.g., a user asks a question and (perhaps after a few thinking  steps or tool-use actions) the agent responds. Typically, little or no information carries over from one episode  to the next, precluding any adaptation over time. Furthermore, the agent aims exclusively for outcomes  within the current episode, such as directly answering a user’s question. In contrast, humans (and other  animals) exist in an ongoing stream of actions and observations that continues for many years. Information is  carried across the entire stream, and their behaviour adapts from past experiences to self-correct and improve.  Furthermore, goals may be specified in terms of actions and observations that stretch far into the future of  the stream. For example, humans may select actions to achieve long-term goals like improving their health,  learning a language, or achieving a scientific breakthrough.  
-->
</p>
  <p>  
強力なエージェントは、人間のように長期にわたって進化する独自の経験の流れを持つべきです。これにより、エージェントは将来の目標を達成するための行動を起こし、時間の経過とともに新しい行動パターンに継続的に適応することができます。例えば、ユーザーのウェアラブルに接続された健康とウェルネスエージェントは、睡眠パターン、活動レベル、食習慣を数か月にわたって監視することができます。そして、長期的な傾向とユーザーの具体的な健康目標に基づいて、パーソナライズされた推奨事項や励ましを提供し、ガイダンスを調整することができます。同様に、パーソナライズされた教育エージェントは、ユーザーの新しい言語学習の進捗状況を追跡し、知識のギャップを特定し、学習スタイルに適応し、数か月または数年にわたって指導方法を調整することができます。さらに、科学エージェントは、新素材の発見や二酸化炭素の削減といった野心的な目標を追求することができます。このようなエージェントは、長期間にわたって現実世界の観察を分析し、シミュレーションを開発・実行し、現実世界の実験や介入を提案することができます。
<!--
Powerful agents should have their own stream of experience that progresses, like humans, over a long  time-scale. This will allow agents to take actions to achieve future goals, and to continuously adapt over  time to new patterns of behaviour. For example, a health and wellness agent connected to a user’s wearables  could monitor sleep patterns, activity levels, and dietary habits over many months. It could then provide  personalized recommendations, encouragement, and adjust its guidance based on long-term trends and the  user’s specific health goals. Similarly, a personalized education agent could track a user’s progress in learning a new language, identify knowledge gaps, adapt to their learning style, and adjust its teaching methods over  months or even years. Furthermore, a science agent could pursue ambitious goals, such as discovering a new  material or reducing carbon dioxide. Such an agent could analyse real-world observations over an extended  period, developing and running simulations, and suggesting real-world experiments or interventions.  
-->
</p>
  <p>  
いずれの場合も、エージェントは指定された目標に関して長期的な成功を最大化するために、一連のステップを実行します。個々のステップはすぐには利益をもたらさない、あるいは短期的には有害となる場合もありますが、それでも全体としては長期的な成功に貢献する可能性があります。これは、要求に対して即座に応答するものの、行動が環境に及ぼす将来の影響を測定または最適化する能力を持たない現在のAIシステムとは大きく対照的です。
<!--
In each case, the agent takes a sequence of steps so as to maximise long-term success with respect to the  specified goal. An individual step may not provide any immediate benefit, or may even be detrimental in the  short term, but may nevertheless contribute in aggregate to longer term success. This contrasts strongly with  current AI systems that provide immediate responses to requests, without any ability to measure or optimise  the future consequences of their actions on the environment.  
-->
</p>

  <h2>行動と観察</h2>

  <p>  
経験の時代のエージェントは、現実世界で自律的に行​​動します。ヒューマンデータの時代におけるLLMは、主に人間に特権的な行動や観察、つまりユーザーへのテキスト出力とユーザーからのテキスト入力をエージェントに返すことに焦点を当てていました。これは、動物が運動制御とセンサーを通して環境と相互作用する自然知能とは大きく異なります。動物、特に人間は他の動物とコミュニケーションをとることがありますが、それは特権的なチャネルではなく、他の感覚運動制御と同じインターフェースを通して行われます。
<!--
Agents in the era of experience will act autonomously in the real world. LLMs in the era of human data  focused primarily on human-privileged actions and observations that output text to a user, and input text from  the user back into the agent. This differs markedly from natural intelligence, in which an animal interacts  with its environment through motor control and sensors. While animals, and most notably humans, may  communicate with other animals, this occurs through the same interface as other sensorimotor control rather  than a privileged channel.  
-->
</p>
  <p>  
LLMがデジタル世界におけるアクション、例えばAPIの呼び出しなどによっても実行できることは、以前から認識されてきました（例えば[43]を参照）。当初、これらの能力はエージェント自身の経験からではなく、主に人間の道具使用例から生まれました。しかし、コーディングと道具使用能力は、エージェントが実際にコードを実行し、何が起こるかを観察する実行フィードバック[17, 7, 12]に基づいて構築されることが多くなりました。最近では、新しいプロトタイプエージェントの波が、人間がコンピュータを操作するのと同じインターフェースを用いることで、より一般的な方法でコンピュータと対話し始めています[3, 15, 24]。これらの変化は、人間のみが特権を持つコミュニケーションから、エージェントが世界の中で独立して行動できる、はるかに自律的な対話への移行を告げています。このようなエージェントは、世界を積極的に探索し、変化する環境に適応し、人間には思いつかないような戦略を発見することができるでしょう。
<!--
It has long been recognised that LLMs may also invoke actions in the digital world, for example by  calling APIs (see for example, [43]). Initially, these capabilities came largely from human examples of  tool-use, rather than from the experience of the agent. However, coding and tool-use capabilities have built  increasingly upon execution feedback [17, 7, 12], where the agent actually runs code and observes what  happens. Recently, a new wave of prototype agents have started to interact with computers in an even more  general manner, by using the same interface that humans use to operate a computer [3, 15, 24]. These  changes herald a transition from exclusively human-privileged communication, to much more autonomous  interactions where the agent is able to act independently in the world. Such agents will be able to actively  explore the world, adapt to changing environments, and discover strategies that might never occur to a human.  
-->
</p>
  <p>  
これらのより豊かなインタラクションは、デジタル世界を自律的に理解し、制御する手段を提供します。エージェントは、ユーザーインターフェースなどの「人間に優しい」行動や観察を用いることで、ユーザーとのコミュニケーションやコラボレーションを自然に促進します。また、コードの実行やAPIの呼び出しといった「機械に優しい」行動をとることで、エージェントは自らの目的を達成するために自律的に行​​動することができます。エクスペリエンスの時代においては、エージェントはデジタルインターフェースを介して現実世界ともインタラクションするようになります。例えば、科学的なエージェントは、環境センサーを監視したり、望遠鏡を遠隔操作したり、実験室でロボットアームを制御して自律的に実験を行ったりすることができます。
<!--
These richer interactions will provide a means to autonomously understand and control the digital world.  The agent may use ‘human-friendly’ actions and observations such as user interfaces, that naturally facilitate  communication and collaboration with the user. The agent may also take ‘machine-friendly’ actions that  execute code and call APIs, allowing the agent to act autonomously in service of its goals. In the era of  experience, agents will also interact with the real world via digital interfaces. For example, a scientific agent  could monitor environmental sensors, remotely operate a telescope, or control a robotic arm in a laboratory  to autonomously conduct experiments  
-->
</p>

  <h2>報酬</h2>

  <p>  
経験的エージェントが人間の好みだけでなく、外部のイベントや信号から学習できたらどうなるでしょうか?
<!--
What if experiential agents could learn from external events and signals, and not just human preferences?  
-->
</p>
  <p>  
人間中心の LLM は通常、人間の予断に基づいて報酬を最適化します。つまり、専門家がエージェントの行動を観察し、それが良い行動かどうかを判断するか、複数の選択肢の中から最適なエージェントの行動を選択します。たとえば、専門家は健康エージェントのアドバイス、教育アシスタントの指導、科学者エージェントの提案した実験を判断する場合があります。これらの報酬や選好は、それらの行動が環境に与える影響を測定するのではなく、その結果がない状態で人間によって決定されるため、世界の現実に直接根拠づけられていません。このように人間の予断に頼ると、通常、エージェントのパフォーマンスに天井知らずの限界が生じます。つまり、エージェントは、人間の評価者によって過小評価されているより良い戦略を発見することができません。既存の人間の知識をはるかに超える新しいアイデアを発見するには、環境自体から生じるシグナルである、根拠のある報酬を使用する必要があります。例えば、健康アシスタントは、ユーザーの健康目標を、安静時の心拍数、睡眠時間、活動レベルなどの信号の組み合わせに基づく報酬に組み込むことができます。一方、教育アシスタントは、試験結果を用いて、語学学習に対する根拠のある報酬を提供することができます。同様に、地球温暖化の軽減を目標とする科学エージェントは、二酸化炭素濃度の実験的観測に基づく報酬を使用するかもしれません。また、より強度の高い材料を発見するという目標は、引張強度やヤング率といった材料シミュレーターの測定値の組み合わせに基づいているかもしれません。
<!--
Human-centric LLMs typically optimise for rewards based on human prejudgement: an expert observes  the agent’s action and decides whether it is a good action, or picks the best agent action among multiple alternatives.  For example, an expert may judge a health agent’s advice, an educational assistant’s teaching, or a  scientist agent’s suggested experiment. The fact that these rewards or preferences are determined by humans  in absence of their consequences, rather than measuring the effect of those actions on the environment, means  that they are not directly grounded in the reality of the world. Relying on human prejudgement in this manner  usually leads to an impenetrable ceiling on the agent’s performance: the agent cannot discover better strategies  that are underappreciated by the human rater. To discover new ideas that go far beyond existing human  knowledge, it is instead necessary to use grounded rewards: signals that arise from the environment itself.  For example, a health assistant could ground the user’s health goals into a reward based on a combination of signals such as their resting heart rate, sleep duration, and activity levels, while an educational assistant could  use exam results to provide a grounded reward for language learning. Similarly, a science agent with a goal  to reduce global warming might use a reward based on empirical observations of carbon dioxide levels, while  a goal to discover a stronger material might be grounded in a combination of measurements from a materials  simulator, such as tensile strength or Young’s modulus.  
-->
</p>
  <p>  
根拠のある報酬は、エージェントの環境を構成する人間から発生する可能性があります<sup>2</sup>。例えば、人間のユーザーがケーキの美味しさ、運動後の疲労感、頭痛の痛みの程度を報告することで、アシスタントエージェントはより良いレシピを提供したり、フィットネスの提案を洗練させたり、推奨される薬を改善したりすることができます。このような報酬は、エージェントの行動が環境内でどのような結果をもたらすかを測定するものであり、最終的には、提案されたケーキのレシピ、運動プログラム、または治療プログラムについて先入観を持つ人間の専門家よりも優れた支援につながるはずです。
<!--
Grounded rewards may arise from humans that are part of the agent’s environment.<sup>2</sup>
 For example, a  human user could report whether they found a cake tasty, how fatigued they are after exercising, or the level  of pain from a headache, enabling an assistant agent to provide better recipes, refine its fitness suggestions,  or improve its recommended medication. Such rewards measure the consequence of the agent’s actions  within their environment, and should ultimately lead to better assistance than a human expert that prejudges  a proposed cake recipe, exercise program, or treatment program.  
-->
</p>
  <p class="margin-large">  <sup>2</sup>
経験と人間のデータは正反対ではありません。例えば、犬は完全に経験から学びますが、人間との交流もその経験の一部です。
<!-- 
Experience and human data are not exact opposites. For example, a dog learns entirely from experience, but human interaction is  part of its experience.  
-->
</p>
  <p>  
人間のデータからではないとしたら、報酬はどこから来るのでしょうか？エージェントが豊富な行動空間と観察空間（上記参照）を通じて世界とつながるようになれば、報酬の根拠となる根拠のあるシグナルは不足することはありません。実際、世界にはコスト、エラー率、空腹感、生産性、健康指標、気候指標、利益、売上高、試験結果、成功、訪問数、収穫量、株価、いいね数、所得、快楽／苦痛、経済指標、正確性、力、距離、速度、効率、エネルギー消費といった量が溢れています。さらに、特定のイベントの発生や、観察と行動の生のシーケンスから得られる特徴から生じる無数の追加シグナルも存在します。
<!--
Where do rewards come from, if not from human data? Once agents become connected to the world  through rich action and observation spaces (see above), there will be no shortage of grounded signals to  provide a basis for reward. In fact, the world abounds with quantities such as cost, error rates, hunger, productivity,  health metrics, climate metrics, profit, sales, exam results, success, visits, yields, stocks, likes,  income, pleasure/pain>
, economic indicators, accuracy, power, distance, speed, efficiency, or energy consumption.  In addition there are innumerable additional signals arising from the occurrence of specific events, or  from features derived from raw sequences of observations and actions.  
-->
</p>
  <p>  
原理的には、それぞれが報酬として単一のグラウンディング信号に最適化する、多様なエージェントを作成することが考えられます。そのような報酬信号が1つでも、非常に効果的に最適化されていれば、広範囲に能力のある知能を誘発するのに十分である可能性があるという議論があります[34]<sup>3</sup>
これは、複雑な環境において単純な目標を達成するためには、多くの場合、多種多様なスキルを習得する必要があるためです。
<!--
One could in principle create a variety of distinct agents, each optimising for one grounded signal as its  reward. There is an argument that even a single such reward signal, optimised with great effectiveness, may  be sufficient to induce broadly capable intelligence [34].<sup>3</sup>
 This is because the achievement of a simple goal  in a complex environment may often require a wide variety of skills to be mastered.  
-->
</p>
  <p class="margin-large">  <sup>3</sup>
報酬で十分という仮説は、知能とそれに関連する能力は報酬の最大化から自然に発現する可能性があると示唆しています。これには、人間との交流や人間のフィードバックに基づく報酬を含む環境が含まれる可能性があります。
<!-- 
The reward-is-enough hypothesis suggests that intelligence, and its associated abilities, can emerge naturally from the maximisation  of reward. This may include environments containing human interaction and rewards based on human feedback.  
-->
</p>
  <p>  
しかし、単一の報酬信号の追求は、一見すると、ユーザーが望む任意の行動へと確実に誘導できる汎用AIの要件を満たしていないように思われます。では、グラウンディングされた非人間的な報酬信号の自律的な最適化は、現代のAIシステムの要件に反するのでしょうか？私たちは、これらの要件を満たす可能性のある一つのアプローチを概説することで、必ずしもそうではないと主張します。他のアプローチも可能である可能性があります。</p>
<p> アイデアは、グラウンディングされた信号に基づいて、ユーザー主導の方法で報酬を柔軟に適応させることです。例えば、報酬関数は、エージェントとユーザーおよび環境の両方との相互作用を入力として受け取り、スカラー報酬を出力するニューラルネットワークによって定義できます。これにより、報酬はユーザーの目標に応じて環境からの信号を選択または組み合わせることができます。例えば、ユーザーが「フィットネスを向上させる」といった大まかな目標を指定した場合、報酬関数はユーザーの心拍数、睡眠時間、歩数などの関数を返すかもしれません。あるいは、ユーザーが「スペイン語の学習を手伝ってほしい」といった目標を指定した場合、報酬関数はユーザーのスペイン語の試験結果を返すかもしれません。
<!--
However, the pursuit of a single reward signal does not on the surface appear to meet the requirements of a  general-purpose AI that can be steered reliably towards arbitrary user-desired behaviours. Is the autonomous  optimisation of grounded, non-human reward signals therefore in opposition to the requirements of modern  AI systems? We argue that this is not necessarily the case, by sketching one approach that may meet these  desiderata; other approaches may also be possible.  </p>
  <p>  The idea is to flexibly adapt the reward, based on grounded signals, in a user-guided manner. For example,  the reward function could be defined by a neural network that takes the agent’s interactions with both the user  and the environment as input, and outputs a scalar reward. This allows the reward to select or combine  together signals from the environment in a manner that depends upon the user’s goal. For example, a user  might specify a broad goal such as ’improve my fitness’ and the reward function might return a function  of the user’s heart rate, sleep duration, and steps taken. Or the user might specify a goal of ‘help me learn  Spanish’ and the reward function could return the user’s Spanish exam results.  
-->
</p>
  <p>  
さらに、ユーザーは学習プロセス中に満足度などのフィードバックを提供し、それを用いて報酬関数を微調整することができます。報酬関数は時間の経過とともに適応し、信号の選択や組み合わせの方法を改善し、不整合を特定して修正することができます。これは、ユーザーからのフィードバックを最上位の目標として最適化し、環境からのグラウンデッドシグナルを下位の目標として最適化する、二階層の最適化プロセスとして理解することもできます。<sup>4</sup>
このように、少量の人間のデータから大量の自律学習を促進できる可能性があります。
<!--
Furthermore, users could provide feedback during the learning process, such as their satisfaction level,  which could be used to fine-tune the reward function. The reward function can then adapt over time, to  improve the way in which it selects or combines signals, and to identify and correct any misalignment. This  can also be understood as a bi-level optimisation process that optimises user feedback as the top-level goal,  and optimises grounded signals from the environment at the low level.<sup>4</sup>
 In this way, a small amount of human  data may facilitate a large amount of autonomous learning.  
-->
</p>
  <p class="margin-large">  <sup>4</sup>
この場合、根拠のある人間のフィードバックは、エージェントの全体的な目的を形成する単一の報酬関数と見なすこともできます。これは、豊富で根拠のあるフィードバックに基づいて内在的報酬関数 [8] を構築および最適化することによって最大化されます。
<!--
 In this case, one may also view grounded human feedback as a singular reward function forming the agent’s overall objective, which  is maximised by constructing and optimising an intrinsic reward function [8] based on rich, grounded feedback.  
-->
</p>

  <h2>計画と推論</h2>

  <p>  
経験の時代は、エージェントの計画と推論の方法を変えるのでしょうか？近年、言語で「考える」、つまり推論できるLLM（言語言語モデル）[23, 14, 10]を用いて大きな進歩が遂げられています。LLMは、思考の連鎖を辿ってから応答を出力することで、推論を行います[16]。概念的には、LLMは汎用コンピュータ[30]として機能することができます。LLMは自身のコンテキストにトークンを追加することで、最終結果を出力する前に任意のアルゴリズムを実行することができます。
<!--
Will the era of experience change the way that agents plan and reason? Recently, there has been significant  progress using LLMs that can reason, or “think” with language [23, 14, 10], by following a chain of thought  before outputting a response [16]. Conceptually, LLMs can act as a universal computer [30]: an LLM can  append tokens into its own context, allowing it to execute arbitrary algorithms before outputting a final result.  
-->
</p>
  <p>  
ヒューマンデータの時代において、これらの推論手法は人間の思考プロセスを模倣するように明確に設計されています。例えば、LLMは人間のような思考の連鎖を出力したり[16]、人間の思考の痕跡を模倣したり[42]、人間の例に一致する思考ステップを強化したりするように促されています[18]。推論プロセスはさらに微調整され、人間の専門家が決定した正解に一致する思考痕跡を生成することもあります[44]。
<!--
In the era of human data, these reasoning methods have been explicitly designed to imitate human thought  processes. For example, LLMs have been prompted to emit human-like chains of thought [16], imitate traces  of human thinking [42], or to reinforce steps of thinking that match human examples [18]. The reasoning  process may be fine-tuned further to produce thinking traces that match the correct answer, as determined by  human experts [44].  
-->
</p>
  <p>  
しかし、人間の言語が万能コンピュータの最適な例となる可能性は極めて低い。より効率的な思考メカニズムは確かに存在し、例えば記号計算、分散計算、連続計算、微分計算といった非人間言語を用いる。自己学習システムは原理的には、経験から思考方法を学習することで、そのようなアプローチを発見したり、改善したりすることができる。例えば、AlphaProofは人間の数学者とは全く異なる方法で、複雑な定理を形式的に証明することを学習した[20]。
<!--
However, it is highly unlikely that human language provides the optimal instance of a universal computer.  More efficient mechanisms of thought surely exist, using non-human languages that may for example utilise  symbolic, distributed, continuous, or differentiable computations. A self-learning system can in principle  discover or improve such approaches by learning how to think from experience. For example, AlphaProof  learned to formally prove complex theorems in a manner quite different to human mathematicians [20].  
-->
</p>
  <p>  
さらに、汎用コンピュータの原理はエージェントの内部計算のみを対象としており、それを外界の現実と結びつけるものではありません。人間の思考を模倣したり、人間の専門家の回答に匹敵するように訓練されたエージェントは、誤った仮定や固有のバイアスなど、データに深く埋め込まれた誤った思考方法を受け継いでいる可能性があります。例えば、あるエージェントが5,000年前の人間の思考と専門家の回答を用いて推論するように訓練されていたとしたら、物理的な問題についてアニミズムの観点から推論していたかもしれません。1,000年前は有神論の観点から推論していたかもしれません。300年前はニュートン力学の観点から推論していたかもしれません。そして50年前は量子力学の観点から推論していたかもしれません。それぞれの思考方法を超えるには、現実世界との相互作用、つまり仮説を立て、実験を実行し、結果を観察し、それに応じて原理を更新することが必要でした。同様に、エージェントが誤った思考方法を覆すためには、現実世界のデータに基づいていなければなりません。このグラウンディングはフィードバックループを提供し、エージェントは受け継いだ仮定を現実に照らし合わせて検証し、現在の支配的な人間の思考様式にとらわれない新たな原理を発見することを可能にします。このグラウンディングがなければ、エージェントはいかに洗練されたものであっても、既存の人間の知識のエコーチェンバーと化してしまうでしょう。そこから脱却するためには、エージェントは積極的に世界と関わり、観察データを収集し、そのデータを用いて理解を反復的に洗練させる必要があります。これは、人類の科学的進歩を推進してきたプロセスを多くの点で反映しています。
<!--
Furthermore, the principle of a universal computer only addresses the internal computation of the agent;  it does not connect it to the realities of the external world. An agent trained to imitate human thoughts or  even to match human expert answers may inherit fallacious methods of thought deeply embedded within that  data, such as flawed assumptions or inherent biases. For example, if an agent had been trained to reason using  human thoughts and expert answers from 5,000 years ago it may have reasoned about a physical problem in  terms of animism; 1,000 years ago it may have reasoned in theistic terms; 300 years ago it may have reasoned  in terms of Newtonian mechanics; and 50 years ago in terms of quantum mechanics. Progressing beyond  each method of thought required interaction with the real world: making hypotheses, running experiments,  observing results, and updating principles accordingly. Similarly, an agent must be grounded in real-world  data in order to overturn fallacious methods of thought. This grounding provides a feedback loop, allowing  the agent to test its inherited assumptions against reality and discover new principles that are not limited by  current, dominant modes of human thought. Without this grounding, an agent, no matter how sophisticated,  will become an echo chamber of existing human knowledge. To move beyond this, agents must actively  engage with the world, collect observational data, and use that data to iteratively refine their understanding,  mirroring in many ways the process that has driven human scientific progress.  
-->
</p>
  <p>  
思考を外界に直接的に根付かせる一つの方法は、報酬予測を含め、エージェントの行動が世界に与える影響を予測する世界モデル [37] を構築することです。例えば、ヘルスアシスタントは、地元のジムや健康ポッドキャストを推奨することを検討するかもしれません。エージェントの世界モデルは、この行動の後にユーザーの心拍数や睡眠パターンがどのように変化するかを予測するだけでなく、ユーザーとの将来の対話も予測するかもしれません。これにより、エージェントは自身の行動とその世界への因果的影響について直接計画を立てることができます [36, 29]。エージェントが経験の流れの中で世界と相互作用し続けるにつれて、そのダイナミクスモデルは継続的に更新され、予測の誤りを修正します。世界モデルが与えられた場合、エージェントはスケーラブルな計画手法を適用することで、予測されるエージェントのパフォーマンスを向上させることができます。
<!--
One possible way to directly ground thinking in the external world is to build a world model [37] that  predicts the consequences of the agent’s actions upon the world, including predicting reward. For example,  a health assistant might consider making a recommendation for a local gym or a health podcast. The agent’s  world model might predict how a user’s heart rate or sleep patterns might subsequently change following this  action, as well as predicting future dialogue with the user. This allows the agent to plan [36, 29] directly in  terms of its own actions and their causal effect upon the world. As the agent continues to interact with the  world throughout its stream of experience, its dynamics model is continually updated to correct any errors  in its predictions. Given a world model, an agent may apply scalable planning methods that improve the  predicted performance of the agent.  
-->
</p>
  <p>  
計画と推論の方法は相互に排他的ではありません。エージェントは内部 LLM 計算を適用して、計画中に各アクションを選択したり、それらのアクションの結果をシミュレートして評価したりすることができます。
<!--
Planning and reasoning methods are not mutually exclusive: an agent may apply internal LLM computations  to select each action during planning, or to simulate and evaluate the consequences of those actions.  
-->
</p>
  
<h2>なぜ今なのか?</h2>

<p>  
経験からの学習は新しいことではありません。強化学習システムはこれまで、明確な報酬信号を持つシミュレータで表現された多数の複雑なタスクを習得してきました（図1の「シミュレーションの時代」と概ね同じ）。たとえば、RL手法は、バックギャモン[39]、囲碁[31]、チェス[32]、ポーカー[22, 6]、ストラテゴ[26]などのボードゲーム、Atari[21]、StarCraft II[40]、Dota 2[4]、グランツーリスモ[41]などのビデオゲーム、ルービックキューブ[1]などの器用な操作タスク、データセンターの冷却[13]などのリソース管理タスクでのセルフプレイを通じて、人間のパフォーマンスに匹敵または上回りました。さらに、AlphaZero[33]などの強力なRLエージェントは、ニューラルネットワークのサイズ、対話型経験の量、および思考時間の長さに関して、印象的で潜在的に無制限のスケーラビリティを示しました。しかし、このパラダイムに基づくエージェントは、シミュレーション (単一かつ正確に定義された報酬を伴う閉じた問題) と現実 (複数の、一見定義が曖昧な報酬を伴うオープンエンドの問題) の間のギャップを飛び越えることはできませんでした。
<!--
Learning from experience is not new. Reinforcement learning systems have previously mastered a large  number of complex tasks that were represented in a simulator with a clear reward signal (c.f., approximately,  the “era of simulation” in Figure 1). For example, RL methods equalled or exceeded human performance through self-play in board games such as backgammon [39], Go [31], chess [32], poker [22, 6] and Stratego  [26]; video games such as Atari [21], StarCraft II [40], Dota 2 [4] and Gran Turismo [41]; dextrous  manipulation tasks such as Rubik’s cube [1]; and resource management tasks such as data center cooling  [13]. Furthermore, powerful RL agents such as AlphaZero [33] exhibited impressive and potentially unlimited  scalability with the size of the neural network, the quantity of interactive experience, and the duration of  thinking time. However, agents based on this paradigm did not leap the gap between simulation (closed problems  with singular, precisely defined rewards) to reality (open-ended problems with a plurality of seemingly  ill-defined rewards).  
-->
</p>
<center><img src="images/fig(jp).png"></center>
<p class="margin-large">
図1：主要なAIパラダイムの概略的な年表。Y軸は、その分野における総労力と計算のうち、強化学習（RL）に重点が置かれている割合を示しています。
<!--
Figure 1: A sketch chronology of dominant AI paradigms. The y-axis suggests the proportion of the field’s total effort and computation that is focused on RL.
-->
</p>
  <p>  
ヒューマンデータの時代は魅力的な解決策をもたらしました。膨大なヒューマンデータのコーパスには、多種多様なタスクに対応する自然言語の例が含まれています。このデータで訓練されたエージェントは、シミュレーション時代の限定的な成功と比較して、幅広い能力を達成しました。その結果、経験に基づく強化学習の手法は、より汎用的なエージェントに取って代わられ、人間中心のAIへの広範な移行が起こりました。
<!--
The era of human data offered an appealing solution. Massive corpuses of human data contain examples  of natural language for a huge diversity of tasks. Agents trained on this data achieved a wide range of competencies  compared to the more narrow successes of the era of simulation. Consequently, the methodology  of experiential RL was largely discarded in favour of more general-purpose agents, resulting in a widespread  transition to human-centric AI.  
-->
</p>
  <p>  
しかし、この移行で失われたものがあります。それは、エージェントが自分の知識を自己発見する能力です。たとえば、AlphaZero はチェスと囲碁の根本的に新しい戦略を発見し、人間がこれらのゲームをプレイする方法を変えました [28, 45]。経験の時代は、この能力と人間のデータの時代に達成されたタスクの一般性のレベルを調和させます。これは、上で概説したように、エージェントが現実世界の経験の流れの中で自律的に行​​動し、観察できるようになり [11]、報酬が豊富な根拠のある現実世界の信号のいずれかに柔軟に接続できるようになったときに可能になります。複雑な現実世界の行動空間と対話する自律エージェントの出現 [3, 15, 24] と、豊富な推論空間でオープンエンドの問題を解決できる強力な RL 手法の出現 [20, 10] は、経験の時代への移行が差し迫っていることを示唆しています。
<!--
However, something was lost in this transition: an agent’s ability to self-discover its own knowledge.  For example, AlphaZero discovered fundamentally new strategies for chess and Go, changing the way that  humans play these games [28, 45]. The era of experience will reconcile this ability with the level of taskgenerality  achieved in the era of human data. This will become possible, as outlined above, when agents are  able to autonomously act and observe in streams of real-world experience [11], and where the rewards may  be flexibly connected to any of an abundance of grounded, real-world signals. The advent of autonomous  agents that interact with complex, real-world action spaces [3, 15, 24], alongside powerful RL methods that  can solve open-ended problems in rich reasoning spaces [20, 10] suggests that the transition to the era of  experience is imminent.  
-->
</p>
  
<h2>強化学習(RL)手法</h2>

 <p>  
強化学習は、エージェントが環境と直接相互作用することで自ら学習する自律学習に深く根ざした豊かな歴史を持っています。初期のRL研究では、一連の強力な概念とアルゴリズムが生み出されました。たとえば、時間差分学習[35]は、エージェントが将来の報酬を予測することを可能にし、バックギャモンにおける超人的なパフォーマンス[39]などのブレークスルーにつながりました。楽観主義や好奇心によって駆動される探索技術は、エージェントが創造的な新しい行動を発見し、次善のルーチンに陥るのを避けるために開発されました[2]。Dynaアルゴリズムなどの手法により、エージェントは世界のモデルを構築して学習し、将来の行動を計画して推論できるようになりました[36, 29]。オプション学習やオプション間/オプション内学習などの概念は、時間的抽象化を促進し、エージェントがより長い時間スケールで推論し、複雑なタスクを管理可能なサブ目標に分解することを可能にしています[38]。
<!--
Reinforcement learning (RL) has a rich history that is deeply rooted in autonomous learning, where agents  learn for themselves through direct interaction with their environment. Early RL research yielded a suite of  powerful concepts and algorithms. For example, temporal difference learning [35] enabled agents to estimate  future rewards, leading to breakthroughs such as superhuman performance in backgammon [39]. Exploration  techniques, driven by optimism or curiosity, were developed to help agents discover creative new behaviors  and avoid getting stuck in suboptimal routines [2]. Methods like the Dyna algorithm enabled agents to  build and learn from models of their world, allowing them to plan and reason about future actions [36,  29]. Concepts like options and inter/intra>
-option learning facilitated temporal abstraction, enabling agents to  reason over longer timescales and break down complex tasks into manageable sub-goals [38].  
-->
</p>
  <p>  
しかし、人間中心のLLMの台頭により、焦点は自律学習から人間の知識の活用へと移行しました。RLHF（人間からのフィードバックによる強化学習）[9, 25]や言語モデルを人間の推論と整合させる手法[44]などの手法は非常に効果的であることが証明され、AI能力の急速な進歩を促しました。これらのアプローチは強力ではありましたが、しばしば強化学習の中核概念を迂回していました。RLHFは、機械推定値の代わりに人間の専門家を呼び出すことで価値関数の必要性を回避し、人間のデータから得られる強力な事前確率は探索への依存を減らし、人間中心の推論は世界モデルと時間的抽象化の必要性を軽減しました。
<!--
The rise of human-centric LLMs, however, shifted the focus away from autonomous learning and towards  leveraging human knowledge. Techniques like RLHF (Reinforcement Learning from Human Feedback) [9,  25] and methods for aligning language models with human reasoning [44] proved incredibly effective, driving  rapid progress in AI capabilities. These approaches, while powerful, often bypassed core RL concepts: RLHF  side-stepped the need for value functions by invoking human experts in place of machine-estimated values,  strong priors from human data reduced the reliance on exploration, and reasoning in human-centric terms  lessened the need for world models and temporal abstraction.  
-->
</p>
  <p>  
しかし、パラダイムシフトは、大切なものを失ってしまったと言えるかもしれません。人間中心の強化学習は、かつてないほど幅広い行動を可能にしましたが、同時にエージェントのパフォーマンスに新たな限界を設けてしまいました。エージェントは既存の人間の知識を超えることはできません。さらに、人間データの時代は、主に、根拠のない人間との短いインタラクションを想定した強化学習手法に焦点を当てており、根拠のある自律的なインタラクションの長いストリームには適していません。
<!--
However, it could be argued that the shift in paradigm has thrown out the baby with the bathwater. While  human-centric RL has enabled an unprecedented breadth of behaviours, it has also imposed a new ceiling  on the agent’s performance: agents cannot go beyond existing human knowledge. Furthermore, the era of  human data has focused predominantly on RL methods that are designed for short episodes of ungrounded,  human interaction, and are not suitable for long streams of grounded, autonomous interaction.  
-->
</p>
  <p>  
経験の時代は、古典的な強化学習の概念を再考し、改善する機会をもたらします。この時代は、観測データに柔軟に根ざした報酬関数についての新たな考え方をもたらします。価値関数と、まだ不完全なシーケンスを持つ長いストリームからそれらを推定する方法を再考するでしょう。人間の事前学習とは根本的に異なる新しい行動を発見するための、原理的でありながら実用的な現実世界の探索方法をもたらします。グラウンデッドインタラクションの複雑さを捉える、世界モデルへの新しいアプローチが開発されるでしょう。時間的抽象化のための新しい手法により、エージェントは経験に基づいて、これまで以上に長い時間範囲で推論できるようになります。強化学習の基礎を基盤とし、その中核となる原理をこの新しい時代の課題に適応させることで、自律学習の可能性を最大限に引き出し、真に超人的な知能への道を切り開くことができます。
<!--
The era of experience presents an opportunity to revisit and improve classic RL concepts. This era will  bring new ways to think about reward functions that are flexibly grounded in observational data. It will  revisit value functions and methods to estimate them from long streams with as yet incomplete sequences. It  will bring principled yet practical methods for real-world exploration that discover new behaviours that are  radically different from human priors. Novel approaches to world models will be developed that capture the  complexities of grounded interactions. New methods for temporal abstraction will allow agents to reason, in  terms of experience, over ever-longer time horizons. By building upon the foundations of RL and adapting  its core principles to the challenges of this new era, we can unlock the full potential of autonomous learning  and pave the way to truly superhuman intelligence.  
-->
</p>

<h2>結果</h2>

<p>  
AIエージェントが世界との相互作用から学習する「経験の時代」の到来は、これまでとは大きく異なる未来を約束します。この新しいパラダイムは、計り知れない可能性を秘めている一方で、以下の点を含む、慎重な検討を要する重要なリスクと課題も伴います。
<!--
The advent of the era of experience, where AI agents learn from their interactions with the world, promises a  future profoundly different from anything we have seen before. This new paradigm, while offering immense  potential, also presents important risks and challenges that demand careful consideration, including but not  limited to the following points.  
-->
</p>
  <p>  
ポジティブな面としては、経験学習は前例のない能力を解き放つでしょう。日常生活において、パーソナライズされたアシスタントは、継続的な経験のストリームを活用し、個人の健康、教育、または職業上のニーズに適応し、数か月から数年にわたる長期的な目標達成へと導きます。おそらく最も大きな変革をもたらすのは、科学的発見の加速でしょう。AIエージェントは、材料科学、医学、ハードウェア設計などの分野で、自律的に実験を設計・実施するようになります。これらのエージェントは、自身の実験結果から継続的に学習することで、新たな知識のフロンティアを迅速に探求し、前例のないペースで新しい材料、医薬品、技術の開発につながる可能性があります。
<!--
On the positive side, experiential learning will unlock unprecedented capabilities. In everyday life, personalized  assistants will leverage continuous streams of experience to adapt to individuals’ health, educational,  or professional needs towards long-term goals over the course of months or years. Perhaps most  transformative will be the acceleration of scientific discovery. AI agents will autonomously design and conduct  experiments in fields like materials science, medicine, or hardware design. By continuously learning  from the results of their own experiments, these agents could rapidly explore new frontiers of knowledge,  leading to the development of novel materials, drugs, and technologies at an unprecedented pace.  
-->
</p>
  <p>  
しかし、この新しい時代は重大かつ新たな課題ももたらします。人間の能力の自動化は生産性の向上を約束する一方で、こうした向上によって雇用が奪われる可能性も秘めています。エージェントは、長期的な問題解決、イノベーション、現実世界への影響に対する深い理解など、これまで人間だけが持つと考えられていた能力を発揮できるようになるかもしれません。
<!--
However, this new era also presents significant and novel challenges. While the automation of human capabilities promises to boost productivity, these improvements could also lead to job displacement. Agents  may even be able to exhibit capabilities previously considered the exclusive realm of humanity, such as longterm  problem-solving, innovation, and a deep understanding of real world consequences.  
-->
</p>
  <p>  
さらに、AIの誤用の可能性については一般的な懸念が存在するものの、長期的な目標を達成するために長期間にわたり自律的に世界と相互作用するエージェントからは、より高いリスクが生じる可能性があります。デフォルトでは、人間がエージェントの行動に介入して仲介する機会が少なくなるため、高い信頼と責任が求められます。人間のデータや人間の思考様式から離れていくことで、将来のAIシステムの解釈が困難になる可能性もあります。
<!--
Furthermore, whilst general concerns exist around the potential misuse of any AI, heightened risks may  arise from agents that can autonomously interact with the world over extended periods of time to achieve  long-term goals. By default, this provides fewer opportunities for humans to intervene and mediate the  agent’s actions, and therefore requires a high bar of trust and responsibility. Moving away from human data  and human modes of thinking may also make future AI systems harder to interpret.  
-->
</p>
  <p>  
しかし、体験学習によって特定の安全上のリスクが増大し、体験の時代への安全な移行を確実にするためにさらなる研究が必要であることを認めつつも、体験学習がいくつかの重要な安全上の利点をもたらす可能性もあることを認識する必要があります。
<!--
However, whilst acknowledging that experiential learning will increase certain safety risks, and that further  research is surely required to ensure a safe transition into the era of experience, we should also recognise  that it may also provide some important safety benefits.  
-->
</p>
  <p>  
まず、経験的エージェントは自身が置かれている環境を認識しており、その行動は時間の経過とともにその環境の変化に適応することができます。固定型AIシステムを含む、事前にプログラムされたシステムは、環境のコンテキストを認識できず、展開された変化する世界に適応できなくなる可能性があります。たとえば、重要なハードウェアが故障したり、パンデミックが急速な社会変化を引き起こしたり、新しい科学的発見が急速な技術開発の連鎖を引き起こしたりする可能性があります。対照的に、経験的エージェントは、故障したハードウェアを回避したり、急速な社会変化に適応したり、新しい科学技術を受け入れて構築したりする方法を観察し、学習することができます。おそらくさらに重要なのは、エージェントは自分の行動が人間の懸念、不満、または苦痛を引き起こしていることを認識でき、これらの悪影響を回避するために行動を適応的に修正できることです。
<!--
Firstly, an experiential agent is aware of the environment it is situated within, and its behaviour can adapt  over time to changes in that environment. Any pre-programmed system, including a fixed AI system, can  be unaware of its environmental context, and become maladapted to the changing world into which it is  deployed. For example, a critical piece of hardware may malfunction, a pandemic might cause rapid societal  change, or a new scientific discovery may trigger a cascade of rapid technological developments. By contrast,  an experiential agent could observe and learn to circumvent malfunctioning hardware, adjust to rapid societal  change, or embrace and build upon new science and technology. Perhaps even more importantly, the agent  could recognise when its behaviour is triggering human concern, dissatisfaction, or distress, and adaptively  modify its behaviour to avoid these negative consequences.  
-->
</p>
  <p>  
第二に、エージェントの報酬関数自体も経験を通じて適応される可能性があり、例えば前述の二段階最適化（報酬の項参照）が用いられる。重要なのは、これが、ずれた報酬関数を試行錯誤によって時間の経過とともに段階的に修正できることを意味する点である。例えば、ペーパークリップの最大化 [5] などの信号を盲目的に最適化するのではなく、ペーパークリップの生産が地球上の資源をすべて消費する前に、人間の関心の兆候に基づいて報酬関数を修正することができる。これは、人間が互いに目標を設定し、システムを悪用したり、長期的な幸福を無視したり、望ましくない悪影響を引き起こしたりする人々を観察した場合に、その目標を適応させる方法に似ている。ただし、人間の目標設定と同様に、完全に一致する保証はない。
<!--
Secondly, the agent’s reward function may itself be adapted through experience, for example using the bilevel  optimisation described earlier (see Rewards). Importantly, this means that misaligned reward functions  can often be incrementally corrected over time by trial and error. For example, rather than blindly optimising  a signal, such as the maximisation of paperclips [5], the reward function could be modified, based upon  indications of human concern, before paperclip production consumes all of the Earth’s resources. This is  analogous to the way that humans set goals for each other, and then adapt those goals if they observe people  gaming the system, neglecting long-term well-being, or causing undesired negative consequences; although  also like human goal-setting, there is no guarantee of perfect alignment.  
-->
</p>
  <p>  
最後に、物理的な経験に依存する進歩は、現実世界で行動を実行し、その結果を観察するのにかかる時間によって本質的に制約されます。例えば、新薬の開発は、AI支援による設計であっても、依然として現実世界での試験を必要とし、一夜にして完了することはできません。これは、AIの潜在的な自己改善のペースに自然なブレーキをかける可能性があります。
<!--
Finally, advancements relying on physical experience are inherently constrained by the time it takes to  execute actions in the real world and observe their consequences. For example, the development of a new  drug, even with AI-assisted design, still requires real-world trials that cannot be completed overnight. This  may provide a natural brake on the pace of potential AI self-improvement.  
-->
</p>
  
<h2>結論</h2>

 <p>  
経験の時代は、AIの進化における極めて重要な転換点となるでしょう。今日の強固な基盤を基盤としつつ、人間由来のデータの限界を超え、エージェントは世界との相互作用からますます学習するようになります。エージェントは豊富な観察と行動を通じて、自律的に環境と相互作用します。そして、生涯にわたる経験の流れの中で適応を続け、その目標は、根拠のある信号のあらゆる組み合わせに向けられるようになります。さらに、エージェントは強力な非人間的推論を活用し、エージェントの行動が環境に及ぼす結果に基づいて計画を立てます。最終的には、経験データは人間が生成するデータの規模と品質を凌駕するでしょう。このパラダイムシフトは、強化学習（RL）におけるアルゴリズムの進歩を伴い、多くの領域において、人間の能力を凌駕する新たな能力を解き放つでしょう。
<!--
The era of experience marks a pivotal moment in the evolution of AI. Building on today’s strong foundations,  but moving beyond the limitations of human-derived data, agents will increasingly learn from their own  interactions with the world. Agents will autonomously interact with environments through rich observations  and actions. They will continue to adapt over the course of lifelong streams of experience. Their goals  will be directable towards any combination of grounded signals. Furthermore, agents will utilise powerful  non-human reasoning, and construct plans that are grounded in the consequences of the agent’s actions upon  its environment. Ultimately, experiential data will eclipse the scale and quality of human generated data.  This paradigm shift, accompanied by algorithmic advancements in RL, will unlock in many domains new  capabilities that surpass those possessed by any human.  
-->
</p>
  
<h2>謝辞</h2>

<p>  
著者は Thomas Degris、Rohin Shah、Tom Schaul、Hado van Hasselt からの有益なコメントと議論に感謝の意を表します。
<!--
The authors would like to acknowledge helpful comments and discussion from Thomas Degris, Rohin Shah,  Tom Schaul and Hado van Hasselt.  
-->
</p>
<h2>参考文献</h2>
<p>
<div class="styleRef">
<ul>
<li>[1] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell,
R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang. Solving
Rubik’s cube with a robot hand, 2019.
</li><br><li>[2] S. Amin, M. Gomrokchi, H. Satija, H. van Hoof, and D. Precup. A survey of exploration methods in reinforcement
learning, 2021.
</li><br><li>[3] Anthropic. Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku. https://www.
anthropic.com/news/3-5-models-and-computer-use, 2024.
</li><br><li>[4] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse,
R. J´ozefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. d. O. Pinto, J. Raiman, T. Salimans, J. Schlatter,
J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang. Dota 2 with large scale deep reinforcement
learning, 2019.
</li><br><li>[5] N. Bostrom. Ethical issues in advanced artificial intelligence. https://nickbostrom.com/ethics/ai,
2003.
</li><br><li>[6] N. Brown and T. Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science,
359(6374):418–424, 2018.
</li><br><li>[7] X. Chen, M. Lin, N. Sch¨arli, and D. Zhou. Teaching large language models to self-debug, 2023.
</li><br><li>[8] N. Chentanez, A. Barto, and S. Singh. Intrinsically motivated reinforcement learning. In L. Saul, Y. Weiss, and
L. Bottou, editors, Advances in Neural Information Processing Systems, volume 17. MIT Press, 2004.
</li><br><li>[9] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human
preferences. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
</li><br><li>[10] DeepSeek AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint
arXiv:2501.12948, 2025.
</li><br><li>[11] M. Elsayed, G. Vasan, and A. R. Mahmood. Streaming deep reinforcement learning finally works, 2024.
</li><br><li>[12] J. Gehring, K. Zheng, J. Copet, V. Mella, Q. Carbonneaux, T. Cohen, and G. Synnaeve. Rlef: Grounding code llms
in execution feedback with reinforcement learning, 2025.
</li><br><li>[13] Google DeepMind. Deepmind AI reduces google data centre cooling
bill by 40%. https://deepmind.google/discover/blog/
deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/, 2016.
</li><br><li>[14] Google DeepMind. Gemini: Flash thinking. https://deepmind.google/technologies/gemini/
flash-thinking/, 2024.
</li><br><li>[15] Google DeepMind. Project Mariner. https://deepmind.google/technologies/
project-mariner, 2024.
</li><br><li>[16] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information
Processing Systems, volume 35, pages 22199–22213. Curran Associates, Inc., 2022.
</li><br><li>[17] H. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. CodeRL: Mastering code generation through
pretrained models and deep reinforcement learning, 2022.
</li><br><li>[18] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and
K. Cobbe. Let’s verify step by step, 2023.
</li><br><li>[19] H. Mahdavi, A. Hashemi, M. Daliri, P. Mohammadipour, A. Farhadi, S. Malek, Y. Yazdanifard, A. Khasahmadi,
and V. Honavar. Brains vs. bytes: Evaluating llm proficiency in olympiad mathematics, 2025.
</li><br><li>[20] H. Masoom, A. Huang, M. Z. Horv´ath, T. Zahavy, V. Veeriah, E. Wieser, J. Yung, L. Yu, Y. Schroecker, J. Schrittwieser,
O. Bertolli, B. Ibarz, E. Lockhart, E. Hughes, M. Rowland, G. Margand, A. Davies, D. Zheng, I. Beloshapka,
I. von Glehn, Y. Li, F. Pedregosa, A. Velingker, G. ˇ Zuˇzi´c, O. Nash, B. Mehta, P. Lezeau, S. Mercuri,
L. Wu, C. Soenne, T. Murrills, L. Massacci, A. Yang, A. Mandhane, T. Eccles, E. Ayg¨un, Z. Gong, R. Evans,
S. Mokr´a, A. Barekatain, W. Shang, H. Openshaw, F. Gimeno, D. Silver, and P. Kohli. AI achieves silver-medal
standard solving International Mathematical Olympiad problems. https://deepmind.google/discover/
blog/ai-solves-imo-problems-at-silver-medal-level/, 2024.
</li><br><li>[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.
Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,
S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533,
2015.
</li><br><li>[22] M. Moravˇc´ık, M. Schmid, N. Burch, V. Lis`y, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, and M. Bowling.
Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508–513, 2017.
</li><br><li>[23] OpenAI. Openai o1 mini: Advancing cost-efficient reasoning. https://openai.com/index/
openai-o1-mini-advancing-cost-efficient-reasoning/, 2024.
</li><br><li>[24] OpenAI. Introducing Operator. https://openai.com/index/introducing-operator, 2025.
</li><br><li>[25] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,
J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P.Welinder, P. Christiano, J. Leike, and R. Lowe.
Training language models to follow instructions with human feedback, 2022.
</li><br><li>[26] J. Perolat, B. D. Vylder, D. Hennes, E. Tarassov, F. Strub, V. de Boer, P. Muller, J. T. Connor, N. Burch, T. Anthony,
S. McAleer, R. Elie, S. H. Cen, Z. Wang, A. Gruslys, A. Malysheva, M. Khan, S. Ozair, F. Timbers, T. Pohlen,
T. Eccles, M. Rowland, M. Lanctot, J.-B. Lespiau, B. Piot, S. Omidshafiei, E. Lockhart, L. Sifre, N. Beauguerlange,
R. Munos, D. Silver, S. Singh, D. Hassabis, and K. Tuyls. Mastering the game of Stratego with model-free
multiagent reinforcement learning. Science, 378(6623):990–996, 2022.
</li><br><li>[27] I. Petrov, J. Dekoninck, L. Baltadzhiev, M. Drencheva, K. Minchev, M. Balunovi´c, N. Jovanovi´c, and M. Vechev.
Proof or bluff? evaluating llms on 2025 usa math olympiad, 2025.
</li><br><li>[28] M. Sadler and N. Regan. Game Changer. New in Chess, 2019.
</li><br><li>[29] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis,
T. Graepel, T. P. Lillicrap, and D. Silver. Mastering Atari, Go, chess and shogi by planning with a learned model.
Nature, 588:604 – 609, 2019.
</li><br><li>[30] D. Schurmanns. Memory augmented large language models are computationally universal. arXiv preprint
arXiv:2501.12948, 2023.
</li><br><li>[31] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap,
M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and
tree search. Nature, 529(7587):484–489, 2016.
</li><br><li>[32] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel,
T. Lillicrap, K. Simonyan, and D. Hassabis. A general reinforcement learning algorithm that masters chess, shogi,
and Go through self-play. Science, 362(6419):1140–1144, 2018.
</li><br><li>[33] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton,
Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Grapel, and D. Hassabis. Mastering the game of go
without human knowledge. Nature, 550(7676):354–359, 2017.
</li><br><li>[34] D. Silver, S. Singh, D. Precup, and R. S. Sutton. Reward is enough. Artificial Intelligence, 299:103535, 2021.
</li><br><li>[35] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9–44, 1988.
</li><br><li>[36] R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming.
In Proceedings of the Seventh International Conference on Machine Learning, pages 216–224. Morgan
Kaufmann, 1990.
</li><br><li>[37] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018.
</li><br><li>[38] R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in
reinforcement learning. Artificial Intelligence, 112(1-2):181–211, 1999.
</li><br><li>[39] G. Tesauro. TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation,
6(2):215–219, 1994.
</li><br><li>[40] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. Choi, R. Powell, T. Ewalds,
P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai, J. P. Agapiou, M. Jaderberg,
A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre,
Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama, D. W¨unsch, K. McKinney, O. Smith, T. Schaul, T. P. Lillicrap,
K. Kavukcuoglu, D. Hassabis, C. Apps, and D. Silver. Grandmaster level in StarCraft II using multi-agent reinforcement
learning. Nature, 575:350 – 354, 2019.
</li><br><li>[41] P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J. Walsh, R. Capobianco, A. Devlic,
F. Eckert, F. Fuchs, L. Gilpin, P. Khandelwal, V. Kompella, H. Lin, P. MacAlpine, D. Oller, T. Seno, C. Sherstan,
M. D. Thomure, H. Aghabozorgi, L. Barrett, R. Douglas, D. Whitehead, P. D¨urr, P. Stone, M. Spranger, and H. Kitano.
Outracing champion Gran Turismo drivers with deep reinforcement learning. Nature, 602(7896):223–228,
2022.
</li><br><li>[42] M. S. Yang, D. Schuurmans, P. Abbeel, and O. Nachum. Chain of thought imitation with procedure cloning. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information
Processing Systems, volume 35, pages 36366–36381. Curran Associates, Inc., 2022.
</li><br><li>[43] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in
large language models. In 11th International Conference on Learning Representations, 2023.
</li><br><li>[44] E. Zelikman, J. M. Mu, N. D. Goodman, and G. Poesia. Star: Bootstrapping reasoning with reasoning. Advances
in Neural Information Processing Systems, 35:24170–24184, 2022.
</li><br><li>[45] Y. Zhou. Rethinking Opening Strategy: AlphaGo’s Impact on Pro Play. CreateSpace Independent, 2018.
11</li>
</ul>
</div>
</p>
    </body>
</html>