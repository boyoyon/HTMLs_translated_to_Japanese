<html lang="ja">
    <head>
        <meta charset="utf-8" />
        <title>HTMLs translated to Japansese</title>
    </head>
    <body>
        <h1><center>日本語に翻訳されたHTML</center></h1>
        <h2>なにものか？</h2>
        <p>
            <a href="https://boyoyon.github.io/HTMLs_translated_to_Japanese/">公開されているPDFを日本語に翻訳してみた。</a>
        </p>
        <h2>その他</h2>
        <p>
            AIが要約してくれるし、言語を選択すれば日本語で読める･･･<br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2507.11906v1">CoCre-Sam (コックリさん): ウイジャ盤の融合言語モデルからの集団ランジュバン動力学サンプリングとしてのモデル化</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.22084v1">トランスフォーマーはグラフニューラルネットワークである</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.21521v1">大規模言語モデルにおける「ポチョムキン的理解」(見せかけの理解)</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2408.10234v2">存在の耐え難い遅さ：なぜ私たちは10ビット/秒で生きるのか？</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2503.20680v1">ビジョンとしてのLoRA</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.21551v1">LLM事前学習におけるグロッキングはどこで見つかるか？：テストなしで記憶から汎化への監視</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.18254v1">RLPR: ベリファイアなしでRLVRを汎用ドメインに外挿する</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.01622v2">汎用エージェントは世界モデルを必要とする</a><br>
            ・<a href="https://www.alphaxiv.org/ja/overview/1803.10122v4">世界モデル</a>　2018年<br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.17218v1">機械心像：潜在視覚トークンを活用したマルチモーダル推論の強化</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2504.07960">VisualCloze: ビジュアルインコンテキスト学習によるユニバーサル画像生成フレームワーク</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/1707.06347 ">近接方策最適化アルゴリズム：強化学習に革命をもたらす</a>　2017年<br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.01716">自己挑戦型言語モデルエージェント</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2505.22954">Darwin Gödel Machine：コーディングのための自己改善型AIシステム</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2503.22430">MVSAnywhere: ゼロショット多視点ステレオ</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.09027">拡散と分散：表現正則化による画像生成</a> Kaiming Heさんの論文<br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2506.07527">ReLIFT: LLM推論のための交互オンラインファインチューニング</a><br><br>
            <a href="https://www.alphaxiv.org/ja/overview/2412.10209">GAF：マルチビュー拡散による単眼ビデオからのガウスアバター再構築</a><br><br>
        <a href="https://www.alphaxiv.org/ja/overview/2406.09756">MASt3Rによる3D画像マッチングの基礎固め：包括的な概要</a><br>
            画像マッチングはコンピュータビジョンにおいて長い歴史を持ち、SIFTやSURFのような手作業で作成された特徴記述子から、SuperGlueやLoFTRのような深層学習アプローチへと進化してきました。
<br><br>
MASt3Rの動機は、画像マッチング問題が本質的に3次元であるという観察に由来しています。2つの画像が異なる視点から同じシーンを捉えるとき、ピクセルはシーン内の同じ3D点を投影しているため、互いに対応します。この3D理解を明示的に組み込むことで、MASt3Rは純粋な2Dアプローチの限界を克服することを目指しています。
            <br><br> 
        <a href="https://www.alphaxiv.org/ja/overview/2506.02878">CoTは真の推論ではなく、模倣するための厳しい制約に過ぎない：理論的な観点<a><br><br>
        <a href="https://www.alphaxiv.org/ja/overview/2505.17117">トークンから思考へ：LLMと人間が圧縮を意味と交換する方法</a><br>
            LLMは、人間の認知と同様の方法で言語を真に理解しているのでしょうか？それとも、その印象的な能力は、高度な統計的パターン認識に由来するのでしょうか？
            <br><br>
大規模言語モデル（LLM）は、人間と比較してどのように意味を表現するのでしょうか？この根本的な疑問が、情報理論を用いて、LLMが人間の認知と同様の概念構造を発達させるのか、それとも主に統計的なパターンマッチングに依存するのかを調査する新しい研究の原動力となっています。
<br><br>
LLMと人間の両方が概念を形成する際に、情報圧縮と意味的忠実さの間のトレードオフをどのようにバランスさせているかを検証します。
            <br><br>
        <a href="https://www.alphaxiv.org/ja/overview/2506.06941">思考の錯覚：制御可能なパズル環境を通じて大規模推論モデルを理解する</a><br>
            現在の推論アプローチは、真の記号操作よりもパターンマッチングに大きく依存しているように見えます。<br><br>
            　<a href="https://www.alphaxiv.org/ja/overview/2506.09250">思考の錯覚の錯覚：AI推論評価の批判的分析</a><br>
            　「思考の錯覚」で示された「精度崩壊」は、根本的な推論の限界ではなく、主に実験的なアーティファクトに起因することが明らかになりました。<br><br>
        <a href="https://www.alphaxiv.org/overview/2505.24832">言語モデルはどの程度記憶しているか？</a><br>
            興味深いことに、高精度（float32対bfloat16）を使用しても容量はパラメータあたりわずか3.83ビットにしか増加せず、追加のビットのほとんどが生の情報を保存するために使用されていないことを示唆しています。
<br><br>
この移行は、機械学習における「二重降下」現象を説明します。著者らは、テスト損失がデータセットサイズがモデル容量を超えるまさにその時に改善し始めることを示しています。モデルが個々のデータポイントを個別に記憶できなくなると、それらは例間の共通のパターンを見つけることを余儀なくされ、より良い汎化につながります。
<br><br>
情報理論的フレームワークは、二重降下やグロッキグのような以前は別々だった現象をモデル容量の概念のもとに統一し、深層学習のダイナミクスに関する理論的理解を深めています。
<br><br>
        <a href="https://www.alphaxiv.org/ja/overview/1912.02292">深層ダブルディセント：より大きなモデルとより多くのデータが有害になる場合</a><br><br>
        <a href="https://www.alphaxiv.org/ja/overview/2404.18400">LLM-SR: 大規模言語モデルを用いたプログラミングによる科学方程式の発見</a><br><br>
        <a href="https://www.alphaxiv.org/ja/overview/2310.11804">ResoNet: 音響共鳴解析のための物理学に基づくニューラルネットワーク</a><br>
            (PINNs＋強化学習でAI長岡鉄男を作ってくれないか･･･)<br><br>
        <a href="https://www.alphaxiv.org/overview/2505.16938">NovelSeek: エージェントが科学者になるとき ― 仮説から検証までのクローズドループシステムの構築</a><br><br>
        <a href="https://www.alphaxiv.org/ja/overview/2505.11409">ビジュアルプランニング：イメージだけで考えよう</a><br>
            現在のMLLMは、著者が「モダリティギャップ」と呼ぶものに直面しています。これは、豊富な視覚データをテキスト記述に変換する際に発生する情報の損失です。このギャップは、空間的関係、物理的ダイナミクス、または幾何学的特性が不可欠なタスクで特に問題になります。
<br><br>
たとえば、迷路をナビゲートする場合、「3歩前に進み、交差点で左に曲がる」とテキストで説明するよりも、パスを直接視覚化する方が直感的でエラーが発生しにくくなります。テキストによる説明では、視覚的な表現で直ちに明らかになる正確な空間レイアウトまたは距離関係を捉えられない場合があります。
            <br><br>
        <a href="https://www.alphaxiv.org/ja/overview/2505.03981">X-Reasoner: モダリティとドメインをまたがる一般化可能な推論に向けて</a><br><br>
        <a href=""></a><br><br>
        <a href=""></a><br><br>
        <a href=""></a><br><br>
        </p>
    </body>
</html>
