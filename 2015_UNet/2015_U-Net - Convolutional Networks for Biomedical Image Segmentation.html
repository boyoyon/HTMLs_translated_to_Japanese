<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>U-Net</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    <style>
        .thin-line {
            margin: 0; 
            margin-left:2em;
            border: 0;
            height: 1px;
            background-color: gray;
        }

        .thick-line {
            margin: 0; 
            margin-left:2em;
            border: 0;
            height: 2px;
            background-color: black;
        }
    </style>
    <style>
        .highlight {
            color: red; /* 好きな色に変更してください */
        }
    </style>
    </head>
    <body>
        <h1><center>U-Net: Convolutional Networks for Biomedical
Image Segmentation</center></h1>
<center>U-Net：生物医学画像セグメンテーションのための畳み込みネットワーク</center><br>
<center>Olaf Ronneberger, Philipp Fischer, and Thomas Brox</center>
<center>Computer Science Department and BIOSS Centre for Biological Signalling Studies,</center>
<center>University of Freiburg, Germany</center>
<center>onneber@informatik.uni-freiburg.de,</center>
<center>WWW home page: http://lmb.informatik.uni-freiburg.de/</center>
<p class="margin-large">
<strong>要旨</strong>　ディープネットワークの学習を成功させるには、数千もの注釈付き学習サンプルが必要であることは広く認められています。本稿では、データ拡張を効果的に活用することで、利用可能な注釈付きサンプルをより効率的に使用するネットワークと学習戦略を紹介します。このアーキテクチャは、コンテキストを捉える収縮パスと、正確な位置特定を可能にする対称拡張パスで構成されています。このようなネットワークは、非常に少ない画像からエンドツーエンドで学習でき、電子顕微鏡スタック内の神経構造のセグメンテーションに関するISBIチャレンジにおいて、これまでの最良の手法（スライディングウィンドウ畳み込みネットワーク）よりも優れた性能を発揮することを示します。透過光顕微鏡画像（位相差およびDIC）で学習した同じネットワークを使用することで、これらのカテゴリでISBI細胞追跡チャレンジ2015を大差で優勝しました。さらに、このネットワークは高速です。最新のGPUを使用すれば、512x512画像のセグメンテーションは1秒未満で完了します。完全な実装（Caeベース）と学習済みネットワークは、http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net で入手できます。
<!--
Abstract. There is large consent that successful training of deep net-
works requires many thousand annotated training samples. In this pa-
per, we present a network and training strategy that relies on the strong
use of data augmentation to use the available annotated samples more
efficiently. The architecture consists of a contracting path to capture
context and a symmetric expanding path that enables precise localiza-
tion. We show that such a network can be trained end-to-end from very
few images and outperforms the prior best method (a sliding-window
convolutional network) on the ISBI challenge for segmentation of neu-
ronal structures in electron microscopic stacks. Using the same net-
work trained on transmitted light microscopy images (phase contrast
and DIC) we won the ISBI cell tracking challenge 2015 in these cate-
gories by a large margin. Moreover, the network is fast. Segmentation
of a 512x512 image takes less than a second on a recent GPU. The full
implementation (based on Ca
e) and the trained networks are available
at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.
-->
</p>
<h2>1 はじめに</h2>
<p>
過去2年間で、深層畳み込みネットワークは多くの視覚認識タスクにおいて最先端の性能を上回るようになりました（例：[7,3]）。畳み込みネットワークは以前から存在していましたが（[8]）、利用可能な学習セットのサイズと対象となるネットワークのサイズのために、その成功は限られていました。Krizhevskyらによるブレークスルー（[7]）は、100万枚の学習画像を含むImageNetデータセットを用いて、8層、数百万のパラメータを持つ大規模なネットワークを教師あり学習したことによるものです。それ以来、さらに大規模で深いネットワークが学習されてきました（[12]）。
<!--
In the last two years, deep convolutional networks have outperformed the state of
the art in many visual recognition tasks, e.g. [7,3]. While convolutional networks
have already existed for a long time [8], their success was limited due to the
size of the available training sets and the size of the considered networks. The
breakthrough by Krizhevsky et al. [7] was due to supervised training of a large
network with 8 layers and millions of parameters on the ImageNet dataset with
1 million training images. Since then, even larger and deeper networks have been
trained [12].
-->
</p><p>
畳み込みネットワークの典型的な用途は分類タスクであり、画像への出力は単一のクラスラベルです。しかし、多くの視覚タスク、特に生物医学画像処理では、望ましい出力には位置特定、つまり各ピクセルにクラスラベルが割り当てられていることが必要です。さらに、生物医学タスクで数千枚のトレーニング画像を用意することは通常不可能です。そこで、Ciresanら[1]は、スライディングウィンドウ設定でネットワークをトレーニングし、各ピクセルの周囲の局所領域（パッチ）を入力として与えて、そのピクセルのクラスラベルを予測しました。まず、このネットワークは位置特定が可能です。次に、パッチで表されるトレーニングデータは、トレーニング画像の数よりもはるかに大きいです。この結果得られたネットワークは、ISBI 2012のEMセグメンテーションチャレンジで大差で優勝しました。
<!--
The typical use of convolutional networks is on classification tasks, where
the output to an image is a single class label. However, in many visual tasks,
especially in biomedical image processing, the desired output should include
localization, i.e., a class label is supposed to be assigned to each pixel. More-
over, thousands of training images are usually beyond reach in biomedical tasks.
Hence, Ciresan et al. [1] trained a network in a sliding-window setup to predict
the class label of each pixel by providing a local region (patch) around that pixel as input. First, this network can localize. Secondly, the training data in terms
of patches is much larger than the number of training images. The resulting
network won the EM segmentation challenge at ISBI 2012 by a large margin.
-->
</p><p>
Ciresanら[1]の戦略には、明らかに2つの欠点があります。第一に、ネットワークをパッチごとに個別に実行する必要があるため、非常に遅く、また、パッチの重複により冗長性が多くなります。第二に、位置推定精度とコンテキストの利用の間にはトレードオフがあります。パッチが大きいほど、より多くのMax-Pooling層が必要になり、位置推定精度が低下します。一方、パッチが小さいと、ネットワークはわずかなコンテキストしか認識できません。より最近のアプローチ[11,4]では、複数の層からの特徴を考慮した分類器出力が提案されています。これにより、優れた位置推定とコンテキストの利用が同時に可能になります。
<!--
Obviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it
is quite slow because the network must be run separately for each patch, and
there is a lot of redundancy due to overlapping patches. Secondly, there is a
trade-o
 between localization accuracy and the use of context. Larger patches
require more max-pooling layers that reduce the localization accuracy, while
small patches allow the network to see only little context. More recent approaches
[11,4] proposed a classifier output that takes into account the features from
multiple layers. Good localization and the use of context are possible at the
same time.
-->
</p><p>
本論文では、より洗練されたアーキテクチャ、いわゆる「完全畳み込みネットワーク」[9]を基盤として構築します。このアーキテクチャを修正・拡張することで、非常に少ない訓練画像でも動作し、より高精度なセグメンテーションを生成できるようになります（図1参照）。[9]の主要なアイデアは、通常の収縮ネットワークを連続層で補完し、プーリング演算子をアップサンプリング演算子に置き換えることにあります。したがって、これらの層は出力の解像度を上げます。局所化を行うために、収縮パスからの高解像度の特徴がアップサンプリングされた出力と組み合わせます。連続畳み込み層は、この情報に基づいてより高精度な出力を組み立てることを学習することができます。
<!--
In this paper, we build upon a more elegant architecture, the so-called \fully
convolutional network" [9]. We modify and extend this architecture such that it
works with very few training images and yields more precise segmentations; see
Figure 1. The main idea in [9] is to supplement a usual contracting network by
successive layers, where pooling operators are replaced by upsampling operators.
Hence, these layers increase the resolution of the output. In order to localize, high
resolution features from the contracting path are combined with the upsampled output. A successive convolution layer can then learn to assemble a more precise
output based on this information.
-->
</p>
<center><img src="images/fig1.png"></center>
<p class="margin-large">
図1. U-netアーキテクチャ（最低解像度の32x32ピクセルの例）。青いボックスはそれぞれ、マルチチャンネルの特徴マップに対応しています。チャンネル数はボックスの上部に表示されています。x-yサイズはボックスの左下に表示されます。白いボックスはコピーされた特徴マップを表します。矢印は異なる操作を示しています。
<!--
Fig. 1. U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue
box corresponds to a multi-channel feature map. The number of channels is denoted
on top of the box. The x-y-size is provided at the lower left edge of the box. White
boxes represent copied feature maps. The arrows denote the different operations.
-->
</p><p>
私たちのアーキテクチャにおける重要な変更点の一つは、アップサンプリング部分に多数の特徴チャネルを追加したことです。これにより、ネットワークはコンテキスト情報を高解像度層に伝播できます。その結果、拡張パスは縮小パスとほぼ対称となり、U字型のアーキテクチャとなります。ネットワークには全結合層がなく、各畳み込みの有効な部分のみを使用します。つまり、セグメンテーションマップには、入力画像で完全なコンテキストが利用可能なピクセルのみが含まれます。この戦略により、オーバーラップタイル戦略（図2参照）によって任意の大きさの画像をシームレスにセグメンテーションできます。画像の境界領域のピクセルを予測するために、欠落しているコンテキストは入力画像をミラーリングすることで外挿されます。このタイリング戦略は、ネットワークを大きな画像に適用する上で重要です。そうでなければ、解像度がGPUメモリによって制限されてしまうからです。
<!--
One important modification in our architecture is that in the upsampling
part we have also a large number of feature channels, which allow the network
to propagate context information to higher resolution layers. As a consequence,
the expansive path is more or less symmetric to the contracting path, and yields
a u-shaped architecture. The network does not have any fully connected layers
and only uses the valid part of each convolution, i.e., the segmentation map only
contains the pixels, for which the full context is available in the input image.
This strategy allows the seamless segmentation of arbitrarily large images by an
overlap-tile strategy (see Figure 2). To predict the pixels in the border region
of the image, the missing context is extrapolated by mirroring the input image.
This tiling strategy is important to apply the network to large images, since
otherwise the resolution would be limited by the GPU memory.
-->
</p>
<center><img src="images/fig2.png"></center>
<p class="margin-large">
図2. 任意の大きな画像（ここではEMスタック内の神経構造のセグメンテーション）をシームレスに分割するためのオーバーラップタイル戦略。黄色の領域におけるセグメンテーションの予測には、青色の領域内の画像データを入力として必要とします。欠損した入力データは、ミラーリングによって外挿されます。
<!--
Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here
segmentation of neuronal structures in EM stacks). Prediction of the segmentation in
the yellow area, requires image data within the blue area as input. Missing input data
is extrapolated by mirroring
-->
</p><p>
私たちのタスクでは利用可能なトレーニングデータが非常に少ないため、利用可能なトレーニング画像に弾性変形を適用することで、大幅なデータ拡張を行います。これにより、ネットワークは、注釈付き画像コーパスでこれらの変形を確認する必要なく、そのような変形に対する不変性を学習できます。これは、変形がかつて組織における最も一般的な変化であり、現実的な変形を効率的にシミュレートできるため、生物医学的セグメンテーションにおいて特に重要です。不変性学習におけるデータ拡張の価値は、Dosovitskiyら[2]によって、教師なし特徴学習の分野で示されています。
<!--
As for our tasks there is very little training data available, we use excessive
data augmentation by applying elastic deformations to the available training im-
ages. This allows the network to learn invariance to such deformations, without
the need to see these transformations in the annotated image corpus. This is
particularly important in biomedical segmentation, since deformation used to
be the most common variation in tissue and realistic deformations can be simu-
lated efficiently. The value of data augmentation for learning invariance has been
shown in Dosovitskiy et al. [2] in the scope of unsupervised feature learning.
-->
</p><p>
多くの細胞セグメンテーションタスクにおけるもう1つの課題は、同じクラスの接触している物体を分離することです（図3を参照）。この目的のために、接触している細胞間の分離背景ラベルが損失関数において大きな重みを得る重み付き損失の使用を提案します。
<!--
Another challenge in many cell segmentation tasks is the separation of touch-
ing objects of the same class; see Figure 3. To this end, we propose the use of
a weighted loss, where the separating background labels between touching cells
obtain a large weight in the loss function.
-->
</p><p>
得られたネットワークは、様々な生物医学的セグメンテーション問題に適用可能です。本論文では、EMスタック（ISBI 2012で開始された継続中のコンペティション）における神経構造のセグメンテーションの結果を示し、Ciresanら[1]のネットワークを上回る性能を示しました。さらに、ISBI細胞追跡チャレンジ2015の光学顕微鏡画像における細胞セグメンテーションの結果も示します。この結果では、最も難易度の高い2つの2D透過光データセットにおいて、大きな差をつけて勝利しました。
<!--
The resulting network is applicable to various biomedical segmentation prob-
lems. In this paper, we show results on the segmentation of neuronal structures
in EM stacks (an ongoing competition started at ISBI 2012), where we out-performed the network of Ciresan et al. [1]. Furthermore, we show results for
cell segmentation in light microscopy images from the ISBI cell tracking chal-
lenge 2015. Here we won with a large margin on the two most challenging 2D
transmitted light datasets.
-->
</p>
<center><img src="images/fig3.png"></center>
<p class="margin-large">
図3. DIC（微分干渉コントラスト）顕微鏡で記録されたガラス上のHeLa細胞。(a) 生画像。(b) 正解セグメンテーションのオーバーレイ。異なる色はHeLa細胞の異なるインスタンスを示す。(c) 生成されたセグメンテーションマスク（白：前景、黒：背景）。(d) ネットワークに境界ピクセルを学習させるためのピクセル単位の損失重み付けマップ。
<!--
Fig. 3. HeLa cells on glass recorded with DIC (di
erential interference contrast) mi-
croscopy. (a) raw image. (b) overlay with ground truth segmentation. Di
erent colors
indicate different instances of the HeLa cells. (c) generated segmentation mask (white:
foreground, black: background). (d) map with a pixel-wise loss weight to force the
network to learn the border pixels.
-->
</p>
<h2>2 ネットワークアーキテクチャー</h2>
<p>
ネットワークアーキテクチャを図1に示します。これは、縮小パス（左側）と拡張パス（右側）で構成されています。縮小パスは、畳み込みネットワークの典型的なアーキテクチャに従っています。2つの3x3畳み込み（パディングなし畳み込み）を繰り返し適用し、それぞれにReLU（Rerectified Linear Unit）とストライド2の2x2最大プーリング演算を適用してダウンサンプリングを行います。ダウンサンプリングの各ステップで、特徴チャンネル数を2倍にします。拡張パスの各ステップは、特徴マップのアップサンプリング、特徴チャンネル数を半分にする2x2畳み込み（「アップ畳み込み」）、縮小パスから切り取られた特徴マップとの連結、そして2つの3x3畳み込み（それぞれにReLUを適用）から構成されます。畳み込みのたびに境界ピクセルが失われるため、クロッピングが必要になります。最終層では、1x1畳み込みを用いて、64要素の特徴ベクトルをそれぞれ必要な数のクラスにマッピングします。ネットワークは合計23の畳み込み層を持ちます。
<!--
The network architecture is illustrated in Figure 1. It consists of a contracting
path (left side) and an expansive path (right side). The contracting path follows
the typical architecture of a convolutional network. It consists of the repeated
application of two 3x3 convolutions (unpadded convolutions), each followed by
a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2
for downsampling. At each downsampling step we double the number of feature
channels. Every step in the expansive path consists of an upsampling of the
feature map followed by a 2x2 convolution ("up-convolution") that halves the
number of feature channels, a concatenation with the correspondingly cropped
feature map from the contracting path, and two 3x3 convolutions, each fol-
lowed by a ReLU. The cropping is necessary due to the loss of border pixels in
every convolution. At the final layer a 1x1 convolution is used to map each 64-
component feature vector to the desired number of classes. In total the network
has 23 convolutional layers.
-->
</p><p>
出力セグメンテーションマップのシームレスなタイリングを可能にするには（図2参照）、すべての2x2 Max-Pooling演算がX軸とY軸のサイズが均一なレイヤーに適用されるように入力タイルのサイズを選択することが重要です。
<!--
To allow a seamless tiling of the output segmentation map (see Figure 2), it
is important to select the input tile size such that all 2x2 max-pooling operations
are applied to a layer with an even x- and y-size.
-->
</p>
<h2>3 トレーニング</h2>
<p>
入力画像とそれに対応するセグメンテーションマップを用いて、Cae [6] の確率的勾配降下法を用いてネットワークを学習します。パディングなしの畳み込みのため、出力画像は入力画像よりも一定の境界幅だけ小さくなります。オーバーヘッドを最小限に抑え、GPUメモリを最大限に活用するために、大きなバッチサイズよりも大きな入力タイルを優先し、バッチを1枚の画像に縮小します。そのため、高いモーメンタム（0.99）を使用し、以前に使用した多数の学習サンプルに基づいて、現在の最適化ステップでの更新を決定します。
<!--
The input images and their corresponding segmentation maps are used to train
the network with the stochastic gradient descent implementation of Ca
e [6].
Due to the unpadded convolutions, the output image is smaller than the input
by a constant border width. To minimize the overhead and make maximum use
of the GPU memory, we favor large input tiles over a large batch size and hence
reduce the batch to a single image. Accordingly we use a high momentum (0.99)
such that a large number of the previously seen training samples determine the
update in the current optimization step.
-->
</p><p>
エネルギー関数は、最終的な特徴マップに対してピクセル単位のソフトマックス法と交差エントロピー損失関数を組み合わせることで計算されます。ソフトマックス法は\(p_k(x) = exp(a_k(x))=/\left(\sum_{k^\prime=1}^K exp(ak^\prime (x))\right)\)と定義されます。ここで、\(a_k(x)\)は、\(\Omega\subset\mathbb Z^2\)におけるピクセル位置\(x\in\Omega\)における特徴チャネルkの活性化を表します。Kはクラス数、\(p_k(x)\)は近似最大値関数です。すなわち、最大の活性化\(a_k(x)\)を持つ\(k\)については\(p_k(x)\approx 1\)、それ以外のすべての\(k\)については\(p_k(x)\approx 0\)とします。そして、クロスエントロピーは各位置において\(p_{l(x)}(x)\)の1からの偏差を次のようにペナルティとして課します。
<!--
The energy function is computed by a pixel-wise soft-max over the final
feature map combined with the cross entropy loss function. The soft-max is
defined as \(p_k(x) = exp(a_k(x))=/\left(\sum_{k^\prime=1}^K exp(ak^\prime (x))\right)\) where \(a_k(x)\) denotes the
activation in feature channel k at the pixel position \(x\in\Omega\) with \(\Omega\subset\mathbb Z^2\). K is the number of classes and \(p_k(x)\) is the approximated maximum-function. I.e. \(p_k(x)\approx 1\) for the \(k\) that has the maximum activation \(a_k(x)\) and \(p_k(x)\approx 0\) for
all other \(k\). The cross entropy then penalizes at each position the deviation of \(p_{l(x)}(x)\) from 1 using
-->
\[
E =\sum_{x\in\Omega} w(x)\log(p_{l(x)}(x)) \tag{1}
\]
ここで、\(l:\Omega \rightarrow \{1,\cdots,K\}\) は各ピクセルの真のラベルであり、\(w :\Omega \rightarrow \mathbb R\) はトレーニングで一部のピクセルにさらに重要性を与えるために導入した重みマップです。
<!--
where \(l:\Omega \rightarrow \{1,\cdots,K\}\) is the true label of each pixel and \(w :\Omega \rightarrow \mathbb R\) is a weight map that we introduced to give some pixels more importance in the training.
-->
</p><p>
学習データセット内の特定のクラスのピクセルの頻度の違いを補正し、接触する細胞間に導入する小さな分離境界をネットワークに学習させるために、各正解セグメンテーションの重みマップを事前に計算します（図3cおよびdを参照）。
<!--
We pre-compute the weight map for each ground truth segmentation to com-
pensate the di
erent frequency of pixels from a certain class in the training
data set, and to force the network to learn the small separation borders that we
introduce between touching cells (See Figure 3c and d).
-->
</p><p>
分離境界は形態学的演算を用いて計算される。重みマップは次のように計算されます。
<!--
The separation border is computed using morphological operations. The
weight map is then computed as
-->
\[
w(x) = w_c(x) + w_0\cdot exp\left(-\frac{(d_1(x)+d_2(x))^2}{2\sigma^2}\right) \tag{2}
\]
ここで、\(w_c :\Omega\rightarrow \mathbb R\) はクラス頻度のバランスをとるための重みマップ、\(d_1:\Omega\rightarrow \mathbb R\) は最も近いセルの境界までの距離、\(d_2 :\Omega\rightarrow \mathbb R\) は2番目に近いセルの境界までの距離を表します。実験では、\(w_0 = 10\) および \(\sigma \approx 5\) ピクセルに設定しました。
<!--
where \(w_c :\Omega\rightarrow \mathbb R\) is the weight map to balance the class frequencies, \(d_1:\Omega \rightarrow \mathbb R\) denotes the distance to the border of the nearest cell and \(d_2 :\Omega\rightarrow \mathbb R\) the distance to the border of the second nearest cell. In our experiments we set(w_0 = 10\) and \(\sigma \approx 5\) pixels.
-->
</p><p>
多数の畳み込み層とネットワークを介した多様な経路を持つ深層ネットワークでは、重みの適切な初期化が極めて重要です。適切に初期化しないと、ネットワークの一部が過剰な活性化を引き起こし、他の部分は全く寄与しない可能性があります。理想的には、ネットワーク内の各特徴マップがほぼ単位分散を持つように初期重みを調整する必要があります。私たちのアーキテクチャ（畳み込み層とReLU層を交互に配置）を持つネットワークでは、標準偏差が\(\sqrt{2/N}\)のガウス分布から初期重みを取得することでこれを実現できます。ここで、\(N\)は1つのニューロンの入力ノード数を表します[5]。例えば、3x3の畳み込み層と前の層の64個の特徴チャネルの場合、\(N = 9\cdot 64 = 576\)となります。
<!--
In deep networks with many convolutional layers and di
erent paths through
the network, a good initialization of the weights is extremely important. Oth-
erwise, parts of the network might give excessive activations, while other parts
never contribute. Ideally the initial weights should be adapted such that each
feature map in the network has approximately unit variance. For a network with
our architecture (alternating convolution and ReLU layers) this can be achieved
by drawing the initial weights from a Gaussian distribution with a standard
deviation of \(\sqrt{2/N}\), where \(N\) denotes the number of incoming nodes of one neuron [5]. E.g. for a 3x3 convolution and 64 feature channels in the previous layer \(N = 9\cdot 64 = 576\).
-->
</p>
<h3>3.1 データ拡張</h3>
<p>
データ拡張は、利用可能なトレーニングサンプルが少ない場合に、ネットワークに必要な不変性と堅牢性を学習させるために不可欠です。顕微鏡画像の場合、主にシフトと回転に対する不変性、そして変形とグレー値の変化に対する堅牢性が必要です。特に、トレーニングサンプルのランダムな弾性変形は、非常に少ない注釈付き画像でセグメンテーションネットワークをトレーニングするための鍵となる概念です。粗い3×3グリッド上のランダム変位ベクトルを用いて、滑らかな変形を生成します。変位は、標準偏差10ピクセルのガウス分布からサンプリングされます。その後、ピクセルごとの変位は双3次補間を用いて計算されます。収縮パスの最後にあるドロップアウト層は、さらに暗黙的なデータ拡張を実行します。
<!--
Data augmentation is essential to teach the network the desired invariance and
robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as
robustness to deformations and gray value variations. Especially random elas-
tic deformations of the training samples seem to be the key concept to train
a segmentation network with very few annotated images. We generate smooth
deformations using random displacement vectors on a coarse 3 by 3 grid. The
displacements are sampled from a Gaussian distribution with 10 pixels standard
deviation. Per-pixel displacements are then computed using bicubic interpola-
tion. Drop-out layers at the end of the contracting path perform further implicit
data augmentation.
-->
</p>
<h2>4 実験</h2>
<p>
u-netを3つの異なるセグメンテーションタスクに適用する例を示します。最初のタスクは、電子顕微鏡記録における神経構造のセグメンテーションです。データセットの例と得られたセグメンテーションを図2に示します。完全な結果は補足資料として提供する。データセットは、ISBI 2012で開始され、現在も新しい投稿を募集しているEMセグメンテーションチャレンジ[14]によって提供されました。トレーニングデータは、ショウジョウバエ1齢幼虫腹側神経索（VNC）の連続切片透過型電子顕微鏡画像から得られた30枚の画像（512×512ピクセル）です。各画像には、細胞（白）と膜（黒）の対応する完全に注釈付けされた正解セグメンテーションマップが付属しています。テストセットは公開されていますが、セグメンテーションマップは非公開です。評価版を入手するには、予測された膜確率マップを主催者に送付します。評価は、マップを10の異なるレベルで閾値処理し、「ワーピング誤差」、「ランダム誤差」、および「ピクセル誤差」を計算することによって行われます[14]。
<!--
We demonstrate the application of the u-net to three di
erent segmentation
tasks. The first task is the segmentation of neuronal structures in electron mi-
croscopic recordings. An example of the data set and our obtained segmentation
is displayed in Figure 2. We provide the full result as Supplementary Material.
The data set is provided by the EM segmentation challenge [14] that was started
at ISBI 2012 and is still open for new contributions. The training data is a set of
30 images (512x512 pixels) from serial section transmission electron microscopy
of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells
(white) and membranes (black). The test set is publicly available, but its seg-
mentation maps are kept secret. An evaluation can be obtained by sending the
predicted membrane probability map to the organizers. The evaluation is done
by thresholding the map at 10 di
erent levels and computation of the \warping
error", the \Rand error" and the "pixel error" [14].
-->
</p><p>
u-net（入力データの7つの回転バージョンを平均）は、追加の前処理や後処理を行わなくても、ワーピング誤差0.0003529（新しい最高スコア、表1を参照）とrand誤差0.0382を達成しました。
<!--
The u-net (averaged over 7 rotated versions of the input data) achieves with-
out any further pre- or postprocessing a warping error of 0.0003529 (the new
best score, see Table 1) and a rand-error of 0.0382.
-->
</p>
<p class="margin-large">
表1. EMセグメンテーションチャレンジ[14]（2015年3月6日）におけるワーピングエラー順のランキング。
<!--
Table 1. Ranking on the EM segmentation challenge [14] (march 6th, 2015), sorted by warping error.
-->
</p>
<center><img src="images/table1.png"></center>
<p>
これは、Ciresanら[1]によるスライディングウィンドウ畳み込みネットワークの結果よりも大幅に優れています。Ciresanらの最高の提出物は、ワーピング誤差が0.000420、rand誤差が0.0504でした。rand誤差に関して、このデータセットでより優れたパフォーマンスを発揮するアルゴリズムは、Ciresanら[1]の確率マップにデータセット固有の高度な後処理手法<sup>1</sup>を適用したアルゴリズムのみです。
<!--
This is significantly better than the sliding-window convolutional network
result by Ciresan et al. [1], whose best submission had a warping error of 0.000420
and a rand error of 0.0504. In terms of rand error the only better performing algorithms on this data set use highly data set specific post-processing methods<sup>1</sup>
applied to the probability map of Ciresan et al. [1].
-->
<p>
<p class="margin-large">
<sup>1</sup> 
このアルゴリズムの著者は、この結果を達成するために78通りの異なる解法を提示しました。
<!--
The authors of this algorithm have submitted 78 di
erent solutions to achieve this
result.
-->
</p><p>
我々はまた、u-netを光学顕微鏡画像における細胞セグメンテーションタスクに適用した。このセグメンテーションタスクは、ISBI細胞追跡チャレンジ2014および2015 [10,13]の一部である。最初のデータセット PhC-U373"<sup>2</sup>には、位相差顕微鏡で記録されたポリアクリルイミド基板上の神経膠腫・星状細胞腫U373細胞が含まれている（図4a、bおよび補足資料を参照）。このデータセットには、部分的に注釈が付けられた35枚のトレーニング画像が含まれている。ここでは平均IOU（intersection over union）が92%となり、2番目に優れたアルゴリズムの83%を大幅に上回った（表2を参照）。 2番目のデータセット「DIC-HeLa」<sup>3</sup>は、微分干渉（DIC）顕微鏡で記録されたガラス上のHeLa細胞です（図3、図4c、d、および補足資料を参照）。このデータセットには、部分的に注釈が付けられた20枚の学習画像が含まれています。ここでは平均IOUが77.5%に達し、2番目に優れたアルゴリズムの46%を大幅に上回っています。
<!--
We also applied the u-net to a cell segmentation task in light microscopic im-
ages. This segmenation task is part of the ISBI cell tracking challenge 2014 and
2015 [10,13]. The first data set \PhC-U373"<sup>2</sup> contains Glioblastoma-astrocytoma
U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy
(see Figure 4a,b and Supp. Material). It contains 35 partially annotated train-
ing images. Here we achieve an average IOU (\intersection over union") of 92%,
which is significantly better than the second best algorithm with 83% (see Ta-
ble 2). The second data set \DIC-HeLa"<sup>3</sup> are HeLa cells on a 
at glass recorded
by di
erential interference contrast (DIC) microscopy (see Figure 3, Figure 4c,d
and Supp. Material). It contains 20 partially annotated training images. Here we
achieve an average IOU of 77.5% which is significantly better than the second
best algorithm with 46%.
-->
</p>
<p class="margin-large">
<sup>2</sup> 
データセット提供：サンジェイ・クマール博士（カリフォルニア大学バークレー校バイオエンジニアリング学部、カリフォルニア州バークレー（米国））<br>
<!--
Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University
of California at Berkeley. Berkeley CA (USA)<br>
-->
<sup>3</sup> 
データセットはゲルト・ファン・カペレン・エラスムス医療センター博士から提供されました。ロッテルダム。
オランダ
<!--
Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam.
The Netherlands
-->
</p>
<center><img src="images/fig4.png"></center>
<p class="margin-large">
図4. ISBI細胞追跡チャレンジの結果。(a) PhC-U373データセットの入力画像の一部。(b) 手動で作成した正解（黄色の枠線）を使用したセグメンテーション結果（シアンマスク）。(c) \DIC-HeLaデータセットの入力画像。(d) 手動で作成した正解（黄色の枠線）を使用したセグメンテーション結果（ランダムカラーマスク）。
<!--
Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the
\PhC-U373" data set. (b) Segmentation result (cyan mask) with manual ground truth
(yellow border) (c) input image of the \DIC-HeLa" data set. (d) Segmentation result
(random colored masks) with manual ground truth (yellow border).
-->
</p>
<p class="margin-large">
表 2. ISBI 細胞追跡チャレンジ 2015 におけるセグメンテーション結果 (IOU)。
<!--
Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015.
-->
</p>
<center><img src="images/table2.png"></center>
<h2>5 結論</h2>
<p>
u-netアーキテクチャは、非常に多様な生物医学セグメンテーションアプリケーションにおいて非常に優れたパフォーマンスを発揮します。弾性変形によるデータ拡張により、必要な注釈付き画像はごくわずかで、NVidia Titan GPU (6 GB) でわずか10時間という非常に合理的な学習時間を実現しています。私たちは、Cae[6]ベースの完全な実装と学習済みネットワーク4を提供します。u-netアーキテクチャは、より多くのタスクに容易に適用できると確信しています。
<!--
The u-net architecture achieves very good performance on very di
erent biomed-
ical segmentation applications. Thanks to data augmentation with elastic defor-mations, it only needs very few annotated images and has a very reasonable
training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the
full Ca
e[6]-based implementation and the trained networks4. We are sure that
the u-net architecture can be applied easily to many more tasks.
</p>
<p class="margin-large">
<sup>4</sup> U-net implementation, trained networks and supplementary material available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net
-->
</p>
<h2>謝辞</h2>
<p>
この研究は、ドイツ連邦政府および州政府のエクセレンス・イニシアチブ（EXC 294）およびBMBF（Fkz 0316185B）の支援を受けて実施されました。
<!--
This study was supported by the Excellence Initiative of the German Federal
and State governments (EXC 294) and by the BMBF (Fkz 0316185B).
-->
</p>
<h2>参考文献</h2>
<p>
<div class="styleRef">
<ul>
<li>1. Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural net-
works segment neuronal membranes in electron microscopy images. In: NIPS. pp.
2852{2860 (2012)
</li><br><li>2. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative un-
supervised feature learning with convolutional neural networks. In: NIPS (2014)
</li><br><li>3. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for ac-
curate object detection and semantic segmentation. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2014)
</li><br><li>4. Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object seg-
mentation and fine-grained localization (2014), arXiv:1411.5752 [cs.CV]
</li><br><li>5. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification (2015), arXiv:1502.01852 [cs.CV]
</li><br><li>6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Ca
e: Convolutional architecture for fast feature embedding
(2014), arXiv:1408.5093 [cs.CV]
</li><br><li>7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con-
volutional neural networks. In: NIPS. pp. 1106{1114 (2012)
</li><br><li>8. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.,
Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural
Computation 1(4), 541{551 (1989)
</li><br><li>9. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation (2014), arXiv:1411.4038 [cs.CV]
</li><br><li>10. Maska, M., (...), de Solorzano, C.O.: A benchmark for comparison of cell tracking
algorithms. Bioinformatics 30, 1609{1617 (2014)
</li><br><li>11. Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded
hierarchical models and logistic disjunctive normal networks. In: Computer Vision
(ICCV), 2013 IEEE International Conference on. pp. 2168{2175 (2013)
</li><br><li>12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition (2014), arXiv:1409.1556 [cs.CV]
</li><br><li>13. WWW: Web page of the cell tracking challenge, http://www.codesolorzano.com/
celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html
</li><br><li>14. WWW: Web page of the em segmentation challenge, http://brainiac2.mit.edu/
isbi_challenge/</li>
</ul>
</div>
</p>
    </body>
</html>