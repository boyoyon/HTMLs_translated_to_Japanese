<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>Light-R1</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    <style>
        .thin-line {
            margin: 0; 
            margin-left:2em;
            border: 0;
            height: 1px;
            background-color: gray;
        }

        .thick-line {
            margin: 0; 
            margin-left:2em;
            border: 0;
            height: 2px;
            background-color: black;
        }
    </style>
    <style>
        .highlight {
            color: red; /* 好きな色に変更してください */
        }
    </style>
    </head>
    <body>
        <h1><center><center>Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and  Beyond </center></h1>
<center>Light-R1: 長い思考の連鎖のための SFT、DPO、RL カリキュラム (ゼロから、さらにその先へ)</center>
<br>
<center>Liang Wen1Yunke Cai1 Fenrui Xiao1 Xin He1 Qi An1 Zhenyu Duan1 </center>
<center>Yimin Du1 Junchen Liu1 Lifu Tang1 Xiaowei Lv1,2 </center>
<center>Haosheng Zou1 Yongchao Deng1 Shousheng Jia1 Xiangzheng Zhang1 </center>
<center>1Qiyuan Tech 2Renmin University </center>
<center>zhangxiangzheng@360.cn </center>

<h2><center>要旨</center></h2>
<!--
<h2><center>Abstract</center></h2>
-->
<p class="margin-large">
本稿では、再現性と費用対効果に優れた手法を用いて長文推論モデルを学習するためのオープンソーススイートであるLight-R1を紹介します。DeepSeek-R1シリーズで使用されるデータは独自のものであるため、公開データとモデルのみを活用する代替アプローチを開発しました。カリキュラム学習では、データの難易度を段階的に高め、多段階の学習後処理と組み合わせます。Qwen2.5-32BInstructで学習したLight-R1-32Bモデルは、数学的推論においてDeepSeek-R1-Distill-Qwen-32Bを上回る性能を発揮します。実験結果によると、このカリキュラムアプローチは、異なる学習段階で異なる多様なデータセットを利用できる場合に、より効果的になることが示されています。DeepSeekチームが独自のデータに基づいて事前調整したDeepSeek-R1-Distilledモデルを、カリキュラムデータセットの3,000個の難解な例で微調整したところ、最先端の7Bおよび14Bモデルが得られました。一方、32BモデルであるLight-R1-32B-DSは、QwQ-32BおよびDeepSeek-R1と同等の性能を発揮しました。さらに、GRPOを長文推論モデルに適用することで、研究を拡張しました。最終的なLight-R1-14B-DSは、数学において14Bモデルの中でSOTA性能を達成し、AIME24および25スコアはそれぞれ74.0と60.2となり、多くの32BモデルやDeepSeek-R1-Distill-Llama-70Bを上回りました。 Light-R1-14B-DS は、数学に重点を置いた学習にもかかわらず、強力なクロスドメイン汎化能力を発揮します。Light-R1 は、高度な推論モデルを実世界のアプリケーションでよりアクセスしやすく実装しやすくする上で、大きな進歩を遂げています。モデル、学習データ、コードは https://github.com/Qihoo360/Light-R1 で公開されています。
<!--
This paper introduces Light-R1, an opensource
suite for training long reasoning models
using reproducible and cost-effective methodology.
Given the proprietary nature of data used
in the DeepSeek-R1 series, we develop an alternative
approach leveraging exclusively public
data and models. Our curriculum training progressively
increases data difficulty, combined
with multi-staged post-training. Our Light-
R1-32B model, trained from Qwen2.5-32BInstruct,
outperforms DeepSeek-R1-Distill-
Qwen-32B in math reasoning. Experimental
results show that this curriculum approach
becomes more effective when distinct, diverse
datasets are available for different training
stages: fine-tuning DeepSeek-R1-Distilled
models (pre-tuned by DeepSeek team on proprietary
data) with 3,000 challenging examples
from our curriculum dataset yielded state-ofthe-
art 7B and 14B models, while the 32B
model, Light-R1-32B-DS performed comparably
to QwQ-32B and DeepSeek-R1. Furthermore,
we extend our work by applying GRPO
on long reasoning models. Our final Light-R1-
14B-DS achieves SOTA performance among
14B models in math, with AIME24 & 25 scores
of 74.0 and 60.2 respectively, surpassing many
32B models and DeepSeek-R1-Distill-Llama-
70B. Despite math-focused training, Light-R1-
14B-DS demonstrates strong cross-domain generalization.
Light-R1 represents a significant
advancement in making sophisticated reasoning
models more accessible and implementable
in real-world applications. Our models, training
data and code have been made available at
https://github.com/Qihoo360/Light-R1.
-->
</p>
<center><img src="images/fig1.png"></center>
<p>
図1：再現可能な最先端のロングCOTモデル
(上)ゼロから開発（ショートCOTベース）、
(下)カリキュラム学習戦略を用いてDeepSeek-R1-Distillモデルから派生
(ロングCOTベース)。
<!--
Figure 1: Reproducible state-of-the-art long COT models
(top) developed from scratch (=short-COT base),
(bottom) derived from DeepSeek-R1-Distill models
(=long-COT base), via curriculum learning strategy.
-->
</p>
<h2>1 はじめに</h2>
<!--
<h2>1 Introduction</h2>
-->
<p>
DeepSeek-R1 (DeepSeek-AI, 2025) のリリース以来、長い思考連鎖 (OpenAI, 2024; Wei et al., 2022; Kimi, 2025; Lightman et al., 2023) 推論は、基礎AIモデルと様々な産業AIアプリケーションの両方で広く普及しています。しかし、フルキャパシティのR1レベルモデル（通常700億以上のパラメータ、DeepSeek-R1では671億のパラメータ）を展開するには、法外な計算コストがかかります (DeepSeek-AI, 2025; Qwen, 2025)。巨大モデルの学習と展開にはリソースの障壁があり、エッジデバイスやリアルタイムアプリケーションには実用的ではありません。この制限により、数10億パラメータのコンパクトかつ高性能なモデルを開発し、長時間のCOTを実行できることへの関心が高まっています。これは、数学的問題解決、アルゴリズム計画、科学的分析にとって重要な要件です。この課題に対処するため、Light-R1シリーズに関する研究成果を紹介します。
<!--
Since the release of DeepSeek-R1 (DeepSeek-AI,
2025), long chain-of-thought (OpenAI, 2024; Wei
et al., 2022; Kimi, 2025; Lightman et al., 2023)
reasoning has gained widespread popularity in both foundational AI models and various industrial
AI applications. However, deploying fullcapacity
R1-level models (typically 70B+ parameters,
DeepSeek-R1 with 671B parameters) incurs
prohibitive computational costs (DeepSeek-AI,
2025; Qwen, 2025). The resource barrier of training
and deploying the giant models makes them
impractical for edge devices and real-time applications.
This limitation has sparked growing interest
in developing compact yet capable models under
a few 10B parameters that can perform extended
long COT - a critical requirement for mathematical
problem solving, algorithmic planning, and scientific
analysis. To address this challenge, we present
our work on the Light-R1 series.
-->
</p>
<p>
本研究の基盤として、まず、DeepSeek-AI (2025) で報告された評価結果を厳密に再現する、堅牢で再現性の高い評価プロトコルを確立しました。この信頼性の高い枠組みを基に、革新的なアルゴリズムとエンジニアリングの進歩を通じて、3つの基本的な課題に体系的に取り組んでいます。
<!--
As a foundation for our research, we first established
robust and reproducible evaluation protocols that rigorously reproduce the evaluation results reported
in DeepSeek-AI (2025). Building upon this
reliable framework, our research systematically addresses
three fundamental challenges through innovative
algorithmic and engineering advancements.
-->
</p>
<p>
最初の課題は、長い思考連鎖最適化の重要な要素である学習後用の効率的なデータセットをキュレーションすることです（Ye et al., 2025; Muennighoff et al., 2025; Li et al., 2025）。数学的推論、論理的演繹、アルゴリズムによる問題解決など、多様なオープンソースの推論データを収集しました。重複を除去し、フォーマットを標準化するための前処理を行った後、DeepScaleR-1.5B-Preview（Luo et al., 2025b）とDeepSeek-R1-Distill-Qwen-32Bモデルを用いた2段階の難易度フィルタリング手法を実装し、合格率に基づいて難易度を定量化しました。
<!--
The first challenge involves curating an efficient
dataset for Post-Training, a critical factor for long-
COT optimization (Ye et al., 2025; Muennighoff
et al., 2025; Li et al., 2025). We collected diverse
open-source reasoning data covering mathematical
reasoning, logical deduction, and algorithmic
problem-solving. After preprocessing to remove
duplicates and standardize formatting, we implemented
a two-stage difficulty filtering methodology
using DeepScaleR-1.5B-Preview (Luo et al.,
2025b) and DeepSeek-R1-Distill-Qwen-32B models
to quantify difficulty based on pass rates.
-->
</p><p>
2つ目の課題は、このデータセットの活用を最適化する方法です。従来のアプローチでは通常、単一のSFTステージが採用されます（DeepSeek-AI, 2025; Xu et al., 2025; Labs, 2025; Yu et al., 2024）。しかし、32Bモデルを用いた予備実験では、重大な限界が明らかになりました。トレーニングデータの約20%は、10回の実行で依然として50%未満の合格率を示しており、異質な難易度のデータセットからの知識の同化が不十分であることが示されました。この問題を解決するために、私たちは、難易度が徐々に上昇する2つの連続するSFTステージと、それに続くDPOステージで構成される、多段階カリキュラムトレーニング戦略を実装しました（Rafailov et al., 2023）。最近の研究では、長い思考連鎖訓練のための様々なカリキュラム戦略が検討されています（Luo et al., 2025a; Min et al., 2024; Xi et al., 2024; Yuan et al., 2025a）。しかし、私たちのアプローチは優れた性能を示しています。Qwen2.5-32BInstruct（Qwen, 2024）から訓練されたLight-R1-32Bモデルは、数学的推論においてDeepSeek-R1-Distill-Qwen-32Bよりも優れた性能を発揮します。
<!--
The second challenge then emerges as how to
optimize the utilization of this dataset. While conventional
approaches typically employ a single SFT
stage (DeepSeek-AI, 2025; Xu et al., 2025; Labs,
2025; Yu et al., 2024), our preliminary experiments
with our 32B model revealed significant limitations—
approximately 20% of training data still
exhibited pass rates below 50% across 10 runs, indicating
insufficient knowledge assimilation from
heterogeneous difficulty datasets. To address this,
we implemented a multi-staged curriculum training
strategy comprising two consecutive SFT stages
with progressively increasing difficulty, followed
by a DPO stage (Rafailov et al., 2023). Although recent
work has explored different curriculum strategies
for long-COT training (Luo et al., 2025a; Min
et al., 2024; Xi et al., 2024; Yuan et al., 2025a), our
approach demonstrates superior performance: our
Light-R1-32B model, trained from Qwen2.5-32BInstruct
(Qwen, 2024), outperforms DeepSeek-R1-
Distill-Qwen-32B in mathematical reasoning.
-->
</p><p>
3つ目の課題は、モデルのパフォーマンスをさらに向上させるために、学習後段階の最終要素である強化学習（Shao et al., 2024; Wang et al., 2024; Ouyang et al., 2022; Schulman et al., 2017, 2015）を実装することから生じます。Light-R1-14B-DSの強化学習学習に成功したことを報告できることを嬉しく思います。最近の研究では、基本モデル（Zeng et al., 2025; Hu et al., 2025; Liu et al., 2025）、小規模モデル（Zeng et al., 2025; Luo et al., 2025b）、あるいは集中的な計算リソースを用いた大規模モデル（Qwen, 2025）の学習において成功が示されていますが、我々のlong-COT RLの学習後処理は、long-COT 14Bモデルにおいて、通常観察される初期の長さの短縮なしに、応答長と報酬スコアの両方が同時に増加することを初めて実証したものです。この画期的な成果は、慎重に設計されたカリキュラム戦略によって、小規模モデルにおける強化学習のこれまで報告されていたスケーラビリティの限界を克服できることを示しています（Gao et al., 2023）。
<!--
The third challenge arises from implementing
the final component of Post-Training — Reinforcement
Learning (Shao et al., 2024; Wang et al.,
2024; Ouyang et al., 2022; Schulman et al., 2017,
2015) — to further enhance model performance.
We are excited to report our successful reinforcement
learning training of Light-R1-14B-DS. While
recent research has shown success in training base
models (Zeng et al., 2025; Hu et al., 2025; Liu et al.,
2025), smaller models (Zeng et al., 2025; Luo et al.,
2025b), or larger models with intensive computational
resources (Qwen, 2025), our long-COT RL
Post-Training represents the first demonstration of
simultaneous increases in both response length and reward scores on long-COT 14B models without
the initial length reduction typically observed. This
breakthrough demonstrates that carefully designed
curriculum strategies can overcome the previously
documented scalability limitations of RL in smaller
models (Gao et al., 2023).
-->
</p><p>
本研究の主な貢献は以下のとおりです。
<div class="styleBullet">
<ul>
<li>• 長いCOTモデルをゼロから学習するための、詳細かつ完全にオープンソースの学習後カリキュラムアプローチ。この多段階カリキュラム学習は、難易度が段階的に変化するデータへの露出を通じて推論能力を段階的に構築し、学習コストはわずか1,000ドル（H800 GPU 12基で6時間）です。このアプローチはQwen2.5-32B-Instructで検証されており、7Bおよび14Bモデルにも容易に移行できます。
</li><br><li>• 3,000問の確立されたSFTステージ2データセット（主に数学の問題）は、SFTステージ1だけでなく、すべてのDeepSeek-R1-Distillモデルを大幅に改善する可能性があります。その結果、SOTA 7BモデルLight-R1-7B-DSが生まれました。
</li><br><li>• 14Bモデルにおける数学的推論の有効性を初めて実証し、強化学習以前のモデルと比較して約2%の絶対値の改善を達成しました。その結果、SOTA 14BモデルLight-R1-14B-DSが誕生しました。</li>
</ul>
</div>
<!--
The key contributions of this work include:
<div class="styleBullet">
<ul>
<li>• A detailed, fully open-source Curriculum Post-
Training approach to train long-COT models
from scratch. The multi-stage curriculum
training incrementally builds reasoning capacity
through difficulty-progressive data exposure,
requiring only $1000 training cost (6
hours on 12×H800 GPUs). This approach is
validated on Qwen2.5-32B-Instruct and could
be easily migrated to 7B and 14B models.
</li><br><li>• A well established SFT stage 2 dataset of 3k
mostly math questions that could significantly
improve not only SFT stage 1 but also all
DeepSeek-R1-Distill models, resulting in our
SOTA 7B model Light-R1-7B-DS.
</li><br><li>• First demonstration of RL effectiveness on
14B models for mathematical reasoning,
achieving around 2% absolute improvement
compared with before-RL, resulting in our
SOTA 14B model Light-R1-14B-DS.</li>
</ul>
</div>
-->
</p>
<center><img src="images/table1.png"></center>
<p>
表1：AIME24 (MAA, 2024) pass@1におけるDeepSeek-AI (2025)とQwen (2025)の評価結果の再現。64回実行の平均。
<!--
Table 1: Reproduction of DeepSeek-AI (2025) and
Qwen (2025) evaluation results on AIME24 (MAA,
2024) pass@1 averaged over 64 runs.
-->
</p>
<h2>2 すべての起源：長い思考連鎖モデルの安定的かつ信頼できる評価</h2>
<!--
<h2>2 The Origin of Everything: Stable and Trustworthy Evaluation of Long-COT Models</h2>
-->
<p>
DeepSeek-AI (2025) に倣い、long-COT モデルはサンプリング温度 0.6 で一般的に導入されています。long-COT モデルは一般的に、貪欲デコードよりもサンプリングを用いた方がパフォーマンスが向上しますが、評価のための貪欲デコードのこれまでの有効なアプローチ (Song et al., 2024) とは異なり、各質問に対して複数のサンプルが必要になる場合があるため、モデル評価の負担が大きくなります。
<!--
Following DeepSeek-AI (2025), long-COT models
are commonly deployed with sampling temperature
0.6. While long-COT models generally perform
better with sampling than with greedy decoding, it
brings more burden for model evaluation as multiple
samples for each question may be required,
contrary to previous viable approaches of greedy
decoding for evaluation (Song et al., 2024).
-->
</p>
<p>
DeepSeek-AI (2025) は、pass@1 を推定するためにクエリごとに64個のレスポンスを生成します。この選択を検証した結果、同じモデルを複数回実行した際に16個以下のレスポンスで3ポイント以上の大きな偏差が確認されました。このようなランダム性は、モデルのパフォーマンスを比較する上で許容できません。
<!--
DeepSeek-AI (2025) generates 64 responses per
query to estimate pass@1. We have verified this
choice, witnessing large deviation of over 3 points
using 16 responses or fewer across different runs of
the same model. Such randomness is unacceptable
to compare model performances.
-->
</p><p>
安定的かつ信頼性の高い評価を行うため、全ての評価実行において (Luo et al., 2025b) の評価コードを採用しました。評価コードとログはすべて公開されています。
<!--
For stable and trustworthy evaluation, we
adapted (Luo et al., 2025b)’s evaluation code for
all our evaluation runs. Our evaluation code and
logs are all released.
-->
</p><p>
DeepSeek-AI (2025) および Qwen (2025) で報告されているすべての DeepSeek-R1-Distill モデルと QwQ のスコアを、表 1 に示すように、クエリあたり 64 サンプルで約 1 ポイントの偏差で再現できます。
<!--
We can reproduce all DeepSeek-R1-Distill models’
and QwQ’s scores as reported in DeepSeek-AI
(2025); Qwen (2025) as shown in Tab. 1 with 64
samples per query, with deviation around 1 point.
-->
</p>
<h2>3 Light-R1-32B: カリキュラム SFT と DPO を使用したゼロからの Long-COAT</h2>
<!--
<h2>3 Light-R1-32B: Long-COT from Scratch with Curriculum SFT & DPO</h2>
-->
<p>
数多くの研究（Ye et al., 2025; Muennighoff et al., 2025; OpenThoughts, 2025; OpenR1, 2025）では、15億から32億までの様々な規模のモデルを用いてDeepSeek-R1を再現するオープンソースの取り組みが行われていますが、難易度の高い数学コンテストAIME24と25でDeepSeek-R1-Distill-Qwen-32Bがそれぞれ72.6と54.9というスコアを記録したのに対し、同様のパフォーマンスを達成した研究は存在しません。
<!--
While numerous studies (Ye et al., 2025; Muennighoff
et al., 2025; OpenThoughts, 2025; OpenR1,
2025) have open-sourced efforts to replicate
DeepSeek-R1 using models of various sizes, ranging
from 1.5B to 32B, none has reached similar performance
on the challenging mathematics competitions
AIME24 & 25, where DeepSeek-R1-Distill-
Qwen-32B scored at 72.6 & 54.9.
-->
</p><p>
このセクションでは、図2に示すように、データ処理と学習後のパイプラインを示します。
<!--
We present our data processing and Post-
Training pipeline in this section as illustrated by
Fig. 2.
-->
</p>
<center><img src="images/fig2.png"></center>
<p>
<center>図2: Light-R1シリーズのトレーニングパイプラインの概要</center>
<!--
<center>Figure 2: Overview of training pipeline of Light-R1 series.</center>
-->
</p>
<h3>3.1 データの準備</h3>
<!--
<h3>3.1 Data Preparation</h3>
-->
<p>
データ準備プロセス全体は、データ収集、データ除染、データ生成から成り、以下に詳しく説明します。
<!--
The whole data preparation process spans data collection,
data decontamination and data generation,
detailed as follows.
-->
</p>
<h4>3.1.1 データ収集</h4>
<!--
<h4>3.1.1 Data Collection</h4>
-->
<p>
まず、グラウンドトゥルースの解答を持つ数学の問題を様々なソースから収集しました。あらゆるソースを反復処理した結果、シードセットとして約100万問の数学の問題が収集されました。データソースの詳細については、付録Aをご覧ください。
<!--
We began by collecting various sources of math
questions with groundtruth answers. Iterating over
all possible sources by the time, we collected
around 1000k math questions as the seed set. See
Appendix A for more details about the data sources.
-->
</p><p>
すべてのデータは集約され、シードセットとして約100万問の数学問題が作成されます。この100万問のデータのうち、グラウンドトゥルース解答を持つ数学問題のみを保持しました。グラウンドトゥルース解答を持たない問題は、複数の強力なLLMにグラウンドトゥルースを投票させることで合成データとして使用できますが、これは将来の研究のために残しました。
<!--
All data are aggregated together to form around
1000k math questions as the seed set. Within
this 1000k data, we kept only math questions
with groundtruth answers. Questions without
groundtruth answers could be used as synthetic
data by letting multiple strong LLMs vote for
groundtruths but we left it for future work.
-->
</p><p>
次に、データは多様性を考慮してフィルタリングされ、社内のタグ付けシステムを使用して各質問にタグが付けられ、過剰なデータを含むカテゴリがダウンサンプリングされます。
<!--
The data is then filtered for diversity, where we
tagged each question with an in-house tagging system
and downsample categories with excessive
data.
-->
</p>
<h4>3.1.2 データの除染</h4>
<!--
<h4>3.1.2 Data Decontamination</h4>
-->
<p>
複数のオープンソースデータセットにおけるデータ汚染を評価しました。分析の結果、MATH-500 (Hendrycks et al., 2021a) には、数値が同一であるか、または数値のみが異なる、数十の不適切な問題が含まれていることが明らかになりました。AIME 24および25は汚染されていませんが、2023年までのAIMEデータを組み込む際には注意が必要です。詳細は付録Bに記載されています。
<!--
We evaluated data contamination in several opensourced
datasets. Our analysis revealed thatMATH-
500 (Hendrycks et al., 2021a) contains tens of compromised
questions that are either identical or differ
only in numerical values. AIME 24 and 25 remain
uncontaminated, though caution is needed when
incorporating AIME data through 2023. Further
details are provided in Appendix B.
-->
</p><p>
Light-R1は、完全一致マッチング（数字を除外し、数値のみの変更がある質問をフィルタリング）とAIME24&25、MATH-500、GPQA（Rein et al., 2023）とのNグラムマッチング（N=32）を用いて、包括的な除染を受けました。
<!--
Light-R1 underwent comprehensive decontamination
using exact matching (excluding digits to
filter questions with only numerical changes) and
N-gram (N=32) matching against AIME24&25,
MATH-500, and GPQA (Rein et al., 2023).
-->
</p>
<h4>3.1.3 データ生成</h4>
<!--
<h4>3.1.3 Data Generation</h4>
-->
<p>
多様でクリーンなデータセットを用いて、教師ありファインチューニング（SFT）のための包括的な思考連鎖（COT）応答を生成します。しかし、すべてのデータポイントがトレーニングにおいて同等に価値があるわけではなく、DeepSeek-R1の抽出には、APIクエリ経由かローカルデプロイメント経由かを問わず、多くのリソースを消費する可能性があります。そこで、長推論モデルのトレーニングにおける最近の進歩（Luo et al., 2025b; Ye et al., 2025; Muennighoff et al., 2025）に着想を得て、データセットに難易度ベースのフィルタリングを実装し、十分に難しい質問のみを保持しました。
<!--
With a diverse and clean dataset, we generate comprehensive
chain-of-thought (COT) responses for
supervised fine-tuning (SFT). However, not all
data points are equally valuable for training, and
distilling DeepSeek-R1 can be resource-intensive
whether through API queries or local deployment.
We therefore implemented difficulty-based filtering
on the dataset to retain only sufficiently challenging
questions, inspired by recent advances in training
long reasoning models (Luo et al., 2025b; Ye et al.,
2025; Muennighoff et al., 2025).
-->
</p><p>
各質問に対する回答生成には、まずLuoら (2025b)のDeepScaleR-1.5B-Previewモデルを採用しました。このモデルは効率性と機能性のバランスに優れています。DeepSeek-R1クエリには、合格率がα未満の質問のみが選択されたため、約76,000のデータポイントが得られました。DeepSeek-R1の回答を取得した後、COT長文正解がある質問のみを保持しました。複数の正解がある質問については、SFT用にCOT長文正解をランダムに1つ選択しました。このプロセスを通じて、多様性と難易度の両方でフィルタリングされたプロンプトと、DeepSeek-R1によって生成され、正解データと照合されたCOT長文解答を含む、70,000例を超えるSFTデータセットを構築しました。
<!--
We initially employ Luo et al. (2025b)’s
DeepScaleR-1.5B-Preview model to generate responses
for each question, as this model offers a
good balance of efficiency and capability. Only
questions with a pass rate < α were selected for
DeepSeek-R1 queries, resulting in approximately
76k data points. After obtaining DeepSeek-R1 responses,
we retained only questions with correct
long-COT answers. For questions with multiple
correct responses, we randomly selected one long-
COT answer for SFT. Through this process, we constructed
an SFT dataset exceeding 70k examples,
featuring prompts filtered for both diversity and
difficulty, with long-COT responses generated by
DeepSeek-R1 and validated against ground truth.
-->
</p><p>
しかし、このデータセットのみを直接学習させた場合、学習エポック数に関わらず満足のいく結果は得られませんでした。学習済みモデルのパフォーマンスを様々な質問タイプで分析した結果、より難しい問題で追加の学習を行う必要があることがわかりました。そのため、DeepScaleR-1.5B-Previewではなく、DeepSeek-R1のフルバージョンを使用して、難易度フィルタリングの第2段階を実装しました。この段階では、合格率がα未満の質問と、DeepSeek-R1のサンプル回答が一様に正解でも一様に不正解でもない質問のみが保持され、約3,000例からなる第2段階のSFTデータセットが生成されました。注目すべきは、この改良されたデータセットは非常に高品質であり、このデータセットのみで学習させることで、すべてのDeepSeek-R1-Distillモデルのパフォーマンスが向上したことです。これについては、第3.4節で説明します。
<!--
However, direct training on this dataset alone
did not yield satisfactory results regardless of the
number of training epochs. Upon analyzing the
trained model’s performance across different question
types, we discovered the need for additional
training on more challenging problems. Consequently,
we implemented a second stage of difficulty
filtering using the full version of DeepSeek-
R1 instead of DeepScaleR-1.5B-Preview. This
stage retained only questions with pass rate < α and questions where DeepSeek-R1’s sampled responses
were neither uniformly correct nor uniformly
incorrect, resulting in a Stage 2 SFT dataset
of approximately 3k examples. Notably, this refined
dataset demonstrated such high quality that
training exclusively on it produced performance improvements
across all DeepSeek-R1-Distill models,
as we will discuss in Section 3.4.
-->
</p>
<h3>3.2 トレーニング後のカリキュラム</h3>
<!--
<h3>3.2 Curriculum Post-Training</h3>
-->
<p>
私たちのアプローチは3つの段階から構成されており、詳細なハイパーパラメータは付録Cに記載されています。
<div class="styleBullet">
<ul>
<li>1. SFT ステージ1：76,000個のフィルタリングされた数学問題によるトレーニング
</li><br><li>2. SFT ステージ2：3,000個の最も難易度の高い問題によるファインチューニング
</li><br><li>3. DPO最適化：検証済みの応答ペアを用いた選好に基づく最適化</li>
</ul>
</div>
<!--
Our approach consists of three stages, detailed hyperparameters
can be found in Appendix C.:
<div class="styleBullet">
<ul>
<li>1. SFT Stage 1: Training on 76k filtered mathematical
problems
</li><br><li>2. SFT Stage 2: Fine-tuning on 3k most challenging
problems
</li><br><li>3. DPO Optimization: Preference-based optimization
using verified response pairs</li>
</ul>
</div>
-->
</p><p>
SFTステージは、3.1.3節で説明したカリキュラムデータ戦略を用いて学習されます。DPOについては、NCA損失（Chen et al., 2024）を用いた半方策アプローチを実装しました。拒否された応答は、検証済みの誤答を含むSFTステージ2モデルからサンプリングされました。拒否された応答の中には32,000トークン以上の長さに達するものもあったため、360-LLaMA-Factory（Zou et al., 2024）のシーケンス並列性を備えたDPO実装を利用しました。選択応答については、DeepSeek-R1の検証済みの正解を使用しました。以前は完全方策DPOを広範囲に使用していましたが、難しい数学の問題では、より強力なモデルからの選択応答を使用することで、より良い結果が得られることを発見しました。
<!--
SFT stages are trained with the curriculum data
strategy as discussed in Sec. 3.1.3. For DPO, we
implemented a semi-on-policy approach using the
NCA loss (Chen et al., 2024). Rejected responses
were sampled from our SFT-stage-2 model with verified incorrect answers. Since some rejected
responses reached lengths of 32k tokens or more,
we utilized the DPO implementation with sequence
parallelism from 360-LLaMA-Factory (Zou et al.,
2024). For chosen responses, we used verified correct
answers from DeepSeek-R1. While we had
previously employed fully on-policy DPO extensively,
we discovered that for challenging mathematical
problems, using chosen responses from
significantly stronger models yielded better results.
-->
</p>
<h3>3.3 結果</h3>
<!--
<h3>3.3 Results</h3>
-->
<p>
カリキュラムSFTとDPOの訓練後段階全体を通して、一貫した改善が見られました（表2）。DPOの後、Goddard et al. (2024) ツールキットのTIES-merging（Yadav et al., 2023）手法を用いて、SFT-stage2、DPO、そして不採用となった回答から誤って特殊トークンが削除された別のDPOバリアント（AIME24スコア：74.7）のモデルを統合しました。その結果得られた統合モデルは、さらなるパフォーマンス向上を示しました。数学に重点を置いた訓練により、訓練されていないGPQAの科学的質問では多少の忘却が生じましたが、Light-R1-32Bは依然として高い一般化能力を示しています。
<!--
We observe consistent improvements across our
curriculum SFT & DPO post-training stages (Tab.
2). Following DPO, we use the TIES-merging (Yadav
et al., 2023) method from the Goddard et al.
(2024) toolkit to merged models from SFT-stage2,
DPO, and another DPO variant (AIME24 score:
74.7) that had special tokens inadvertently removed
from rejected responses, the resulting merged
model demonstrates additional performance gains.
Although our mathematics-focused training led to
some forgetting on untrained GPQA scientific questions,
Light-R1-32B still demonstrates strong generalization
capabilities.
-->
</p>
<center><img src="images/table2.png"></center>
<p>
表2：Light-R1-32Bの段階的なパフォーマンス向上。STF-stage2以降、GPQA（科学QA）スコアの低下が見られ、数学に重点を置いた広範なトレーニング中にモデルの汎化能力が部分的に低下したことを示しています。しかしながら、Light-R1-32Bはベースモデルと比較して依然として高い汎化性能を示しています。
<!--
Table 2: Stage-wise performance improvement of our
Light-R1-32B. We observe a decrease in GPQA (Science
QA) scores beginning from STF-stage2, indicating
a partial degradation of the model’s generalization capabilities
during extensive math-focused training. However,
Light-R1-32B still demonstrates strong generalization
compared to the base model.
-->
</p>
<center><img src="images/table3.png"></center>
<p>
表3：SFTステージ2の3Kデータの有効性。
おそらく我々のモデルとは直交するデータセットを活用している、より強力なベースモデルを微調整することで、あらゆるモデルサイズにおいて一貫してパフォーマンスが向上します。「Light-R1-14B-DS」という表記は、最終的なLight-R1-14B-DSモデルのSFTのみのバージョンを指し、その後、GRPO RLトレーニングの追加ステージが行われます。
<!--
Table 3: Effectiveness of the 3k data from SFT stage2.
Fine-tuning on stronger base models, which presumably
utilize datasets orthogonal to ours, consistently enhances
performance across all model sizes. The notation
Light-R1-14B-DS’ refers to the SFT-only version of
our final Light-R1-14B-DS model, which subsequently
undergoes an additional stage of GRPO RL training.
-->
</p>
<h3>3.4 必要なのは高品質なデータだけ</h3>
<!--
<h3>3.4 High-Quality Data is All You Need</h3>
-->
<p>
DeepSeek-R1-Distill-QwenモデルをSFTステージ1の強化版と見なし、3Kのステージ2データを用いてDeepSeek-R1-Distill-Qwenモデル上にSFTステージ2を実行しました。
<!--
Considering DeepSeek-R1-Distill-Qwen models
as a stronger version of our SFT stage 1, we performed
SFT stage 2 with the 3k stage 2 data on top
of DeepSeek-R1-Distill-Qwen models.
-->
</p><p>
驚くべきことに、表3に示すように、この3kデータだけでDeepSeek-R1-Distill-Qwenモデルの普遍的な改善を達成することができ、ステージ2データの品質の高さを実証しました。また、この3kデータはDeepSeek-R1-Distill-Qwenモデルの800k SFTデータとある程度直交しているため、このような改善が容易になったとも考えられます。
<!--
Surprisingly as Tab. 3, we could achieve universal
improvement on DeepSeek-R1-Distill-Qwen
models with this 3k data alone, demonstrating the
high quality of the stage 2 data. It may also be
because this 3k data is to some extent orthogonal
to DeepSeek-R1-Distill-Qwen models’ 800k SFT
data, hence such easy improvement.
-->
</p><p>
Light-R1-32B-DSは、科学分野とコード分野におけるドメイン固有のトレーニングが欠如しているにもかかわらず、GPQAの性能が予想外に高く、より強力なベースモデルはより強力な一般化能力の恩恵を受ける可能性があることを示唆しています。対照的に、Light-R1-7B-DSは、同一のデータカリキュラムでトレーニングされているにもかかわらず、ドメイン内タスクのみに限定された改善を示しています。
<!--
GPQA performance is unexpectedly high for
Light-R1-32B-DS, despite the absence of domainspecific
training in science and code domains, suggesting
that stronger base models may benefit from
stronger generalization capacities. In contrast,
Light-R1-7B-DS, while trained on identical data
curriculum, exhibits improvements confined solely
to in-domain tasks.
-->
</p>
<h2>4 Light-R1-14B-DS: Long-COTモデルからの強化学習</h2>
<!--
<h2>4 Light-R1-14B-DS: Reinforcement Learning from Long-COT Models</h2>
-->
<p>
強化学習の実験は、DeepSeek-R1-Distill-Qwen-14Bを用いて行いました。私たちの知る限り、これは、既に長期間使用されているCOT 14Bモデルにおいて、強化学習によってパフォーマンスが大幅に向上したことを実証した、初めて公に文書化された研究です。
<!--
We conduct our reinforcement learning experiments
on DeepSeek-R1-Distill-Qwen-14B. To the
best of our knowledge, this is the first publicly
documented work demonstrating significant improvement
in performance through RL on already
long-COT 14B models.
-->
</p><p>
DeepSeek-AI (2025)、Yuan et al. (2025b)、Zhang et al. (2025) によるこれまでの研究では、320億パラメータ以下の小規模モデルでも、大規模な推論モデルからの蒸留によって高い性能レベルに到達できることが示されています。しかし、すでに長時間のCOTで微調整されたモデルに対する強化学習（RL）によるさらなる改善は、コミュニティではまだ広く普及しておらず、ゼロRL（第1節）ほど容易には達成できません。Luo et al. (2025b) は、小規模モデルDeepSeek-R1-Distill-Qwen-1.5Bで有望なRLトレーニングを実証しましたが、同じレシピを使用して、より大規模なDeepSeek-R1-Distill-Qwen-14Bモデルで同様の結果を再現する際に課題に直面しました。
<!--
Previous studies by DeepSeek-AI (2025), Yuan
et al. (2025b), and Zhang et al. (2025) have shown
that smaller models (with 32 billion parameters or
fewer) can reach high performance levels through
distillation from larger reasoning models. However,
further improvement via RL (Reinforcement Learning)
on already long-COT finetuned models is not
yet widely reached by the community and is not as
easily reachable as zero RL (Sec. 1). While Luo
et al. (2025b) successfully demonstrated promising
RL training on a smaller model DeepSeek-R1-
Distill-Qwen-1.5B, we encountered challenges in
replicating similar results with the larger DeepSeek-
R1-Distill-Qwen-14B model using the same recipe.
-->
</p><p>
数週間にわたる調査を経て、私たちは効果的なカリキュラムSFTの試みとCuiら (2025) を参考に、2パスプロセスで構成される最終的な強化学習ソリューションに到達しました。プロセスは以下のとおりです。
<div class="styleBullet">
<ul>
<li>1. オフラインデータの選択：Light-R1-7BDSを使用して、強化学習トレーニングプロンプトの結果をサンプリングします。
パス率が0.25～0.625のプロンプトのみを保持します。
</li><br><li>2. オンライン強化学習：フィルタリングされたデータセットにGRPOを適用します。</li>
</ul>
</div>
<!--
After weeks of investigation, we arrived at our
final RL solution consisting of a two-pass process,
drawing inspiration from our effective curriculum
SFT attempt and Cui et al. (2025). The process is
as follows:
<div class="styleBullet">
<ul>
<li>1. Offline Data Selection: Use Light-R1-7BDS
to sample results of RL training prompts.
Keep only the prompts whose pass rate is between
0.25 and 0.625.
</li><br><li>2. Online Reinforcement Learning: Apply
GRPO on the filtered dataset.</li>
</ul>
</div>
-->
</p><p>
私たちの観察では、オフラインデータ選択が重要な役割を果たしています。これは、簡単すぎる、または難しすぎるプロンプトを除外し、トレーニングデータがルールベースの回答検証ツールと一致することを保証します。合格率0のデータを手動でチェックしたところ、プロンプトの回答の半分以上が検証不能（テキストまたは複雑な条件式が含まれているため）か、誤りであることがわかりました。難易度推定モデルとしてLight-R1-7B-DSを使用しました。これはより効率的であり、pass@64の点で大規模なモデルと同等のパフォーマンスを示すためです。さらに、合格率0のデータを再チェックするためにモデル検証ツールを使用しました。誤検証されたデータを除外することで、将来のカリキュラム強化学習において難しいプロンプトを正確に特定できます。
<!--
In our observation, offline data selection plays
a critical role. It filters out prompts that are too
easy or too hard and ensures that the training data
aligns with our rule-based answer verifier. When
manually checking data with a pass rate of 0, we
found that over half of the prompt answers are
either unverifiable (due to containing text or complex
conditional expressions) or incorrect. We utilize
Light-R1-7B-DS as the difficulty estimation
model because it is more efficient and demonstrates
similar performance to larger models in terms of
pass@64. Additionally, we use a model verifier to
re-check data with a pass rate of 0. By filtering out
the mis-verified data, we can successfully identify difficult prompts for future curriculum reinforcement
learning.
-->
</p><p>
最適化アルゴリズムとしてGRPO (Shao et al., 2024) を選択し、Verl (Sheng et al., 2024) に基づいて実装しました。また、強化学習プロセスを安定化するために、短い正解に対する選好度を弱めた長さ報酬の修正版 (Yeo et al., 2025) と、重要度サンプリング重みクリッピング (Mini-Max, 2025) という2つの手法を採用しています。
<!--
We choose GRPO (Shao et al., 2024) as the optimization
algorithm and implement it based on verl
(Sheng et al., 2024). We also employ two techniques
to stabilize the RL training process: modified
version of length reward (Yeo et al., 2025)
with weaker preference for short correct answers
and importance sampling weight clipping (Mini-
Max, 2025).
-->
</p><p>
長さの制御については、(Yeo et al., 2025) が提案したアプローチの改良版を採用します。具体的には、回答が正解の場合に短縮報酬を切り捨てることで、初期の長さの短縮を防ぎます。この手法は、学習中に適切な回答の長さを維持し、学習プロセスの初期段階でモデルが応答を過度に短縮しないようにするのに役立ちます。
<!--
For length control, we adopt a modified version
of the approach proposed by (Yeo et al., 2025).
Specifically, we clip the shortening reward when
answers are correct to prevent initial length collapse.
This technique helps maintain a reasonable
answer length during training, ensuring that the
model does not overly shorten its responses at the
beginning of the learning process.
-->
</p><p>
重要度サンプリングの重みクリッピングに関しては、より広範な両側クリッピング機構を実装します。
これまでの観察結果から、時折、大きな正のポリシー比率と負のアドバンテージが組み合わさると、損失の急増につながり、ポリシー最適化を阻害する可能性があることが示されています。この両側クリッピング手法は、MiniMax (2025) で報告された知見と並行して、以前の実験でも実装されています。重要度サンプリングの重みをクリッピングすることで、極端な値の影響を制限し、学習プロセスをより安定させることができます。
<!--
Regarding importance sampling weight clipping,
we implement a broader two-sided clipping mechanism.
Our observations have shown that occasional
large positive policy ratios combined with
negative advantages can lead to loss spikes, disrupting
policy optimization. This two-sided clipping
technique was also implemented in our previous
experiments, in parallel with the findings reported
by MiniMax (2025). By clipping the importance
sampling weights, we can limit the influence of
extreme values and make the training process more
stable.
-->
</p><p>
ルールベースの報酬とBig-Mathデータセット（Albalak他 (2025)）の重複排除版を使用します。実験は16 x 8のA100 GPUクラスターで実施しました。オフラインのデータ選択プロセスには4時間かかり、オンライン強化学習は140ステップの完了に26時間、220ステップの完了に42時間かかりました。
<!--
We use a rule-based reward and the deduplicated
version of the Big-Math dataset (Albalak
et al. (2025)). The experiments are conducted
on a cluster of 16 * 8 A100 GPUs. The offline data
selection process takes 4 hours, while the online
reinforcement learning takes 26 hours to complete
140 steps and 42 hours to complete 220 steps.
-->
</p>
<p>
図3からわかるように、RLトレーニングは期待通りの挙動を示し、応答長と報酬スコアが同時に増加しています。開始時に注目すべき長さの減少は見られません。3エポックのトレーニングを終えた後、RLエポック1と2を評価しました。表4に示すように、最初の2エポックでは大きな改善は見られませんが、健全なRLトレーニング曲線はトレーニングを継続する自信を与えてくれます。Light-R1-14B-DSは最終的に約3エポック、つまり220ステップのRLトレーニングを受けました。
<!--
As can be seen from Fig. 3, our RL training
demonstrates expected behavior: simultaneous increase
in response length and reward score. No
interesting length dropping in the beginning. We
evaluated RL epochs 1 and 2 after we finished training
3 epochs. As shown in Tab. 4, although first
two epochs seem to bring not much improvement,
the healthy RL training curves offer us confidence
to continue training. Light-R1-14B-DS is finally
RL trained for around 3 epochs, or 220 steps.
-->
</p>
<center><img src="images/fig3.png"></center>
<p>
図3：Savitzky-Golayフィルタで平滑化された、応答長と訓練報酬のRL学習曲線。
<!--
Figure 3: RL Learning curves of response length and
train-reward, smoothed with Savitzky-Golay filter.
-->
</p>
<center><img src="images/table4.png"></center>
<p>
表4：Light-R1-14B-DSの強化学習性能の向上。特に、GPQAにおいて領域外の改善が見られ、数学に重点を置いたデータセットにおける強化学習が、多様な領域にわたる一般化を促進する可能性があることを示唆しています。
<!--
Table 4: RL performance improvement of Light-R1-
14B-DS. Notably, we observe out-of-domain improvement
in GPQA, indicating that reinforcement learning
on mathematics-focused datasets potentially facilitates
generalization across diverse domains.
-->
</p>
<h2>5 結論</h2>
<!--
<h2>5 Conclusion</h2>
-->
<p>
Light-R1シリーズは、リソース制約下での長時間推論モデルの学習という課題に取り組んでいます。カリキュラム学習戦略を用いて、長時間COTモデルをゼロから学習することに成功しました。厳選された3Kデータセットは、様々なモデルサイズ間で優れた移植性を示し、DeepSeek-R1-Distillモデルを大幅に強化し、7B、14B、32Bのパラメータを持つモデルの新たなパフォーマンスベンチマークを確立しました。さらに、強力な多段階微調整ベースモデルに強化学習を適用した場合の有効性を調査し、学習プロセス全体を通して安定した応答長の増加を維持しながら、優れたパフォーマンスを実現しました。
<!--
Our Light-R1 series addresses the challenge of
training long reasoning models under resource constraints.
We successfully train a long-COT model
from scratch through our curriculum training strategy.
Our carefully curated 3K dataset demonstrates
remarkable transferability across various model
sizes, significantly enhancing DeepSeek-R1-Distill
models and establishing new performance benchmarks
for models with 7B, 14B, and 32B parameters.
Additionally, we investigate the efficacy of
reinforcement learning when applied to a strong
multi-stage finetuned base model, achieving superior
performance while maintaining stable response
length growth throughout the training process.
-->
</p><p>
これらの進歩は、R1レベルの推論機能へのアクセスを民主化するだけでなく、長期推論モデルのカリキュラム設計、データ効率、RLのスケーラビリティに関する貴重な洞察も提供します。私たちのオープンソースモデル、データセット、そしてコードは、特にリソースが限られたアプリケーション向けに、コンパクトでありながら強力な推論システムの開発研究を加速することを目的としています。今後の研究では、長期推論モデル向けの強化された一般化機能の統合と、RLトレーニング効率のさらなる最適化を検討します。
<!--
These advancements not only democratize access
to R1-level reasoning capabilities but also provide
valuable insights into curriculum design, data
efficiency, and RL scalability for long reasoning
models. Our open-source models, datasets, and
code aim to accelerate research in developing compact
yet powerful reasoning systems, particularly
for resource-constrained applications. Future work
will explore the integration of enhanced generalization
capabilities for long reasoning models and
further optimization of RL training efficiency.
-->
</p>
<h2>参考文献</h2>
<!--
<h2>References</h2>
-->
<p>
<div class="styleRef">
<ul>
<li>Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov,
Kanishk Gandhi, Louis Castricato, Anikait Singh,
Chase Blagden, Violet Xiang, Dakota Mahan, and
Nick Haber. 2025. Big-math: A large-scale, highquality
math dataset for reinforcement learning in
language models. Preprint, arXiv:2502.17387.
</li><br><li>Huayu Chen, Guande He, Hang Su, and Jun Zhu. 2024.
Noise contrastive alignment of language models with
explicit rewards. arXiv preprint arXiv:2402.05369.
</li><br><li>Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang,
Wendi Li, Bingxiang He, Yuchen Fan, Tianyu
Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu
Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan
Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu,
Maosong Sun, Bowen Zhou, and Ning Ding. 2025.
Process Reinforcement through Implicit Rewards.
Preprint, arXiv:2502.01456.
</li><br><li>DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning
capability in llms via reinforcement learning.
Preprint, arXiv:2501.12948.
</li><br><li>Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo
Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang
Chen, Runxin Xu, Zhengyang Tang, Benyou Wang,
Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei
Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu,
and Baobao Chang. 2024. Omni-math: A universal
olympiad level mathematic benchmark for large
language models. Preprint, arXiv:2410.07985.
</li><br><li>Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling
laws for reward model overoptimization. In International
Conference on Machine Learning, pages
10835–10866. PMLR.
</li><br><li>Charles Goddard, Shamane Siriwardhana, Malikeh
Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian
Benedict, Mark McQuade, and Jacob Solawetz. 2024.
Arcee’s MergeKit: A toolkit for merging large language
models. In Proceedings of the 2024 Conference
on Empirical Methods in Natural Language
Processing: Industry Track, pages 477–485, Miami,
Florida, US. Association for Computational Linguistics.
</li><br><li>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021a. Measuring mathematical
problem solving with the math dataset. Preprint,
arXiv:2103.03874.
</li><br><li>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021b. Measuring mathematical
problem solving with the math dataset. NeurIPS.
</li><br><li>Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang,
and Heung-Yeung Shum Xiangyu Zhang. 2025.
Open-reasoner-zero: An open source approach to
scaling reinforcement learning on the base model.
https://github.com/Open-Reasoner-Zero/
Open-Reasoner-Zero.
</li><br><li>Kimi. 2025. Kimi k1.5: Scaling reinforcement learning
with llms. Preprint, arXiv:2501.12599.
</li><br><li>Bespoke Labs. 2025. Bespoke-stratos: The unreasonable
effectiveness of reasoning distillation. Accessed:
2025-01-22.
</li><br><li>Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025.
Limr: Less is more for rl scaling. Preprint,
arXiv:2502.11886.
</li><br><li>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri
Edwards, Bowen Baker, Teddy Lee, Jan Leike,
John Schulman, Ilya Sutskever, and Karl Cobbe.
2023. Let’s verify step by step. Preprint,
arXiv:2305.20050.
</li><br><li>Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang,
Chao Du, and Min Lin. 2025. There may not
be aha moment in r1-zero-like training — a pilot
study. https://oatllm.notion.site/oat-zero.
Notion Blog.
</li><br><li>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang
Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, Yansong Tang, and Dongmei
Zhang. 2025a. Wizardmath: Empowering mathematical
reasoning for large language models via reinforced
evol-instruct. Preprint, arXiv:2308.09583.
</li><br><li>Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi,
William Y. Tang, Manan Roongta, Colin Cai, Jeffrey
Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa,
and Ion Stoica. 2025b. Deepscaler: Surpassing o1-
preview with a 1.5b model by scaling rl. Notion
Blog.
</li><br><li>MAA. 2024. American invitational mathematics examination
- aime. In American Invitational Mathematics
Examination - AIME 2024.
</li><br><li>Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen,
Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang,
Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao,
Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen.
2024. Imitate, explore, and self-improve: A reproduction
report on slow-thinking reasoning systems.
Preprint, arXiv:2412.09413.
</li><br><li>MiniMax. 2025. MiniMax-01: Scaling Foundation
Models with Lightning Attention. Preprint,
arXiv:2501.08313.
</li><br><li>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang
Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke
Zettlemoyer, Percy Liang, Emmanuel Candès, and
Tatsunori Hashimoto. 2025. s1: Simple test-time
scaling. arXiv preprint arXiv:2501.19393.
</li><br><li>OpenAI. 2024. Learning to reason with llms.
</li><br><li>OpenR1. 2025. Open r1: A fully open reproduction of
deepseek-r1.
</li><br><li>OpenThoughts. 2025. Open Thoughts. https://openthoughts.
ai.
</li><br><li>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instructions
with human feedback. Advances in neural information
processing systems, 35:27730–27744.
</li><br><li>Qwen. 2024. Qwen2.5: A party of foundation models.
</li><br><li>Qwen. 2025. Qwq-32b: Embracing the power of reinforcement
learning.
</li><br><li>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. Preprint,
arXiv:2305.18290.
</li><br><li>David Rein, Betty Li Hou, Asa Cooper Stickland,
Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,
Julian Michael, and Samuel R. Bowman. 2023.
Gpqa: A graduate-level google-proof q&a benchmark.
Preprint, arXiv:2311.12022.
</li><br><li>John Schulman, Philipp Moritz, Sergey Levine, Michael
Jordan, and Pieter Abbeel. 2015. High-dimensional
continuous control using generalized advantage estimation.
Preprint, arXiv:1506.02438.
</li><br><li>John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proximal
policy optimization algorithms. Preprint,
arXiv:1707.06347.
</li><br><li>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan
Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024.
DeepSeekMath: Pushing the Limits of Mathematical
Reasoning in Open Language Models. Preprint,
arXiv:2402.03300.
</li><br><li>Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin
Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin
Lin, and Chuan Wu. 2024. Hybridflow: A flexible
and efficient rlhf framework. arXiv preprint arXiv:
2409.19256.
</li><br><li>Yifan Song, Guoyin Wang, Sujian Li, and Bill Yuchen
Lin. 2024. The good, the bad, and the greedy: Evaluation
of llms should not ignore non-determinism.
arXiv preprint arXiv:2407.10457.
</li><br><li>Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav
Kisacanin, Alexan Ayrapetyan, and Igor Gitman.
2024. Openmathinstruct-2: Accelerating ai for math
with massive open-source instruction data. arXiv
preprint arXiv:2410.01560.
</li><br><li>Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai
Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui.
2024. Math-shepherd: Verify and reinforce llms
step-by-step without human annotations. Preprint,
arXiv:2312.08935.
</li><br><li>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,
and Denny Zhou. 2022. Chain-of-thought prompting
elicits reasoning in large language models. In
NeurIPS.
</li><br><li>Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin,
Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin
Guo, JunzheWang, Honglin Guo,Wei Shen, Xiaoran
Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo
Zhang, Peng Sun, Tao Gui, Qi Zhang, and Xuanjing
</li><br><li>Huang. 2024. Training large language models for
reasoning through reverse curriculum reinforcement
learning. Preprint, arXiv:2402.05808.
Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li,
Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming
Ji, Yingying Zhang, Zhijiang Guo, Yaodong
Yang, Muhan Zhang, and Debing Zhang. 2025. Redstar:
Does scaling long-cot data unlock better slowreasoning
systems? Preprint, arXiv:2501.11284.
</li><br><li>Prateek Yadav, Derek Tam, Leshem Choshen, Colin
Raffel, and Mohit Bansal. 2023. Ties-merging: Resolving
interference when merging models. Preprint,
arXiv:2306.01708.
</li><br><li>Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie
Xia, and Pengfei Liu. 2025. Limo: Less is more for
reasoning. Preprint, arXiv:2502.03387.
</li><br><li>Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig,
and Xiang Yue. 2025. Demystifying Long
Chain-of-Thought Reasoning in LLMs. Preprint,
arXiv:2502.03373.
</li><br><li>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo
Li, Adrian Weller, and Weiyang Liu. 2024.
Metamath: Bootstrap your own mathematical
questions for large language models. Preprint,
arXiv:2309.12284.
</li><br><li>Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye,
Zhengyin Du, and Jiecao Chen. 2025a. Agent-r:
Training language model agents to reflect via iterative
self-training. Preprint, arXiv:2501.11425.
</li><br><li>Yufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and
Lin Yan. 2025b. What’s Behind PPO’s Collapse
in Long-CoT? Value Optimization Holds the Secret.
Preprint, arXiv:2503.01491.
</li><br><li>Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing
He, Qian Liu, Zejun Ma, and Junxian He. 2025.
7b model and 8k examples: Emerging reasoning
with reinforcement learning is both effective
and efficient. https://hkust-nlp.notion.site/
simplerl-reason. Notion Blog.
</li><br><li>Hanning Zhang, Jiarui Yao, Chenlu Ye, Wei Xiong,
and Tong Zhang. 2025. Online-dpo-r1: Unlocking
effective reasoning without the ppo overhead. Notion
Blog.
</li><br><li>Haosheng Zou, Xiaowei Lv, Shousheng Jia, and Xiangzheng
Zhang. 2024. 360-llama-factory.</li>
</ul>
</div>
</p>
<h2>59,000 の質問すべてに対応するデータセットの構成</h2>
<!--
<h2>A Dataset composition for full 59K questions</h2>
-->
<p>
表5：公開データの構成。ここでは、第一段階の多様性と難易度によるフィルタリング後のデータ構成をまとめています。異なるソースには重複する例が含まれている可能性があります。そのため、初期のシードデータセットとしてOpenR1-Math-220kを使用しています。これが、このソースがデータの大部分を占めている理由です。
<!--
Table 5: Composition of the released data. Here we summarize the data composition after the first stage diversity
and difficulty filtering. Different sources may contain overlapping examples, we use OpenR1-Math-220k as our
initial seed dataset, which explains why this source contributes the largest portion of our data.
-->
            <table border="1">
                
<tr>
<th>
ソース
<!-- Source -->
</th>
<th>
説明
<!-- Description -->
</th>
<th>
サンプル数
<!-- #Samples --> 
</th>
</tr>
                
<tr>
<td>
OpenR1-Math-220k (OpenR1,2025)
</td>
<td>
2～4つの推論要素を含む数学問題
NuminaMath 1.5の問題に対してDeepSeek R1で生成されたトレース。
<!--
Math problems with two to four reasoning
traces generated by DeepSeek R1 for problems
from NuminaMath 1.5.
-->
</td>
<td>
58224
</td>
</tr>

<tr>
<td>
OpenThoughts-
114k (OpenThoughts, 2025)
</td>
<td>
数学、科学、コード、パズルを網羅した11万4千件の高品質な例を含む、オープンな合成推論データセット
<!--
Open synthetic reasoning dataset with
114k high-quality examples covering math,
science, code, and puzzles
-->
</td>
<td>
14214
</td>
</tr>

<tr>
<td>
OpenMathInstruct-2 (Toshniwal
et al., 2024)
</td>
<td>
Llama3.1-405B-Instructモデルを使用して生成された
Nvidiaによる数学命令チューニングデータセット
<!--
Math instruction tuning dataset generated
using the Llama3.1-405B-Instruct model
by Nvidea
-->
</td>
<td>
1786
</td>
</tr>

<tr>
<td>
OmniMath (Gao et al., 2024)
</td>
<td>
コンテストの数学の問題
<!--
Math problems from competitions
-->
</td>
<td>
567
</td>
</tr>

<tr>
<td>
s1K-1.1 (Muennighoff et al.,
2025)
</td>
<td>
多様で高品質、そして難解な問題
DeepSeek-R1から抽出された推論の痕跡と解答付き
<!--
Diverse, high-quality & difficult questions
with distilled reasoning traces & solutions
from DeepSeek-R1
-->
</td>
<td>
346
</td>
</tr>

<tr>
<td>
LIMO (Ye et al., 2025)
</td>
<td>
LIMO論文からの3段階フィルタリングデータ
<!--
Three-stage filtered data from the LIMO paper
-->
</td>
<td>
246
</td>
</tr>

<tr>
<td>
hendrycks-math (Hendrycks et al., 2021b)
</td>
<td>
12,500問の難解な数学競技問題。各問題にはステップバイステップの解答が用意されており、解答の導出と説明を生成するモデルを学習する際に役立ちます。
<!--
12,500 challenging competition mathematics
problems. Each problem in MATH has
a full step-by-step solution which can be
used to teach models to generate answer
derivations and explanation
-->
</td>
<td>
179
</td>
</tr>

<tr>
<td>
Ours
</td>
<td>
社内数学データセット
<!--
In-house math dataset
-->
</td>
<td>
3877
</td>
</tr>

<tr>
<td>
Total
</td>
<td>
上記のデータセットと推論のトレースおよび解決策を組み合わせたもの
<!--
Composite of the above datasets with reasoning
traces and solutions
-->
</td>
<td>
79439
</td>
</tr>

</table>

</p>
<h2>B データの除染</h2>
<!--
<h2>B Data Decontamination</h2>
-->
<center><img src="images/table6.png"></center>
<p>
表 6: オープンソース データセット内のベンチマークに一致したプロンプトの数。
<!--
Table 6: Number of matched prompts in open-source datasets against benchmarks.
-->
</p>
<h2>Light-R1 シリーズの C トレーニングハイパーパラメータ</h2>
<!--
<h2>C Training hyperparameters for Light-R1 series</h2>
-->
<center><img src="images/table7.png"></center>
<p>
表7：Light-R1シリーズの学習ハイパーパラメータ。シーケンス長は学習データの特性によって決定されますが、GRPOは例外で、ロールアウト計算コストの最小化、推論カットオフ比の低減、32kコンテキスト評価性能の最適化など、複数の要素のバランスを考慮します。32kコンテキスト長のDPOを学習する際のGPUメモリの制限を克服するため、360-LLaMA-Factory (Zou et al., 2024) のシーケンス並列処理を備えたDPO実装を利用します。「-DS」サフィックスが付いたモデルはDeepSeek-R1-Distill-Qwenシリーズから派生したもので、その他のモデルはQwen2.5-32B-Instructシリーズから派生したものです。
<!--
Table 7: Training hyperparameters for Light-R1 series. Sequence length is determined by training data characteristics,
except for GRPO where it balances multiple factors: minimizing roll-out computational costs, reducing inference
cut-off ratio, and optimizing 32k context evaluation performance. To overcome the limitation of GPU memory
for training DPO with 32k context length, we utilize the DPO implementation with sequence parallelism from
360-LLaMA-Factory (Zou et al., 2024). Models with the "-DS" suffix derive from the DeepSeek-R1-Distill-Qwen
series, while others from Qwen2.5-32B-Instruct.
-->
</p>
    </body>
</html>